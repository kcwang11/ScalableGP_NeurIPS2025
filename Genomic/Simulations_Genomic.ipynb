{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416b0d9a",
   "metadata": {},
   "source": [
    "# Important Note\n",
    "RAM and VRAM measurements are dependent on the computer state, and should only be interpreted relative to each other. In order to obtain RAM and VRAM measurements, perform the following steps:\n",
    "\n",
    "1 - Restart the Kernel\n",
    "\n",
    "2 - Run the \"Loading Required Packages and Helper Functions\" cell\n",
    "\n",
    "3 - Run the \"Loading Data\" cell\n",
    "\n",
    "4 - Run ONLY ONE iteration of the desired method, and read the RAM and VRAM usage reports printed by the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6bfc9",
   "metadata": {},
   "source": [
    "# Loading Required Packages and Helper Functions\n",
    "If you would like to use Cuda, set gpu = True. Otherwise set gpu = False. \n",
    "\n",
    "Step 1: Run the following cell to import the required packages and helper functions. Set the number of replicates desired.\n",
    "\n",
    "Step 2: Load the Data\n",
    "\n",
    "Step 3: Execute the cells under the method you wish to replicate.\n",
    "\n",
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a4ade5-38b2-4f05-8626-7fb74d9fe5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import gpytorch\n",
    "import pynvml\n",
    "import psutil\n",
    "import statistics\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import pynvml\n",
    "import psutil\n",
    "\n",
    "def log_memory():\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "    max_allocated = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "    max_reserved = torch.cuda.max_memory_reserved() / 1024**2    # MB\n",
    "    gpu_used = meminfo.used / 1024**2                            # MB\n",
    "    sys_used = psutil.virtual_memory().used / 1024**3            # GB\n",
    "    print(f\"[PyTorch] Max Allocated: {max_allocated:.2f} MB | Max Reserved: {max_reserved:.2f} MB\")\n",
    "    print(f\"[GPU VRAM] Used (nvidia-smi): {gpu_used:.2f} MB | [System RAM]: {sys_used:.2f} GB\")\n",
    "\n",
    "    return max_allocated, max_reserved, gpu_used, sys_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6605e5-f791-44f8-ada3-9efb2237ed1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu = True\n",
    "n_replicates = 10\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "import statistics\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import psutil\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def get_mem():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss/(1024**2)\n",
    "\n",
    "max_vram = 0\n",
    "def vram_usage():\n",
    "    global max_vram\n",
    "    max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.mlls import DeepApproximateMLL\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "max_vram = 0\n",
    "max_ram = 0\n",
    "\n",
    "def vram_usage():\n",
    "    global max_vram\n",
    "    max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7e28d-807c-40e4-8868-4146cd1c5911",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "Step 2: Load the data (note: must run the DataGenerator.Rmd file first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f66626-c1c8-4f7f-a55d-29b5b743d2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_x shape: torch.Size([393542, 2])\n",
      "all_y shape: torch.Size([393542])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "gpu = torch.cuda.is_available()\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "coords_df = pd.read_csv('Data/coordinates.csv')\n",
    "all_x = torch.tensor(coords_df.values, dtype=torch.float32)\n",
    "\n",
    "expr_df = pd.read_csv('Data/Mbp.csv')\n",
    "all_y = torch.tensor(expr_df.iloc[:, 0].values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "all_x = all_x.contiguous()\n",
    "all_y = all_y.contiguous()\n",
    "\n",
    "\n",
    "print(\"all_x shape:\", all_x.shape)\n",
    "print(\"all_y shape:\", all_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec0bbfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def splitter(x_cpu, y_cpu, n_train=80000, n_test=20000, random_state=42, move_to_gpu=True):\n",
    "    assert x_cpu.shape[0] == y_cpu.shape[0], \"Mismatch in number of samples\"\n",
    "    total_samples = x_cpu.shape[0]\n",
    "    assert n_train + n_test <= total_samples, \"Not enough samples to split\"\n",
    "\n",
    "    # Set random seed\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "\n",
    "    # Randomly permute indices\n",
    "    indices = rng.permutation(total_samples)\n",
    "\n",
    "    # Select subsets\n",
    "    train_idx = indices[:n_train]\n",
    "    test_idx  = indices[n_train:n_train + n_test]\n",
    "\n",
    "    train_x = x_cpu[train_idx].contiguous()\n",
    "    train_y = y_cpu[train_idx].contiguous()\n",
    "    test_x  = x_cpu[test_idx].contiguous()\n",
    "    test_y  = y_cpu[test_idx].contiguous()\n",
    "\n",
    "    if move_to_gpu and torch.cuda.is_available():\n",
    "        train_x = train_x.cuda()\n",
    "        train_y = train_y.cuda()\n",
    "        test_x = test_x.cuda()\n",
    "        test_y = test_y.cuda()\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a9d9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: torch.Size([80000, 2])\n",
      "train_y shape: torch.Size([80000])\n",
      "test_x shape: torch.Size([20000, 2])\n",
      "test_y shape: torch.Size([20000])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "coords_df = pd.read_csv('Data/coordinates.csv')\n",
    "expr_df = pd.read_csv('Data/Mbp.csv')\n",
    "\n",
    "all_x = torch.tensor(coords_df.values, dtype=torch.float32).contiguous()\n",
    "all_y = torch.tensor(expr_df.iloc[:, 0].values, dtype=torch.float32).contiguous()\n",
    "\n",
    "\n",
    "total_samples = all_x.shape[0]\n",
    "assert total_samples >= 100_000, \"Not enough samples for split\"\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "indices = rng.permutation(total_samples)\n",
    "\n",
    "train_idx = indices[:80_000]\n",
    "test_idx  = indices[80_000:100_000]\n",
    "\n",
    "train_x = all_x[train_idx]\n",
    "train_y = all_y[train_idx]\n",
    "test_x  = all_x[test_idx]\n",
    "test_y  = all_y[test_idx]\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_x = train_x.cuda()\n",
    "    train_y = train_y.cuda()\n",
    "    test_x = test_x.cuda()\n",
    "    test_y = test_y.cuda()\n",
    "\n",
    "\n",
    "print(\"train_x shape:\", train_x.shape)\n",
    "print(\"train_y shape:\", train_y.shape)\n",
    "print(\"test_x shape:\", test_x.shape)\n",
    "print(\"test_y shape:\", test_y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f697e43",
   "metadata": {},
   "source": [
    "# Simulations\n",
    "Step 3: Execute the simulations to be reproduced. If all simulations are run, there is a summarizer at the end. Otherwise, the relevant statistics are printed at the end of each method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db075e72",
   "metadata": {},
   "source": [
    "# Deep Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09de4a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Replicate 1/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (Rep 1):   0%|          | 0/10 [00:00<?, ?it/s]c:\\ProgramData\\miniconda3\\envs\\scaleGP3\\Lib\\site-packages\\linear_operator\\utils\\sparse.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if nonzero_indices.storage():\n",
      "c:\\ProgramData\\miniconda3\\envs\\scaleGP3\\Lib\\site-packages\\linear_operator\\utils\\sparse.py:66: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\tensor\\python_tensor.cpp:80.)\n",
      "  res = cls(index_tensor, value_tensor, interp_size)\n",
      "c:\\ProgramData\\miniconda3\\envs\\scaleGP3\\Lib\\site-packages\\linear_operator\\utils\\sparse.py:66: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:643.)\n",
      "  res = cls(index_tensor, value_tensor, interp_size)\n",
      "                                                        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 102\u001b[0m\n\u001b[0;32m    100\u001b[0m output \u001b[38;5;241m=\u001b[39m model(train_x)\n\u001b[0;32m    101\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, train_y)\n\u001b[1;32m--> 102\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    104\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP3\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP3\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP3\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "import gpytorch\n",
    "import pynvml\n",
    "import psutil\n",
    "import statistics\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "# Memory tracking function from the first prompt\n",
    "def log_memory():\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    \n",
    "    print(f\"[GPU] Used: {meminfo.used / 1024**2:.2f} MB\")\n",
    "    print(f\"[PyTorch] Max Allocated: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"[System RAM] Used: {psutil.virtual_memory().used / 1024**3:.2f} GB\")\n",
    "\n",
    "# Process memory tracker (RAM used by the model/training)\n",
    "def get_process_ram_mb():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024**2  # in MB\n",
    "\n",
    "# Hyperparameters\n",
    "n_replicates = 10\n",
    "training_iterations = 10\n",
    "batch_size = 32#00\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "# Model definition\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(input_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 2))\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, feature_extractor):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5)),\n",
    "            num_dims=2, grid_size=100\n",
    "        )\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.scale_to_bounds(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(\n",
    "            self.mean_module(x), self.covar_module(x)\n",
    "        )\n",
    "\n",
    "# Benchmark loop\n",
    "mse_list, time_list = [], []\n",
    "\n",
    "for i in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {i + 1}/{n_replicates} ===\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Sample new data\n",
    "    train_x, train_y, test_x, test_y = splitter(all_x, all_y, n_train=80000, n_test=20000, random_state=i)\n",
    "\n",
    "    # Record initial process RAM usage\n",
    "    initial_ram = get_mem()\n",
    "    \n",
    "    # Model + likelihood reset\n",
    "    feature_extractor = LargeFeatureExtractor(train_x.size(-1))\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood, feature_extractor)\n",
    "\n",
    "    if gpu:\n",
    "        model, likelihood = model.cuda(), likelihood.cuda()\n",
    "        train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "\n",
    "    model.train(); likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.feature_extractor.parameters()},\n",
    "        {'params': model.covar_module.parameters()},\n",
    "        {'params': model.mean_module.parameters()},\n",
    "        {'params': model.likelihood.parameters()},\n",
    "    ], lr=0.5)\n",
    "\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # Train\n",
    "    start = time.time()\n",
    "    pbar = trange(training_iterations, desc=f\"Training (Rep {i + 1})\", leave=False)\n",
    "    for j in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    final_ram = get_mem()\n",
    "\n",
    "    # Print memory usage information\n",
    "    log_memory()\n",
    "    \n",
    "    # Record final process RAM usage after training and evaluation\n",
    "    \n",
    "    print(f\"[RAM Tracker] Model & Training RAM Usage: {final_ram:.2f} MB (Initial: {initial_ram:.2f} MB, Increase: {final_ram - initial_ram:.2f} MB)\")\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval(); likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        pred = likelihood(model(test_x)).mean.cpu()\n",
    "    mse = torch.mean((pred - test_y.cpu())**2).item()\n",
    "\n",
    "    # Record results\n",
    "    mse_list.append(mse)\n",
    "    time_list.append(elapsed)\n",
    "\n",
    "    \n",
    "    # GPU cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nMSE: {round(statistics.mean(mse_list), 5)} ± {round(statistics.stdev(mse_list), 5)}\")\n",
    "print(f\"Time: {round(statistics.mean(time_list), 2)}s ± {round(statistics.stdev(time_list), 2)}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nMSE mean: {statistics.mean(mse_list)}\")\n",
    "print(f\"MSE std: {statistics.stdev(mse_list)}\")\n",
    "\n",
    "print(f\"Time mean: {statistics.mean(time_list)}s\")\n",
    "print(f\"Time std: {statistics.stdev(time_list)}s\")\n",
    "\n",
    "mse_mean = statistics.mean(mse_list)\n",
    "mse_std = statistics.stdev(mse_list)\n",
    "time_mean = statistics.mean(time_list)\n",
    "time_std = statistics.stdev(time_list)\n",
    "\n",
    "print(f\"{mse_mean:.5f}  & ({mse_std:.5f})  & {time_mean:.2f}  & ({time_std:.2f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5806e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6aae2815-2c3a-4ea5-85bc-e7d80e7c52c9",
   "metadata": {},
   "source": [
    "# Sparse GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe2fcd-83d3-4459-b120-9fe92575fbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Replicate 1/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     72\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 73\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m#torch.cuda.empty_cache()\u001b[39;00m\n\u001b[0;32m     75\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange, tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "n_replicates = 10\n",
    "training_iterations = 350\n",
    "batch_size = 3200\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.base_covar_module = ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2))\n",
    "        self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=train_x[torch.randperm(train_x.shape[0])[:100]].clone(), likelihood=likelihood)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.base_covar_module = ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2))\n",
    "        self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=train_x[::300].clone(), likelihood=likelihood)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Benchmark loop\n",
    "mse_list, time_list = [], []\n",
    "\n",
    "for i in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {i + 1}/{n_replicates} ===\")\n",
    "\n",
    "    # Sample new data\n",
    "    train_x, train_y, test_x, test_y = splitter(all_x, all_y, n_train=80000, n_test=20000, random_state=i)\n",
    "\n",
    "\n",
    "    initial_ram = get_mem()\n",
    "\n",
    "    # Model + likelihood reset\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "\n",
    "    if gpu:\n",
    "        model, likelihood = model.cuda(), likelihood.cuda()\n",
    "        train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "\n",
    "    model.train(); likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # Train\n",
    "    start = time.time()\n",
    "    pbar = trange(training_iterations, desc=f\"Training (Rep {i + 1})\", leave=False)\n",
    "    for j in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "        #torch.cuda.empty_cache()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    final_ram = get_mem()\n",
    "    log_memory()\n",
    "    print(f\"[RAM Tracker] Model & Training RAM Usage: {final_ram:.2f} MB (Initial: {initial_ram:.2f} MB, Increase: {final_ram - initial_ram:.2f} MB)\")\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval(); likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        pred = likelihood(model(test_x)).mean.cpu()\n",
    "    mse = torch.mean((pred - test_y.cpu())**2).item()\n",
    "\n",
    "    # Record results\n",
    "    mse_list.append(mse)\n",
    "    time_list.append(elapsed)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nMSE: {round(statistics.mean(mse_list), 5)} ± {round(statistics.stdev(mse_list), 5)}\")\n",
    "print(f\"Time: {round(statistics.mean(time_list), 2)}s ± {round(statistics.stdev(time_list), 2)}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97bc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_mean = statistics.mean(mse_list)\n",
    "mse_std = statistics.stdev(mse_list)\n",
    "time_mean = statistics.mean(time_list)\n",
    "time_std = statistics.stdev(time_list)\n",
    "\n",
    "print(f\"{mse_mean:.5f}  & ({mse_std:.5f})  & {time_mean:.2f}  & ({time_std:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e92cd-510c-4dd4-8577-bf9ad4d0dfe7",
   "metadata": {},
   "source": [
    "# LOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91503e-88f6-4084-adfe-79aadc7fb7b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m training_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      6\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3200\u001b[39m\n\u001b[1;32m----> 7\u001b[0m gpu \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLargeFeatureExtractor\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import trange, tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "n_replicates = 10\n",
    "training_iterations = 10\n",
    "batch_size = 3200\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(input_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 2))\n",
    "        print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2), \"MB\")\n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2)\n",
    "            ),\n",
    "            grid_size=100, num_dims=2,\n",
    "        )\n",
    "        self.feature_extractor = LargeFeatureExtractor(input_dim=train_x.size(-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected_x = self.feature_extractor(x)\n",
    "        projected_x = projected_x - projected_x.min(0)[0]\n",
    "        projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "\n",
    "        mean_x = self.mean_module(projected_x)\n",
    "        covar_x = self.covar_module(projected_x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "mse_l_love = []\n",
    "time_l_love = []\n",
    "\n",
    "for i in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {i + 1}/{n_replicates} ===\")\n",
    "\n",
    "    # Split data\n",
    "    train_x, train_y, test_x, test_y = splitter(all_x, all_y, n_train=80000, n_test=20000, random_state=i)\n",
    "\n",
    "    process = psutil.Process()\n",
    "    ram_before = process.memory_info().rss / (1024 ** 2)\n",
    "    mem_begin = get_mem()\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "\n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "        train_x, train_y = train_x.cuda(), train_y.cuda()\n",
    "        test_x, test_y = test_x.cuda(), test_y.cuda()\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # Training loop\n",
    "    def train():\n",
    "        pbar = trange(training_iterations, desc=f\"Training (Rep {i + 1})\", leave=False)\n",
    "        for _ in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            vram_usage()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    start = time.time()\n",
    "    train()\n",
    "    uTime = time.time() - start\n",
    "\n",
    "    log_memory()\n",
    "\n",
    "    print(\"Time:\", uTime)\n",
    "    print(torch.cuda.memory_allocated() / (1024 ** 2), \"MB allocated\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "        means = observed_pred.mean.cpu()\n",
    "        mse = torch.mean((means - test_y.cpu()) ** 2).item()\n",
    "\n",
    "    mse_l_love.append(mse)\n",
    "    time_l_love.append(uTime)\n",
    "\n",
    "    print(\"Test MSE:\", mse)\n",
    "    print(torch.cuda.memory_allocated() / (1024 ** 2), \"MB allocated\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    ram_after = process.memory_info().rss / (1024 ** 2)\n",
    "    print(\"RAM Delta (MB):\", ram_after - ram_before)\n",
    "\n",
    "# Final results summary\n",
    "print(f\"\\nMSE: {round(statistics.mean(mse_l_love), 5)} ± {round(statistics.stdev(mse_l_love), 5)}\")\n",
    "print(f\"Time: {round(statistics.mean(time_l_love), 2)}s ± {round(statistics.stdev(time_l_love), 2)}s\")\n",
    "\n",
    "# LaTeX-friendly output\n",
    "print(f\"{round(statistics.mean(mse_l_love), 5)}  & ({round(statistics.stdev(mse_l_love), 5)})  & \"\n",
    "      f\"{round(statistics.mean(time_l_love), 2)}  & ({round(statistics.stdev(time_l_love), 2)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c952be-27ac-4038-99eb-e670ec4b8b34",
   "metadata": {},
   "source": [
    "# NGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeed018-ae3b-4b7d-a1cb-990009d4eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "\n",
    "my_batch_size = 320\n",
    "n_replicates = 10\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "mse_l_ngd = []\n",
    "time_l_ngd = []\n",
    "\n",
    "for i in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {i + 1}/{n_replicates} ===\")\n",
    "\n",
    "    # Data split\n",
    "    train_x, train_y, test_x, test_y = splitter(all_x, all_y, n_train=80000, n_test=20000, random_state=i)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "    mem_begin = get_mem()\n",
    "\n",
    "    inducing_points = train_x[::100]\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "        train_x, train_y = train_x.cuda(), train_y.cuda()\n",
    "        test_x, test_y = test_x.cuda(), test_y.cuda()\n",
    "\n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(\n",
    "        model.variational_parameters(), num_data=train_y.size(0), lr=0.01\n",
    "    )\n",
    "\n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.1)\n",
    "\n",
    "    print(\"VRAM Usage:\", torch.cuda.memory_allocated() / (1024 ** 2), \"MB\")\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "    num_epochs = 5\n",
    "    epochs_iter = tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in epochs_iter:\n",
    "        minibatch_iter = tqdm(train_loader, desc=\"Minibatch\", leave=False, position=0)\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            variational_ngd_optimizer.step()\n",
    "            hyperparameter_optimizer.step()\n",
    "\n",
    "    uTime = time.time() - start\n",
    "    print(\"Time:\", uTime)\n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"Memory Usage:\", mem_diff / (1024 ** 2), \"MB\")\n",
    "    log_memory()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    mse = torch.mean((means - test_y.cpu()) ** 2).item()\n",
    "\n",
    "    mse_l_ngd.append(mse)\n",
    "    time_l_ngd.append(uTime)\n",
    "\n",
    "    print(\"Test MSE:\", mse)\n",
    "    print(torch.cuda.memory_allocated() / (1024 ** 2), \"MB allocated\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nMSE: {round(statistics.mean(mse_l_ngd), 5)} ± {round(statistics.stdev(mse_l_ngd), 5)}\")\n",
    "print(f\"Time: {round(statistics.mean(time_l_ngd), 2)}s ± {round(statistics.stdev(time_l_ngd), 2)}s\")\n",
    "\n",
    "# LaTeX-style output\n",
    "print(f\"{round(statistics.mean(mse_l_ngd), 5)}  & ({round(statistics.stdev(mse_l_ngd), 5)})  & \"\n",
    "      f\"{round(statistics.mean(time_l_ngd), 2)}  & ({round(statistics.stdev(time_l_ngd), 2)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a445a-b249-48da-be6e-d5937da43568",
   "metadata": {},
   "source": [
    "# SVGP_CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9020718-e748-44ac-b8e4-7f5813f65419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import trange, tqdm\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "my_batch_size = 3200\n",
    "n_replicates = 10\n",
    "num_epochs = 10\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "# Use every 1000-th point as inducing points (will be re-sampled from train_x each replicate)\n",
    "# Note: 'splitter' should return train_x, train_y, test_x, test_y.\n",
    "    \n",
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.CiqVariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2)\n",
    "        )\n",
    "        # Specific initialization for the 3droad dataset\n",
    "        self.covar_module.base_kernel.initialize(lengthscale=0.01)\n",
    "        print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2), \"MB\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "mse_l_svgpci = []\n",
    "time_l_svgpci = []\n",
    "\n",
    "for rep in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {rep + 1}/{n_replicates} ===\")\n",
    "    \n",
    "    # Split the data for this replicate\n",
    "    train_x, train_y, test_x, test_y = splitter(all_x, all_y, n_train=80000, n_test=20000, random_state=rep)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "    \n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "    \n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    # Define inducing points from the current training set\n",
    "    inducing_points = train_x[::1000]\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "        train_x, train_y = train_x.cuda(), train_y.cuda()\n",
    "        test_x, test_y = test_x.cuda(), test_y.cuda()\n",
    "    \n",
    "    # Setup optimizers: one for the variational parameters and one for hyperparameters.\n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(\n",
    "        model.variational_parameters(), num_data=train_y.size(0), lr=0.1\n",
    "    )\n",
    "    \n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.002)\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    \n",
    "    # Training loop over epochs with minibatch training\n",
    "    start = time.time()\n",
    "    epochs_iter = trange(num_epochs, desc=\"Epoch\")\n",
    "    for epoch in epochs_iter:\n",
    "        minibatch_iter = tqdm(train_loader, desc=\"Minibatch\", leave=False, position=0)\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "            vram_usage()\n",
    "            hyperparameter_optimizer.step()\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "    \n",
    "    uTime = time.time() - start\n",
    "    print(\"Time:\", uTime)\n",
    "    print(\"RAM usage:\", (get_mem() - mem_begin), \"MB\")\n",
    "    log_memory()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    means = observed_pred.mean.cpu()\n",
    "    mse = torch.mean((means - test_y.cpu())**2).item()\n",
    "    \n",
    "    mse_l_svgpci.append(mse)\n",
    "    time_l_svgpci.append(uTime)\n",
    "    \n",
    "    print(torch.cuda.memory_allocated() / (1024 ** 2), \"MB allocated\")\n",
    "    print(\"Test MSE:\", mse)\n",
    "\n",
    "# Summary of results\n",
    "print(f\"\\nMSE: {round(statistics.mean(mse_l_svgpci), 5)} ± {round(statistics.stdev(mse_l_svgpci), 5)}\")\n",
    "print(f\"Time: {round(statistics.mean(time_l_svgpci), 2)}s ± {round(statistics.stdev(time_l_svgpci), 2)}s\")\n",
    "\n",
    "# LaTeX-friendly output (mean & standard deviation)\n",
    "print(f\"{round(statistics.mean(mse_l_svgpci),5)}  & ({round(statistics.stdev(mse_l_svgpci),5)})  & \"\n",
    "      f\"{round(statistics.mean(time_l_svgpci),5)}  & ({round(statistics.stdev(time_l_svgpci),5)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ac0a0-1575-4237-852e-385f23a4f791",
   "metadata": {},
   "source": [
    "# SVGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d8af0-7edb-4c15-80a3-e6eb62eda014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import trange, tqdm\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "my_batch_size = 3200\n",
    "n_replicates = 10\n",
    "num_epochs = 20\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "# Define the GPModel using ApproximateGP with a CholeskyVariationalDistribution\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5))\n",
    "        # Uncomment these prints if needed:\n",
    "        # print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        # Uncomment if using VRAM tracking:\n",
    "        # vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Containers to store the metrics for each replicate\n",
    "mse_l_svgp = []\n",
    "time_l_svgp = []\n",
    "\n",
    "for rep in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate: {rep + 1}/{n_replicates} ===\")\n",
    "    \n",
    "    # Split the data for the current replicate\n",
    "    train_x, train_y, test_x, test_y = splitter(all_x, all_y, n_train=80000, n_test=20000, random_state=rep)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "    \n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "    \n",
    "    # Measure initial RAM\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    # Select inducing points from the current training set\n",
    "    inducing_points = train_x[::100]\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"RAM:\", mem_diff / (1024 ** 2), \"MB\")\n",
    "    \n",
    "    # Set up the optimizer and loss (marginal log likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.001)\n",
    "    \n",
    "    # Here we use the exact marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"RAM:\", mem_diff / (1024 ** 2), \"MB\")\n",
    "    \n",
    "    # Initialize variables to track maximum RAM and VRAM usage during training\n",
    "    max_ram = 0\n",
    "    max_vram = 0\n",
    "    \n",
    "    epochs_iter = trange(num_epochs, desc=\"Epoch\")\n",
    "    start = time.time()\n",
    "    for epoch in epochs_iter:\n",
    "        minibatch_iter = tqdm(train_loader, desc=\"Minibatch\", leave=False, position=0)\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            # Update max RAM usage\n",
    "            max_ram = max(max_ram, get_mem() - mem_begin)\n",
    "            optimizer.step()\n",
    "            if gpu:\n",
    "                max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "            epoch + 1, num_epochs, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item(),\n",
    "            likelihood.noise.item()\n",
    "        ))\n",
    "    uTime = time.time() - start\n",
    "    print(\"Time:\", uTime)\n",
    "    print(\"RAM:\", max_ram, \"MB\")\n",
    "    log_memory()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    means = observed_pred.mean.cpu()\n",
    "    mse = torch.mean((means - test_y.cpu())**2).item()\n",
    "    mse_l_svgp.append(mse)\n",
    "    time_l_svgp.append(uTime)\n",
    "    print(\"Test MSE:\", mse)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nMSE: {round(statistics.mean(mse_l_svgp),5)} ± {round(statistics.stdev(mse_l_svgp),5)}\")\n",
    "print(f\"Time: {round(statistics.mean(time_l_svgp),5)}s ± {round(statistics.stdev(time_l_svgp),5)}s\")\n",
    "\n",
    "# LaTeX-friendly output: mean & (std)\n",
    "print(f\"{round(statistics.mean(mse_l_svgp),5)}  & ({round(statistics.stdev(mse_l_svgp),5)})  & \"\n",
    "      f\"{round(statistics.mean(time_l_svgp),5)}  & ({round(statistics.stdev(time_l_svgp),5)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(f\"\\nMSE: {round(statistics.mean(mse_l_svgp),5)} ± {round(statistics.stdev(mse_l_svgp),5)}\")\n",
    "print(f\"Time: {round(statistics.mean(time_l_svgp),5)}s ± {round(statistics.stdev(time_l_svgp),5)}s\")\n",
    "\n",
    "# LaTeX-friendly output: mean & (std)\n",
    "print(f\"{round(statistics.mean(mse_l_svgp),5)}  & ({round(statistics.stdev(mse_l_svgp),5)})  & \"\n",
    "      f\"{round(statistics.mean(time_l_svgp),5)}  & ({round(statistics.stdev(time_l_svgp),5)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4ccda-39fa-4f20-8b0c-eb815d86c99f",
   "metadata": {},
   "source": [
    "# SKI - Can only handle up to 40,000 datapoints before running out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85087eb9-8b3b-4dad-9125-4ebefd372189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = None\n",
    "likelihood = None\n",
    "\n",
    "if gpu:\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24869027-c361-400b-a950-33cdab17cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "n_replicates = 10\n",
    "training_iterations = 30\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "# Define the Exact GP Regression Model using SKI\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        # Choose grid size for SKI\n",
    "        grid_size = gpytorch.utils.grid.choose_grid_size(train_x, 2)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5)),\n",
    "                grid_size=grid_size, num_dims=2\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Containers for metrics\n",
    "mse_l_ski = []\n",
    "time_l_ski = []\n",
    "\n",
    "for rep in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {rep + 1}/{n_replicates} ===\")\n",
    "    # Split the data for SKI (assumes splitter returns train_x_ski, train_y_ski, test_x, test_y)\n",
    "    train_x_ski, train_y_ski, test_x, test_y = splitter(all_x, all_y, n_train=40000, n_test=20000, random_state=rep)\n",
    "    \n",
    "    # Track RAM usage (get_mem() is assumed to be defined elsewhere)\n",
    "    mem_begin = get_mem()\n",
    "    max_vram = 0\n",
    "\n",
    "    # Initialize model and likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x_ski, train_y_ski, likelihood)\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "        train_x_ski, train_y_ski, test_x, test_y = train_x_ski.cuda(), train_y_ski.cuda(), test_x.cuda(), test_y.cuda()\n",
    "    \n",
    "    print(\"Initial RAM:\", (get_mem() - mem_begin) / (1024 ** 2), \"MB\")\n",
    "    \n",
    "    # Set to training mode and initialize optimizer and loss\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    if gpu:\n",
    "        mll = mll.cuda()\n",
    "    \n",
    "    print(\"RAM after initialization:\", (get_mem() - mem_begin) / (1024 ** 2), \"MB\")\n",
    "    \n",
    "    # Training loop with tqdm loss updates\n",
    "    start = time.time()\n",
    "    tbar = tqdm(range(training_iterations), desc=\"Train\", leave=False, position=0)\n",
    "    for _ in tbar:\n",
    "        optimizer.zero_grad()\n",
    "        if gpu:\n",
    "            max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        output = model(train_x_ski)\n",
    "        loss = -mll(output, train_y_ski)\n",
    "        loss.backward()\n",
    "        if gpu:\n",
    "            max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        optimizer.step()\n",
    "        tbar.set_postfix(loss=loss.item())\n",
    "    uTime = time.time() - start\n",
    "    print(\"Training Time:\", uTime)\n",
    "    print(\"RAM Usage:\", (get_mem() - mem_begin), \"MB\")\n",
    "    log_memory()\n",
    "    \n",
    "    # Evaluation in prior mode\n",
    "    model.eval()\n",
    "    with gpytorch.settings.prior_mode():\n",
    "        output = model(test_x)\n",
    "    means = output.mean.cpu()\n",
    "    mse = torch.mean((means - test_y.cpu())**2).item()\n",
    "    mse_l_ski.append(mse)\n",
    "    time_l_ski.append(uTime)\n",
    "    print(\"Test MSE:\", mse)\n",
    "\n",
    "# Summary of results\n",
    "print(f\"\\nMSE: {round(statistics.mean(mse_l_ski),5)} ± {round(statistics.stdev(mse_l_ski),5)}\")\n",
    "print(f\"Time: {round(statistics.mean(time_l_ski),5)}s ± {round(statistics.stdev(time_l_ski),5)}s\")\n",
    "print(f\"{round(statistics.mean(mse_l_ski),5)}  & ({round(statistics.stdev(mse_l_ski),5)})  & {round(statistics.mean(time_l_ski),5)}  & ({round(statistics.stdev(time_l_ski),5)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc3f645-92de-4bed-86da-6c77d005c177",
   "metadata": {},
   "source": [
    "# VNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aecce4-4e59-48d5-a6ab-b89d65c77184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Replicate: 0 ===\n",
      "Memory before training: 0.0 MB\n",
      "Step 1\n",
      "Step 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\faiss\\contrib\\torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  x.storage().data_ptr() + x.storage_offset() * 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21\n",
      "Step 22\n",
      "Step 23\n",
      "Step 3\n",
      "Time since start: 1742979758.891962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 127.14it/s, loss=1.42]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.21it/s, loss=1.4] \n",
      "100%|██████████| 64/64 [00:00<00:00, 176.86it/s, loss=1.27]\n",
      "100%|██████████| 64/64 [00:00<00:00, 194.59it/s, loss=1.24]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.77it/s, loss=1.08]\n",
      "100%|██████████| 64/64 [00:00<00:00, 191.66it/s, loss=0.922]\n",
      "100%|██████████| 64/64 [00:00<00:00, 191.68it/s, loss=0.0861]\n",
      "100%|██████████| 64/64 [00:00<00:00, 185.52it/s, loss=-0.37] \n",
      "100%|██████████| 64/64 [00:00<00:00, 192.98it/s, loss=-0.358]\n",
      "100%|██████████| 64/64 [00:00<00:00, 189.93it/s, loss=-0.222]\n",
      "100%|██████████| 64/64 [00:00<00:00, 193.13it/s, loss=-0.373]\n",
      "100%|██████████| 64/64 [00:00<00:00, 196.09it/s, loss=-0.683]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.78it/s, loss=-0.643]\n",
      "100%|██████████| 64/64 [00:00<00:00, 189.99it/s, loss=-0.723]\n",
      "100%|██████████| 64/64 [00:00<00:00, 183.93it/s, loss=-0.719]\n",
      "100%|██████████| 64/64 [00:00<00:00, 190.81it/s, loss=-0.682]\n",
      "100%|██████████| 64/64 [00:00<00:00, 192.48it/s, loss=-0.76] \n",
      "100%|██████████| 64/64 [00:00<00:00, 182.65it/s, loss=-0.749]\n",
      "100%|██████████| 64/64 [00:00<00:00, 186.94it/s, loss=-0.764]\n",
      "100%|██████████| 64/64 [00:00<00:00, 181.12it/s, loss=-0.9]  \n",
      "100%|██████████| 64/64 [00:00<00:00, 178.80it/s, loss=-0.657]\n",
      "100%|██████████| 64/64 [00:00<00:00, 183.44it/s, loss=-0.771]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.49it/s, loss=-0.619]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.04it/s, loss=-0.746]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.77it/s, loss=-0.773]\n",
      "100%|██████████| 64/64 [00:00<00:00, 175.85it/s, loss=-0.775]\n",
      "100%|██████████| 64/64 [00:00<00:00, 194.01it/s, loss=-0.814]\n",
      "100%|██████████| 64/64 [00:00<00:00, 182.65it/s, loss=-0.817]\n",
      "100%|██████████| 64/64 [00:00<00:00, 182.86it/s, loss=-0.788]\n",
      "100%|██████████| 64/64 [00:00<00:00, 182.13it/s, loss=-0.769]\n",
      "Epoch: 100%|██████████| 30/30 [00:10<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 10.50261640548706\n",
      "[GPU] Used: 3539.25 MB\n",
      "[PyTorch] Max Allocated: 245.70 MB\n",
      "[System RAM] Used: 19.74 GB\n",
      "RAM: 657.1015625 MB\n",
      "Test MSE: 0.7416879534721375\n",
      "Test MAE: 0.7416879534721375\n",
      "\n",
      "=== Replicate: 1 ===\n",
      "Memory before training: 0.0 MB\n",
      "Step 1\n",
      "Step 2\n",
      "Step 21\n",
      "Step 22\n",
      "Step 23\n",
      "Step 3\n",
      "Time since start: 1742979805.1846957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 179.04it/s, loss=1.43]\n",
      "100%|██████████| 64/64 [00:00<00:00, 190.32it/s, loss=1.34]\n",
      "100%|██████████| 64/64 [00:00<00:00, 186.29it/s, loss=1.36]\n",
      "100%|██████████| 64/64 [00:00<00:00, 177.64it/s, loss=1.16]\n",
      "100%|██████████| 64/64 [00:00<00:00, 181.93it/s, loss=1.02]\n",
      "100%|██████████| 64/64 [00:00<00:00, 183.78it/s, loss=0.767]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.67it/s, loss=0.148] \n",
      "100%|██████████| 64/64 [00:00<00:00, 186.28it/s, loss=-0.352]\n",
      "100%|██████████| 64/64 [00:00<00:00, 185.39it/s, loss=-0.315]\n",
      "100%|██████████| 64/64 [00:00<00:00, 192.29it/s, loss=-0.254]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.10it/s, loss=-0.434]\n",
      "100%|██████████| 64/64 [00:00<00:00, 183.92it/s, loss=-0.759]\n",
      "100%|██████████| 64/64 [00:00<00:00, 187.77it/s, loss=-0.628]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.10it/s, loss=-0.702]\n",
      "100%|██████████| 64/64 [00:00<00:00, 185.78it/s, loss=-0.74] \n",
      "100%|██████████| 64/64 [00:00<00:00, 188.02it/s, loss=-0.699]\n",
      "100%|██████████| 64/64 [00:00<00:00, 190.19it/s, loss=-0.66] \n",
      "100%|██████████| 64/64 [00:00<00:00, 190.60it/s, loss=-0.812]\n",
      "100%|██████████| 64/64 [00:00<00:00, 192.03it/s, loss=-0.588]\n",
      "100%|██████████| 64/64 [00:00<00:00, 189.95it/s, loss=-0.685]\n",
      "100%|██████████| 64/64 [00:00<00:00, 186.91it/s, loss=-0.707]\n",
      "100%|██████████| 64/64 [00:00<00:00, 193.02it/s, loss=-0.641]\n",
      "100%|██████████| 64/64 [00:00<00:00, 187.06it/s, loss=-0.77] \n",
      "100%|██████████| 64/64 [00:00<00:00, 190.11it/s, loss=-0.751]\n",
      "100%|██████████| 64/64 [00:00<00:00, 176.26it/s, loss=-0.745]\n",
      "100%|██████████| 64/64 [00:00<00:00, 180.85it/s, loss=-0.732]\n",
      "100%|██████████| 64/64 [00:00<00:00, 190.48it/s, loss=-0.777]\n",
      "100%|██████████| 64/64 [00:00<00:00, 187.88it/s, loss=-0.852]\n",
      "100%|██████████| 64/64 [00:00<00:00, 186.35it/s, loss=-0.877]\n",
      "100%|██████████| 64/64 [00:00<00:00, 187.61it/s, loss=-0.852]\n",
      "Epoch: 100%|██████████| 30/30 [00:10<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 10.33055830001831\n",
      "[GPU] Used: 3538.87 MB\n",
      "[PyTorch] Max Allocated: 245.70 MB\n",
      "[System RAM] Used: 19.74 GB\n",
      "RAM: 255.6640625 MB\n",
      "Test MSE: 0.7424277067184448\n",
      "Test MAE: 0.7424277067184448\n",
      "\n",
      "=== Replicate: 2 ===\n",
      "Memory before training: 0.0 MB\n",
      "Step 1\n",
      "Step 2\n",
      "Step 21\n",
      "Step 22\n",
      "Step 23\n",
      "Step 3\n",
      "Time since start: 1742979850.7772772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 186.43it/s, loss=1.42]\n",
      "100%|██████████| 64/64 [00:00<00:00, 189.66it/s, loss=1.4] \n",
      "100%|██████████| 64/64 [00:00<00:00, 186.59it/s, loss=1.33]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.37it/s, loss=1.2] \n",
      "100%|██████████| 64/64 [00:00<00:00, 187.83it/s, loss=1.1]  \n",
      "100%|██████████| 64/64 [00:00<00:00, 184.92it/s, loss=0.838]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.99it/s, loss=-0.00252]\n",
      "100%|██████████| 64/64 [00:00<00:00, 186.02it/s, loss=-0.304]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.53it/s, loss=-0.333]\n",
      "100%|██████████| 64/64 [00:00<00:00, 186.52it/s, loss=-0.255]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.22it/s, loss=-0.406]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.88it/s, loss=-0.838]\n",
      "100%|██████████| 64/64 [00:00<00:00, 185.64it/s, loss=-0.667]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.06it/s, loss=-0.824]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.39it/s, loss=-0.763]\n",
      "100%|██████████| 64/64 [00:00<00:00, 183.39it/s, loss=-0.739]\n",
      "100%|██████████| 64/64 [00:00<00:00, 186.79it/s, loss=-0.796]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.25it/s, loss=-0.744]\n",
      "100%|██████████| 64/64 [00:00<00:00, 185.47it/s, loss=-0.763]\n",
      "100%|██████████| 64/64 [00:00<00:00, 185.54it/s, loss=-0.818]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.90it/s, loss=-0.744]\n",
      "100%|██████████| 64/64 [00:00<00:00, 181.78it/s, loss=-0.776]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.07it/s, loss=-0.819]\n",
      "100%|██████████| 64/64 [00:00<00:00, 183.11it/s, loss=-0.881]\n",
      "100%|██████████| 64/64 [00:00<00:00, 183.63it/s, loss=-0.8]  \n",
      "100%|██████████| 64/64 [00:00<00:00, 190.34it/s, loss=-0.725]\n",
      "100%|██████████| 64/64 [00:00<00:00, 187.99it/s, loss=-0.837]\n",
      "100%|██████████| 64/64 [00:00<00:00, 185.06it/s, loss=-0.809]\n",
      "100%|██████████| 64/64 [00:00<00:00, 189.35it/s, loss=-0.82] \n",
      "100%|██████████| 64/64 [00:00<00:00, 187.16it/s, loss=-0.974]\n",
      "Epoch: 100%|██████████| 30/30 [00:10<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 10.350124597549438\n",
      "[GPU] Used: 3534.79 MB\n",
      "[PyTorch] Max Allocated: 245.70 MB\n",
      "[System RAM] Used: 19.71 GB\n",
      "RAM: 255.859375 MB\n",
      "Test MSE: 0.741576611995697\n",
      "Test MAE: 0.741576611995697\n",
      "\n",
      "=== Replicate: 3 ===\n",
      "Memory before training: 0.0 MB\n",
      "Step 1\n",
      "Step 2\n",
      "Step 21\n",
      "Step 22\n",
      "Step 23\n",
      "Step 3\n",
      "Time since start: 1742979896.2889376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 188.56it/s, loss=1.39]\n",
      "100%|██████████| 64/64 [00:00<00:00, 193.41it/s, loss=1.39]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.06it/s, loss=1.29]\n",
      "100%|██████████| 64/64 [00:00<00:00, 189.93it/s, loss=1.23]\n",
      "100%|██████████| 64/64 [00:00<00:00, 194.94it/s, loss=1.1] \n",
      "100%|██████████| 64/64 [00:00<00:00, 194.09it/s, loss=0.68] \n",
      "100%|██████████| 64/64 [00:00<00:00, 188.88it/s, loss=0.15]  \n",
      "100%|██████████| 64/64 [00:00<00:00, 192.37it/s, loss=-0.258]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.65it/s, loss=-0.289]\n",
      "100%|██████████| 64/64 [00:00<00:00, 189.80it/s, loss=-0.252]\n",
      "100%|██████████| 64/64 [00:00<00:00, 191.38it/s, loss=-0.419]\n",
      "100%|██████████| 64/64 [00:00<00:00, 185.01it/s, loss=-0.807]\n",
      "100%|██████████| 64/64 [00:00<00:00, 182.14it/s, loss=-0.646]\n",
      "100%|██████████| 64/64 [00:00<00:00, 181.99it/s, loss=-0.759]\n",
      "100%|██████████| 64/64 [00:00<00:00, 179.12it/s, loss=-0.574]\n",
      "100%|██████████| 64/64 [00:00<00:00, 178.78it/s, loss=-0.551]\n",
      "100%|██████████| 64/64 [00:00<00:00, 180.63it/s, loss=-0.712]\n",
      "100%|██████████| 64/64 [00:00<00:00, 179.95it/s, loss=-0.811]\n",
      "100%|██████████| 64/64 [00:00<00:00, 181.61it/s, loss=-0.846]\n",
      "100%|██████████| 64/64 [00:00<00:00, 180.09it/s, loss=-0.829]\n",
      "100%|██████████| 64/64 [00:00<00:00, 175.44it/s, loss=-0.699]\n",
      "100%|██████████| 64/64 [00:00<00:00, 180.16it/s, loss=-0.833]\n",
      "100%|██████████| 64/64 [00:00<00:00, 178.15it/s, loss=-0.811]\n",
      "100%|██████████| 64/64 [00:00<00:00, 176.44it/s, loss=-0.752]\n",
      "100%|██████████| 64/64 [00:00<00:00, 179.22it/s, loss=-0.812]\n",
      "100%|██████████| 64/64 [00:00<00:00, 181.89it/s, loss=-0.73] \n",
      "100%|██████████| 64/64 [00:00<00:00, 181.61it/s, loss=-0.789]\n",
      "100%|██████████| 64/64 [00:00<00:00, 182.97it/s, loss=-0.856]\n",
      "100%|██████████| 64/64 [00:00<00:00, 166.55it/s, loss=-0.797]\n",
      "100%|██████████| 64/64 [00:00<00:00, 175.49it/s, loss=-0.837]\n",
      "Epoch: 100%|██████████| 30/30 [00:10<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 10.512325763702393\n",
      "[GPU] Used: 3504.29 MB\n",
      "[PyTorch] Max Allocated: 245.70 MB\n",
      "[System RAM] Used: 19.77 GB\n",
      "RAM: 255.80078125 MB\n",
      "Test MSE: 0.7372857332229614\n",
      "Test MAE: 0.7372857332229614\n",
      "\n",
      "=== Replicate: 4 ===\n",
      "Memory before training: 0.0 MB\n",
      "Step 1\n",
      "Step 2\n",
      "Step 21\n",
      "Step 22\n",
      "Step 23\n",
      "Step 3\n",
      "Time since start: 1742979941.9534883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 177.85it/s, loss=1.35]\n",
      "100%|██████████| 64/64 [00:00<00:00, 179.32it/s, loss=1.32]\n",
      "100%|██████████| 64/64 [00:00<00:00, 175.30it/s, loss=1.31]\n",
      "100%|██████████| 64/64 [00:00<00:00, 179.32it/s, loss=1.13]\n",
      "100%|██████████| 64/64 [00:00<00:00, 177.77it/s, loss=1.06]\n",
      "100%|██████████| 64/64 [00:00<00:00, 179.76it/s, loss=0.643]\n",
      "100%|██████████| 64/64 [00:00<00:00, 182.97it/s, loss=-0.0847] \n",
      "100%|██████████| 64/64 [00:00<00:00, 178.61it/s, loss=-0.448]\n",
      "100%|██████████| 64/64 [00:00<00:00, 177.68it/s, loss=-0.392]\n",
      "100%|██████████| 64/64 [00:00<00:00, 180.39it/s, loss=-0.325]\n",
      "100%|██████████| 64/64 [00:00<00:00, 175.40it/s, loss=-0.312]\n",
      "100%|██████████| 64/64 [00:00<00:00, 181.70it/s, loss=-0.837]\n",
      "100%|██████████| 64/64 [00:00<00:00, 178.85it/s, loss=-0.578]\n",
      "100%|██████████| 64/64 [00:00<00:00, 180.82it/s, loss=-0.668]\n",
      "100%|██████████| 64/64 [00:00<00:00, 173.99it/s, loss=-0.74] \n",
      "100%|██████████| 64/64 [00:00<00:00, 176.34it/s, loss=-0.661]\n",
      "100%|██████████| 64/64 [00:00<00:00, 178.08it/s, loss=-0.76] \n",
      "100%|██████████| 64/64 [00:00<00:00, 178.64it/s, loss=-0.789]\n",
      "100%|██████████| 64/64 [00:00<00:00, 177.92it/s, loss=-0.801]\n",
      "100%|██████████| 64/64 [00:00<00:00, 179.80it/s, loss=-0.795]\n",
      "100%|██████████| 64/64 [00:00<00:00, 178.41it/s, loss=-0.779]\n",
      "100%|██████████| 64/64 [00:00<00:00, 180.49it/s, loss=-0.709]\n",
      "100%|██████████| 64/64 [00:00<00:00, 179.43it/s, loss=-0.807]\n",
      "100%|██████████| 64/64 [00:00<00:00, 177.34it/s, loss=-0.742]\n",
      "100%|██████████| 64/64 [00:00<00:00, 177.28it/s, loss=-0.644]\n",
      "100%|██████████| 64/64 [00:00<00:00, 179.89it/s, loss=-0.701]\n",
      "100%|██████████| 64/64 [00:00<00:00, 177.57it/s, loss=-0.736]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.14it/s, loss=-0.743]\n",
      "100%|██████████| 64/64 [00:00<00:00, 190.24it/s, loss=-0.791]\n",
      "100%|██████████| 64/64 [00:00<00:00, 183.93it/s, loss=-0.875]\n",
      "Epoch: 100%|██████████| 30/30 [00:10<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 10.744665384292603\n",
      "[GPU] Used: 3503.26 MB\n",
      "[PyTorch] Max Allocated: 245.70 MB\n",
      "[System RAM] Used: 19.62 GB\n",
      "RAM: 255.8125 MB\n",
      "Test MSE: 0.7406355738639832\n",
      "Test MAE: 0.7406355738639832\n",
      "\n",
      "=== Replicate: 5 ===\n",
      "Memory before training: 0.0 MB\n",
      "Step 1\n",
      "Step 2\n",
      "Step 21\n",
      "Step 22\n",
      "Step 23\n",
      "Step 3\n",
      "Time since start: 1742979990.784418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 141.26it/s, loss=1.47]\n",
      "100%|██████████| 64/64 [00:00<00:00, 126.54it/s, loss=1.38]\n",
      "100%|██████████| 64/64 [00:00<00:00, 144.97it/s, loss=1.34]\n",
      "100%|██████████| 64/64 [00:00<00:00, 143.22it/s, loss=1.2] \n",
      "100%|██████████| 64/64 [00:00<00:00, 150.42it/s, loss=1.12] \n",
      "100%|██████████| 64/64 [00:00<00:00, 153.19it/s, loss=0.738]\n",
      "100%|██████████| 64/64 [00:00<00:00, 146.75it/s, loss=-0.00962]\n",
      "100%|██████████| 64/64 [00:00<00:00, 166.29it/s, loss=-0.316]\n",
      "100%|██████████| 64/64 [00:00<00:00, 170.09it/s, loss=-0.379]\n",
      "100%|██████████| 64/64 [00:00<00:00, 177.10it/s, loss=-0.272]\n",
      "100%|██████████| 64/64 [00:00<00:00, 161.36it/s, loss=-0.337]\n",
      "100%|██████████| 64/64 [00:00<00:00, 174.70it/s, loss=-0.827]\n",
      "100%|██████████| 64/64 [00:00<00:00, 161.65it/s, loss=-0.695]\n",
      "100%|██████████| 64/64 [00:00<00:00, 164.13it/s, loss=-0.649]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.81it/s, loss=-0.702]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.59it/s, loss=-0.749]\n",
      "100%|██████████| 64/64 [00:00<00:00, 166.21it/s, loss=-0.616]\n",
      "100%|██████████| 64/64 [00:00<00:00, 164.51it/s, loss=-0.809]\n",
      "100%|██████████| 64/64 [00:00<00:00, 168.21it/s, loss=-0.863]\n",
      "100%|██████████| 64/64 [00:00<00:00, 163.59it/s, loss=-0.846]\n",
      "100%|██████████| 64/64 [00:00<00:00, 179.94it/s, loss=-0.684]\n",
      "100%|██████████| 64/64 [00:00<00:00, 173.03it/s, loss=-0.695]\n",
      "100%|██████████| 64/64 [00:00<00:00, 173.74it/s, loss=-0.785]\n",
      "100%|██████████| 64/64 [00:00<00:00, 170.87it/s, loss=-0.761]\n",
      "100%|██████████| 64/64 [00:00<00:00, 171.51it/s, loss=-0.772]\n",
      "100%|██████████| 64/64 [00:00<00:00, 182.15it/s, loss=-0.72] \n",
      "100%|██████████| 64/64 [00:00<00:00, 179.57it/s, loss=-0.875]\n",
      "100%|██████████| 64/64 [00:00<00:00, 172.12it/s, loss=-0.787]\n",
      "100%|██████████| 64/64 [00:00<00:00, 172.71it/s, loss=-0.875]\n",
      "100%|██████████| 64/64 [00:00<00:00, 174.69it/s, loss=-0.932]\n",
      "Epoch: 100%|██████████| 30/30 [00:11<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 11.735092639923096\n",
      "[GPU] Used: 4385.17 MB\n",
      "[PyTorch] Max Allocated: 245.70 MB\n",
      "[System RAM] Used: 20.18 GB\n",
      "RAM: 255.796875 MB\n",
      "Test MSE: 0.7341556549072266\n",
      "Test MAE: 0.7341556549072266\n",
      "\n",
      "=== Replicate: 6 ===\n",
      "Memory before training: 0.0 MB\n",
      "Step 1\n",
      "Step 2\n",
      "Step 21\n",
      "Step 22\n",
      "Step 23\n",
      "Step 3\n",
      "Time since start: 1742980037.263741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 181.90it/s, loss=1.46]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.47it/s, loss=1.38]\n",
      "100%|██████████| 64/64 [00:00<00:00, 180.23it/s, loss=1.28]\n",
      "100%|██████████| 64/64 [00:00<00:00, 182.76it/s, loss=1.19]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.94it/s, loss=1.03]\n",
      "100%|██████████| 64/64 [00:00<00:00, 183.89it/s, loss=0.71] \n",
      "100%|██████████| 64/64 [00:00<00:00, 183.57it/s, loss=0.0926] \n",
      "100%|██████████| 64/64 [00:00<00:00, 183.43it/s, loss=-0.378]\n",
      "100%|██████████| 64/64 [00:00<00:00, 175.76it/s, loss=-0.357]\n",
      "100%|██████████| 64/64 [00:00<00:00, 177.93it/s, loss=-0.335]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.38it/s, loss=-0.307]\n",
      "100%|██████████| 64/64 [00:00<00:00, 183.44it/s, loss=-0.727]\n",
      "100%|██████████| 64/64 [00:00<00:00, 178.23it/s, loss=-0.672]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.21it/s, loss=-0.771]\n",
      "100%|██████████| 64/64 [00:00<00:00, 182.83it/s, loss=-0.671]\n",
      "100%|██████████| 64/64 [00:00<00:00, 187.23it/s, loss=-0.903]\n",
      "100%|██████████| 64/64 [00:00<00:00, 185.63it/s, loss=-0.687]\n",
      "100%|██████████| 64/64 [00:00<00:00, 187.48it/s, loss=-0.629]\n",
      "100%|██████████| 64/64 [00:00<00:00, 183.23it/s, loss=-0.804]\n",
      "100%|██████████| 64/64 [00:00<00:00, 187.21it/s, loss=-0.734]\n",
      "100%|██████████| 64/64 [00:00<00:00, 180.63it/s, loss=-0.67] \n",
      "100%|██████████| 64/64 [00:00<00:00, 180.91it/s, loss=-0.791]\n",
      "100%|██████████| 64/64 [00:00<00:00, 181.83it/s, loss=-0.757]\n",
      "100%|██████████| 64/64 [00:00<00:00, 183.05it/s, loss=-0.77] \n",
      "100%|██████████| 64/64 [00:00<00:00, 174.37it/s, loss=-0.67] \n",
      "100%|██████████| 64/64 [00:00<00:00, 187.17it/s, loss=-0.818]\n",
      "100%|██████████| 64/64 [00:00<00:00, 185.37it/s, loss=-0.783]\n",
      "100%|██████████| 64/64 [00:00<00:00, 186.71it/s, loss=-0.815]\n",
      "100%|██████████| 64/64 [00:00<00:00, 183.46it/s, loss=-0.794]\n",
      "100%|██████████| 64/64 [00:00<00:00, 176.67it/s, loss=-0.89] \n",
      "Epoch: 100%|██████████| 30/30 [00:10<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 10.54262375831604\n",
      "[GPU] Used: 4384.27 MB\n",
      "[PyTorch] Max Allocated: 245.70 MB\n",
      "[System RAM] Used: 19.70 GB\n",
      "RAM: 255.80078125 MB\n",
      "Test MSE: 0.7324655652046204\n",
      "Test MAE: 0.7324655652046204\n",
      "\n",
      "=== Replicate: 7 ===\n",
      "Memory before training: 0.0 MB\n",
      "Step 1\n",
      "Step 2\n",
      "Step 21\n",
      "Step 22\n",
      "Step 23\n",
      "Step 3\n",
      "Time since start: 1742980081.9121838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 192.68it/s, loss=1.38]\n",
      "100%|██████████| 64/64 [00:00<00:00, 196.13it/s, loss=1.29]\n",
      "100%|██████████| 64/64 [00:00<00:00, 199.48it/s, loss=1.31]\n",
      "100%|██████████| 64/64 [00:00<00:00, 190.63it/s, loss=1.17]\n",
      "100%|██████████| 64/64 [00:00<00:00, 191.78it/s, loss=1.16]\n",
      "100%|██████████| 64/64 [00:00<00:00, 194.28it/s, loss=0.693]\n",
      "100%|██████████| 64/64 [00:00<00:00, 190.10it/s, loss=0.0808]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.50it/s, loss=-0.304]\n",
      "100%|██████████| 64/64 [00:00<00:00, 190.39it/s, loss=-0.378]\n",
      "100%|██████████| 64/64 [00:00<00:00, 187.38it/s, loss=-0.173]\n",
      "100%|██████████| 64/64 [00:00<00:00, 187.60it/s, loss=-0.387]\n",
      "100%|██████████| 64/64 [00:00<00:00, 194.10it/s, loss=-0.807]\n",
      "100%|██████████| 64/64 [00:00<00:00, 190.39it/s, loss=-0.631]\n",
      "100%|██████████| 64/64 [00:00<00:00, 191.61it/s, loss=-0.767]\n",
      "100%|██████████| 64/64 [00:00<00:00, 192.70it/s, loss=-0.819]\n",
      "100%|██████████| 64/64 [00:00<00:00, 196.41it/s, loss=-0.563]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.95it/s, loss=-0.713]\n",
      "100%|██████████| 64/64 [00:00<00:00, 176.79it/s, loss=-0.842]\n",
      "100%|██████████| 64/64 [00:00<00:00, 192.99it/s, loss=-0.771]\n",
      "100%|██████████| 64/64 [00:00<00:00, 201.06it/s, loss=-0.692]\n",
      "100%|██████████| 64/64 [00:00<00:00, 192.01it/s, loss=-0.763]\n",
      "100%|██████████| 64/64 [00:00<00:00, 192.71it/s, loss=-0.724]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.24it/s, loss=-0.683]\n",
      "100%|██████████| 64/64 [00:00<00:00, 194.76it/s, loss=-0.718]\n",
      "100%|██████████| 64/64 [00:00<00:00, 189.90it/s, loss=-0.802]\n",
      "100%|██████████| 64/64 [00:00<00:00, 180.48it/s, loss=-0.837]\n",
      "100%|██████████| 64/64 [00:00<00:00, 192.11it/s, loss=-0.738]\n",
      "100%|██████████| 64/64 [00:00<00:00, 191.50it/s, loss=-0.803]\n",
      "100%|██████████| 64/64 [00:00<00:00, 193.65it/s, loss=-0.737]\n",
      "100%|██████████| 64/64 [00:00<00:00, 194.75it/s, loss=-0.886]\n",
      "Epoch: 100%|██████████| 30/30 [00:10<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 10.07969045639038\n",
      "[GPU] Used: 4363.63 MB\n",
      "[PyTorch] Max Allocated: 245.70 MB\n",
      "[System RAM] Used: 19.76 GB\n",
      "RAM: 255.796875 MB\n",
      "Test MSE: 0.7241892218589783\n",
      "Test MAE: 0.7241892218589783\n",
      "\n",
      "=== Replicate: 8 ===\n",
      "Memory before training: 0.0 MB\n",
      "Step 1\n",
      "Step 2\n",
      "Step 21\n",
      "Step 22\n",
      "Step 23\n",
      "Step 3\n",
      "Time since start: 1742980126.808674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 183.38it/s, loss=1.41]\n",
      "100%|██████████| 64/64 [00:00<00:00, 192.40it/s, loss=1.39]\n",
      "100%|██████████| 64/64 [00:00<00:00, 199.30it/s, loss=1.34]\n",
      "100%|██████████| 64/64 [00:00<00:00, 194.75it/s, loss=1.25]\n",
      "100%|██████████| 64/64 [00:00<00:00, 196.84it/s, loss=0.997]\n",
      "100%|██████████| 64/64 [00:00<00:00, 195.44it/s, loss=0.661]\n",
      "100%|██████████| 64/64 [00:00<00:00, 195.13it/s, loss=0.0519]\n",
      "100%|██████████| 64/64 [00:00<00:00, 199.39it/s, loss=-0.343]\n",
      "100%|██████████| 64/64 [00:00<00:00, 192.24it/s, loss=-0.285]\n",
      "100%|██████████| 64/64 [00:00<00:00, 195.43it/s, loss=-0.305]\n",
      "100%|██████████| 64/64 [00:00<00:00, 195.47it/s, loss=-0.356]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.13it/s, loss=-0.762]\n",
      "100%|██████████| 64/64 [00:00<00:00, 190.31it/s, loss=-0.68] \n",
      "100%|██████████| 64/64 [00:00<00:00, 193.36it/s, loss=-0.748]\n",
      "100%|██████████| 64/64 [00:00<00:00, 192.90it/s, loss=-0.654]\n",
      "100%|██████████| 64/64 [00:00<00:00, 186.00it/s, loss=-0.74] \n",
      "100%|██████████| 64/64 [00:00<00:00, 192.01it/s, loss=-0.631]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.03it/s, loss=-0.745]\n",
      "100%|██████████| 64/64 [00:00<00:00, 192.13it/s, loss=-0.742]\n",
      "100%|██████████| 64/64 [00:00<00:00, 193.76it/s, loss=-0.875]\n",
      "100%|██████████| 64/64 [00:00<00:00, 187.83it/s, loss=-0.82] \n",
      "100%|██████████| 64/64 [00:00<00:00, 181.27it/s, loss=-0.728]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.31it/s, loss=-0.703]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.92it/s, loss=-0.748]\n",
      "100%|██████████| 64/64 [00:00<00:00, 189.17it/s, loss=-0.768]\n",
      "100%|██████████| 64/64 [00:00<00:00, 189.05it/s, loss=-0.698]\n",
      "100%|██████████| 64/64 [00:00<00:00, 194.02it/s, loss=-0.72] \n",
      "100%|██████████| 64/64 [00:00<00:00, 191.38it/s, loss=-0.748]\n",
      "100%|██████████| 64/64 [00:00<00:00, 192.67it/s, loss=-0.853]\n",
      "100%|██████████| 64/64 [00:00<00:00, 189.75it/s, loss=-0.908]\n",
      "Epoch: 100%|██████████| 30/30 [00:10<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 10.065503597259521\n",
      "[GPU] Used: 4328.72 MB\n",
      "[PyTorch] Max Allocated: 245.70 MB\n",
      "[System RAM] Used: 19.64 GB\n",
      "RAM: 255.796875 MB\n",
      "Test MSE: 0.7310738563537598\n",
      "Test MAE: 0.7310738563537598\n",
      "\n",
      "=== Replicate: 9 ===\n",
      "Memory before training: 0.0 MB\n",
      "Step 1\n",
      "Step 2\n",
      "Step 21\n",
      "Step 22\n",
      "Step 23\n",
      "Step 3\n",
      "Time since start: 1742980171.4772363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 189.80it/s, loss=1.43]\n",
      "100%|██████████| 64/64 [00:00<00:00, 195.61it/s, loss=1.35]\n",
      "100%|██████████| 64/64 [00:00<00:00, 195.30it/s, loss=1.38]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.81it/s, loss=1.25]\n",
      "100%|██████████| 64/64 [00:00<00:00, 195.11it/s, loss=1.01]\n",
      "100%|██████████| 64/64 [00:00<00:00, 193.41it/s, loss=0.689]\n",
      "100%|██████████| 64/64 [00:00<00:00, 194.38it/s, loss=0.013]   \n",
      "100%|██████████| 64/64 [00:00<00:00, 190.11it/s, loss=-0.282]\n",
      "100%|██████████| 64/64 [00:00<00:00, 185.08it/s, loss=-0.291]\n",
      "100%|██████████| 64/64 [00:00<00:00, 193.39it/s, loss=-0.29] \n",
      "100%|██████████| 64/64 [00:00<00:00, 191.26it/s, loss=-0.371]\n",
      "100%|██████████| 64/64 [00:00<00:00, 185.25it/s, loss=-0.898]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.28it/s, loss=-0.732]\n",
      "100%|██████████| 64/64 [00:00<00:00, 193.04it/s, loss=-0.618]\n",
      "100%|██████████| 64/64 [00:00<00:00, 187.96it/s, loss=-0.659]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.95it/s, loss=-0.688]\n",
      "100%|██████████| 64/64 [00:00<00:00, 190.90it/s, loss=-0.631]\n",
      "100%|██████████| 64/64 [00:00<00:00, 189.31it/s, loss=-0.746]\n",
      "100%|██████████| 64/64 [00:00<00:00, 190.64it/s, loss=-0.706]\n",
      "100%|██████████| 64/64 [00:00<00:00, 187.82it/s, loss=-0.783]\n",
      "100%|██████████| 64/64 [00:00<00:00, 185.31it/s, loss=-0.693]\n",
      "100%|██████████| 64/64 [00:00<00:00, 192.17it/s, loss=-0.671]\n",
      "100%|██████████| 64/64 [00:00<00:00, 184.99it/s, loss=-0.752]\n",
      "100%|██████████| 64/64 [00:00<00:00, 191.51it/s, loss=-0.773]\n",
      "100%|██████████| 64/64 [00:00<00:00, 188.22it/s, loss=-0.746]\n",
      "100%|██████████| 64/64 [00:00<00:00, 189.66it/s, loss=-0.74] \n",
      "100%|██████████| 64/64 [00:00<00:00, 180.97it/s, loss=-0.777]\n",
      "100%|██████████| 64/64 [00:00<00:00, 193.20it/s, loss=-0.753]\n",
      "100%|██████████| 64/64 [00:00<00:00, 187.04it/s, loss=-0.89] \n",
      "100%|██████████| 64/64 [00:00<00:00, 184.55it/s, loss=-0.861]\n",
      "Epoch: 100%|██████████| 30/30 [00:10<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 10.16049861907959\n",
      "[GPU] Used: 4330.16 MB\n",
      "[PyTorch] Max Allocated: 245.70 MB\n",
      "[System RAM] Used: 19.50 GB\n",
      "RAM: 255.77734375 MB\n",
      "Test MSE: 0.7243191003799438\n",
      "Test MAE: 0.7243191003799438\n",
      "\n",
      "Summary:\n",
      "Mean MSE: 0.7349816977977752\n",
      "Std MSE: 0.00694132325302037\n",
      "Mean Time: 10.502369952201843\n",
      "Std Time: 0.4852394503488141\n",
      "0.73498  & (0.00694)  & 10.50237  & (0.48524)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "import statistics\n",
    "import torch\n",
    "import gpytorch\n",
    "import faiss  # if needed elsewhere\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "my_batch_size = 32\n",
    "n_replicates = 10\n",
    "smoke_test = False\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "# Set training hyperparameters depending on smoke_test flag\n",
    "if smoke_test:\n",
    "    k = 32\n",
    "    training_batch_size = 32\n",
    "    num_epochs = 1\n",
    "else:\n",
    "    # You can adjust these values as needed.\n",
    "    k = 160  # or 320 as required\n",
    "    training_batch_size = 320 * 4\n",
    "    num_epochs = 30  # or 30 as desired\n",
    "\n",
    "# Define the GPModel using NNVariationalStrategy\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points, likelihood, k=256, training_batch_size=256):\n",
    "        # Get shape of inducing points\n",
    "        m, d = inducing_points.shape\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "        print(\"Step 1\")\n",
    "        variational_distribution = gpytorch.variational.MeanFieldVariationalDistribution(m)\n",
    "        if gpu:\n",
    "            inducing_points = inducing_points.cuda()\n",
    "        print(\"Step 2\")\n",
    "        variational_strategy = NNVariationalStrategy(\n",
    "            self, inducing_points, variational_distribution,\n",
    "            k=k, training_batch_size=training_batch_size\n",
    "        )\n",
    "        print(\"Step 21\")\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        print(\"Step 22\")\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        print(\"Step 23\")\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=d)\n",
    "        )\n",
    "        print(\"Step 3\")\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        vram_usage()  # assumed to be defined elsewhere\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def __call__(self, x, prior=False, **kwargs):\n",
    "        if x is not None:\n",
    "            if x.dim() == 1:\n",
    "                x = x.unsqueeze(-1)\n",
    "        return self.variational_strategy(x=x, prior=False, **kwargs)\n",
    "\n",
    "# Containers for metrics\n",
    "mse_l_vnn = []\n",
    "time_l_vnn = []\n",
    "\n",
    "for rep in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate: {rep} ===\")\n",
    "    # Re-split the data for each replicate\n",
    "    # Assumes splitter(all_x, all_y, n_train, n_test, random_state) returns:\n",
    "    # train_x, train_y, test_x, test_y\n",
    "    train_x, train_y, test_x, test_y = splitter(all_x, all_y, n_train=80000, n_test=20000, random_state=rep)\n",
    "    \n",
    "    # If GPU is available, move the data to GPU\n",
    "    if gpu:\n",
    "        train_x = train_x.cuda()\n",
    "        train_y = train_y.cuda()\n",
    "        test_x = test_x.cuda()\n",
    "        test_y = test_y.cuda()\n",
    "    \n",
    "    # Create DataLoader for evaluation (training uses variational mini-batching inside the model)\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "    \n",
    "    mem_begin = get_mem()  # assumes get_mem() is defined elsewhere\n",
    "    print(\"Memory before training:\", (get_mem()-mem_begin)/(1024**2), \"MB\")\n",
    "    \n",
    "    # Initialize likelihood and model\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    # Note: for the inducing points, we use the full training set\n",
    "    model = GPModel(inducing_points=train_x[::1].contiguous(), likelihood=likelihood, k=64, training_batch_size=training_batch_size)\n",
    "    \n",
    "    if gpu:\n",
    "        likelihood = likelihood.cuda()\n",
    "        model = model.cuda()\n",
    "    \n",
    "    print(\"Time since start:\", time.time())\n",
    "    \n",
    "    # Set number of epochs (the number of outer iterations) based on your hyperparameters\n",
    "    # num_batches is determined by the variational strategy internal variable\n",
    "    num_batches = model.variational_strategy._total_training_batches\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    # Here we use the Exact Marginal Log Likelihood; adjust if needed.\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    begin_train = time.time()\n",
    "    epochs_iter = trange(num_epochs, desc=\"Epoch\", leave=True, position=0)\n",
    "    for epoch in epochs_iter:\n",
    "        minibatch_iter = tqdm(range(num_batches), leave=True, position=0)\n",
    "        for batch_idx in minibatch_iter:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x=None)  # x is not used; model uses current_training_indices\n",
    "            # Get current mini-batch indices from variational strategy\n",
    "            current_training_indices = model.variational_strategy.current_training_indices\n",
    "            # Retrieve the corresponding y_batch (ensure consistency between train_x and train_y)\n",
    "            y_batch = train_y[..., current_training_indices]\n",
    "            if gpu:\n",
    "                y_batch = y_batch.cuda()\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            vram_usage()  # Track VRAM usage (assumed defined)\n",
    "            optimizer.step()\n",
    "    uTime = time.time() - begin_train\n",
    "    print(\"Training Time:\", uTime)\n",
    "    log_memory()\n",
    "    print(\"RAM:\", (get_mem()-mem_begin), \"MB\")\n",
    "    \n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([])  # Leave it on CPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            if gpu:\n",
    "                x_batch = x_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    mse = torch.mean((means - test_y.cpu())**2).item()\n",
    "    mse_l_vnn.append(mse)\n",
    "    time_l_vnn.append(uTime)\n",
    "    print(\"Test MSE:\", mse)\n",
    "    \n",
    "    # Clean up between replicates\n",
    "    model = None\n",
    "    likelihood = None\n",
    "    mll = None\n",
    "    optimizer = None\n",
    "    epochs_iter = None\n",
    "    gc.collect()\n",
    "    print(\"Test MAE:\", torch.mean((means - test_y.cpu())**2).item())\n",
    "\n",
    "# Summary of results\n",
    "print(\"\\nSummary:\")\n",
    "print(\"Mean MSE:\", statistics.mean(mse_l_vnn))\n",
    "print(\"Std MSE:\", statistics.stdev(mse_l_vnn))\n",
    "print(\"Mean Time:\", statistics.mean(time_l_vnn))\n",
    "print(\"Std Time:\", statistics.stdev(time_l_vnn))\n",
    "print(f\"{round(statistics.mean(mse_l_vnn),5)}  & ({round(statistics.stdev(mse_l_vnn),5)})  & {round(statistics.mean(time_l_vnn),5)}  & ({round(statistics.stdev(time_l_vnn),5)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ed2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d76261-c467-43ea-a72b-e0ec5a412fe1",
   "metadata": {},
   "source": [
    "# Compile Table (MSE and Time only)\n",
    "\n",
    "SKI\n",
    "SGPR\n",
    "LOVE\n",
    "DKL\n",
    "SVGP-CI\n",
    "SVGP\n",
    "NGD\n",
    "VNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SKI     --- MSE:\",statistics.mean(mse_l_ski), \"(\",statistics.stdev(mse_l_ski),\")  Time:\", statistics.mean(time_l_ski), \"(\",statistics.stdev(time_l_ski),\")\")\n",
    "print(\"SGPR    --- MSE:\",statistics.mean(mse_l_sgpr), \"(\",statistics.stdev(mse_l_sgpr),\")  Time:\", statistics.mean(time_l_sgpr), \"(\",statistics.stdev(time_l_sgpr),\")\")\n",
    "print(\"LOVE    --- MSE:\",statistics.mean(mse_l_love), \"(\",statistics.stdev(mse_l_love),\")  Time:\", statistics.mean(time_l_love), \"(\",statistics.stdev(time_l_love),\")\")\n",
    "print(\"DKL     --- MSE:\",statistics.mean(mse_l_dkl), \"(\",statistics.stdev(mse_l_dkl),\")  Time:\", statistics.mean(time_l_dkl), \"(\",statistics.stdev(time_l_dkl),\")\")\n",
    "print(\"SVGP-CI --- MSE:\",statistics.mean(mse_l_svgpci), \"(\",statistics.stdev(mse_l_svgpci),\")  Time:\", statistics.mean(time_l_svgpci), \"(\",statistics.stdev(time_l_svgpci),\")\")\n",
    "print(\"SVGP    --- MSE:\",statistics.mean(mse_l_svgp), \"(\",statistics.stdev(mse_l_svgp),\")  Time:\", statistics.mean(time_l_svgp), \"(\",statistics.stdev(time_l_svgp),\")\")\n",
    "print(\"NGD     --- MSE:\",statistics.mean(mse_l_ngd), \"(\",statistics.stdev(mse_l_ngd),\")  Time:\", statistics.mean(time_l_ngd), \"(\",statistics.stdev(time_l_ngd),\")\")\n",
    "print(\"VNN     --- MSE:\",statistics.mean(mse_l_vnn), \"(\",statistics.stdev(mse_l_vnn),\")  Time:\", statistics.mean(time_l_vnn), \"(\",statistics.stdev(time_l_vnn),\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b1b7aa",
   "metadata": {},
   "source": [
    "Reordering\n",
    "SVGP\n",
    "SVGP-CI\n",
    "VNN\n",
    "NGD\n",
    "DKL\n",
    "SGPR\n",
    "SKI\n",
    "LOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b5546-a5ea-4818-ba7d-37e7fdd2ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVGP    --- MSE:\",statistics.mean(mse_l_svgp), \"(\",statistics.stdev(mse_l_svgp),\")  Time:\", statistics.mean(time_l_svgp), \"(\",statistics.stdev(time_l_svgp),\")\")\n",
    "print(\"SVGP-CI --- MSE:\",statistics.mean(mse_l_svgpci), \"(\",statistics.stdev(mse_l_svgpci),\")  Time:\", statistics.mean(time_l_svgpci), \"(\",statistics.stdev(time_l_svgpci),\")\")\n",
    "print(\"VNN     --- MSE:\",statistics.mean(mse_l_vnn), \"(\",statistics.stdev(mse_l_vnn),\")  Time:\", statistics.mean(time_l_vnn), \"(\",statistics.stdev(time_l_vnn),\")\")\n",
    "print(\"NGD     --- MSE:\",statistics.mean(mse_l_ngd), \"(\",statistics.stdev(mse_l_ngd),\")  Time:\", statistics.mean(time_l_ngd), \"(\",statistics.stdev(time_l_ngd),\")\")\n",
    "print(\"DKL     --- MSE:\",statistics.mean(mse_l_dkl), \"(\",statistics.stdev(mse_l_dkl),\")  Time:\", statistics.mean(time_l_dkl), \"(\",statistics.stdev(time_l_dkl),\")\")\n",
    "print(\"SGPR    --- MSE:\",statistics.mean(mse_l_sgpr), \"(\",statistics.stdev(mse_l_sgpr),\")  Time:\", statistics.mean(time_l_sgpr), \"(\",statistics.stdev(time_l_sgpr),\")\")\n",
    "print(\"SKI     --- MSE:\",statistics.mean(mse_l_ski), \"(\",statistics.stdev(mse_l_ski),\")  Time:\", statistics.mean(time_l_ski), \"(\",statistics.stdev(time_l_ski),\")\")\n",
    "print(\"LOVE    --- MSE:\",statistics.mean(mse_l_love), \"(\",statistics.stdev(mse_l_love),\")  Time:\", statistics.mean(time_l_love), \"(\",statistics.stdev(time_l_love),\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e940a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc9fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True if a GPU is accessible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134c3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c613a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scaleGP)",
   "language": "python",
   "name": "scalegp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
