{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268464e-aa9f-4ae0-80cd-d0c7bb885b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a4ade5-38b2-4f05-8626-7fb74d9fe5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = True\n",
    "n_replicates = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6605e5-f791-44f8-ada3-9efb2237ed1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ApproximateGP\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariational\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnearest_neighbor_variational_strategy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NNVariationalStrategy\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "import statistics\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import psutil\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "# Make plots inline\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def get_mem():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss\n",
    "\n",
    "max_vram = 0\n",
    "def vram_usage():\n",
    "    global max_vram\n",
    "    max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.mlls import DeepApproximateMLL\n",
    "\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "max_vram = 0\n",
    "max_ram = 0\n",
    "\n",
    "def vram_usage():\n",
    "    global max_vram\n",
    "    max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "import faiss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7e28d-807c-40e4-8868-4146cd1c5911",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f66626-c1c8-4f7f-a55d-29b5b743d2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470.60546875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(get_mem()/(1024**2))\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "csvfile = pd.read_csv('train_x_2d.csv', header = None, dtype=float, delimiter=' ')\n",
    "train_x = torch.tensor(np.array(csvfile)).float()\n",
    "#train_x = torch.reshape(train_x,[80000,2])\n",
    "csvfile = pd.read_csv('train_y_2d.csv', header = None)\n",
    "train_y = torch.tensor(csvfile[0]).float()\n",
    "csvfile = pd.read_csv('test_x_2d.csv', header = None, dtype=float, delimiter=' ')\n",
    "test_x = torch.tensor(np.array(csvfile)).float()\n",
    "csvfile = pd.read_csv('test_y_2d.csv', header = None)\n",
    "test_y = torch.tensor(csvfile[0]).float()\n",
    "\n",
    "\n",
    "train_n = int(len(train_x))\n",
    "train_x = train_x.contiguous()\n",
    "train_y = train_y.contiguous()\n",
    "\n",
    "test_x = test_x.contiguous()\n",
    "test_y = test_y.contiguous()\n",
    "\n",
    "if gpu:\n",
    "    train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b6d278-66bf-4276-83bc-946cab62602a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80000, 2])\n",
      "2\n",
      "torch.Size([80000])\n",
      "torch.Size([20489, 2])\n",
      "torch.Size([20489])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_x.size(-1))\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db075e72",
   "metadata": {},
   "source": [
    "# Deep Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09de4a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]C:\\Users\\kevwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\utils\\sparse.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if nonzero_indices.storage():\n",
      "C:\\Users\\kevwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\utils\\sparse.py:66: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:85.)\n",
      "  res = cls(index_tensor, value_tensor, interp_size)\n",
      "C:\\Users\\kevwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\utils\\sparse.py:66: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:607.)\n",
      "  res = cls(index_tensor, value_tensor, interp_size)\n",
      "100%|██████████| 60/60 [00:18<00:00,  3.21it/s, loss=0.48] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.693429946899414\n",
      "Memory Usage: 677.48828125 MB\n",
      "VRAM Usage: 25.48779296875 MB\n",
      "Test MAE: 0.11530447751283646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.18it/s, loss=0.478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.898695468902588\n",
      "Memory Usage: 0.33984375 MB\n",
      "VRAM Usage: 42.79150390625 MB\n",
      "Test MAE: 0.11499617993831635\n",
      "0.11515 0.00022 18.79606 0.14514\n"
     ]
    }
   ],
   "source": [
    "my_batch_size = 32\n",
    "smoke_test = False\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "data_dim = train_x.size(-1)\n",
    "print(data_dim)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 1))\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()\n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.ScaleKernel(  gpytorch.kernels.MaternKernel(nu=1.5)  ),\n",
    "                num_dims=1, grid_size=100\n",
    "            )\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "            # This module will scale the NN features so that they're nice values\n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\"\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            vram_usage()\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "mse_l_dkl = []\n",
    "time_l_dkl = []\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    mem_begin = get_mem()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    training_iterations = 60\n",
    "    \n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.feature_extractor.parameters()},\n",
    "        {'params': model.covar_module.parameters()},\n",
    "        {'params': model.mean_module.parameters()},\n",
    "        {'params': model.likelihood.parameters()},\n",
    "    ], lr=0.02)# 0.02 for 100k, 0.02 of Dense\n",
    "    \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    def train():\n",
    "        iterator = tqdm.tqdm(range(training_iterations), leave = True)\n",
    "        for i in iterator:\n",
    "            # Zero backprop gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Get output from model\n",
    "            output = model(train_x)\n",
    "            # Calc loss and backprop derivatives\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            iterator.set_postfix(loss=loss.item())\n",
    "            vram_usage()\n",
    "            optimizer.step()\n",
    "    \n",
    "    \n",
    "    begin=time.time()\n",
    "    train()\n",
    "    uTime = time.time()-begin\n",
    "    print(uTime)\n",
    "    #%time train()\n",
    "    mem_diff = get_mem()-mem_begin\n",
    "    print(\"Memory Usage:\", mem_diff / (1024 ** 2), \"MB\")\n",
    "    print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    # Test points are regularly spaced along [0,1]\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    #    test_x = torch.linspace(0, 1, 51)\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_dkl.append(MSE.item())\n",
    "    time_l_dkl.append(uTime)\n",
    "    print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    \n",
    "\n",
    "# print(statistics.mean(mse_l_dkl))\n",
    "# print(statistics.stdev(mse_l_dkl))\n",
    "\n",
    "# print(statistics.mean(time_l_dkl))\n",
    "# print(statistics.stdev(time_l_dkl))\n",
    "\n",
    "print(round(statistics.mean(mse_l_dkl),5),round(statistics.stdev(mse_l_dkl),5), round(statistics.mean(time_l_dkl),5), round(statistics.stdev(time_l_dkl),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae2815-2c3a-4ea5-85bc-e7d80e7c52c9",
   "metadata": {},
   "source": [
    "# Sparse GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34fe2fcd-83d3-4459-b120-9fe92575fbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1/1 [00:00<00:00, 13.51it/s, loss=0.818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.07860541343688965\n",
      "Test MAE: 0.11041893064975739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1/1 [00:00<00:00, 12.58it/s, loss=0.818]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.08254146575927734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.11041893064975739\n",
      "0.11041893064975739\n",
      "0.0\n",
      "0.0805734395980835\n",
      "0.002783209288265398\n",
      "0.11042 0.0 0.08057 0.00278\n"
     ]
    }
   ],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.base_covar_module = ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2))\n",
    "        self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=train_x[::300].clone(), likelihood=likelihood)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "my_batch_size = 320\n",
    "smoke_test = False\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "mse_l_sgpr = []\n",
    "time_l_sgpr = []\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    \n",
    "    training_iterations = 1#350\n",
    "    \n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    def train():\n",
    "        iterator = tqdm.tqdm(range(training_iterations), desc=\"Train\")\n",
    "    \n",
    "        for i in iterator:\n",
    "            # Zero backprop gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Get output from model\n",
    "            output = model(train_x)\n",
    "            # Calc loss and backprop derivatives\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            iterator.set_postfix(loss=loss.item())\n",
    "            optimizer.step()\n",
    "            vram_usage()#(torch.cuda.memory_allocated())\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    #%time train()\n",
    "    \n",
    "    begin = time.time()\n",
    "    train()\n",
    "    uTime = time.time()-begin\n",
    "    print(\"Time: \", time.time()-begin)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    # with torch.no_grad():\n",
    "    #     for x_batch, y_batch in tqdm.tqdm(test_loader):\n",
    "    #         preds = model(x_batch.cuda())\n",
    "    #         means = torch.cat([means, preds.mean.cpu()])\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    \n",
    "    #means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_sgpr.append(MSE.item())\n",
    "    time_l_sgpr.append(uTime)\n",
    "    print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "\n",
    "# print(\"Time: \", time.time() - begin)\n",
    "# print(\"RAM: \", (get_mem() - mem_begin) / (1024 ** 2))\n",
    "# print(\"VRAM: \", max_vram / (1024 ** 2))\n",
    "print(statistics.mean(mse_l_sgpr))\n",
    "print(statistics.stdev(mse_l_sgpr))\n",
    "\n",
    "print(statistics.mean(time_l_sgpr))\n",
    "print(statistics.stdev(time_l_sgpr))\n",
    "\n",
    "print(round(statistics.mean(mse_l_sgpr),5),round(statistics.stdev(mse_l_sgpr),5), round(statistics.mean(time_l_sgpr),5), round(statistics.stdev(time_l_sgpr),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e92cd-510c-4dd4-8577-bf9ad4d0dfe7",
   "metadata": {},
   "source": [
    "# LOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb91503e-88f6-4084-adfe-79aadc7fb7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM Usage: 379.4443359375 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:12<00:00,  3.14it/s, loss=0.699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  12.745288133621216\n",
      "Test MAE: 0.11364194750785828\n",
      "VRAM Usage: 397.56494140625 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:13<00:00,  3.02it/s, loss=0.702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  13.229908466339111\n",
      "Test MAE: 0.11631685495376587\n",
      "0.11497940123081207\n",
      "0.001891445194047647\n",
      "12.987598299980164\n",
      "0.3426783235657048\n",
      "0.11498 0.00189 12.9876 0.34268\n"
     ]
    }
   ],
   "source": [
    "my_batch_size = 3200\n",
    "smoke_test = False\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=320, shuffle=False)\n",
    "\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(input_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 1))\n",
    "        print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        \n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=1)),\n",
    "            grid_size=100, num_dims=1,\n",
    "        )\n",
    "\n",
    "        # Also add the deep net\n",
    "        self.feature_extractor = LargeFeatureExtractor(input_dim=train_x.size(-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We're first putting our data through a deep net (feature extractor)\n",
    "        # We're also scaling the features so that they're nice values\n",
    "        projected_x = self.feature_extractor(x)\n",
    "        projected_x = projected_x - projected_x.min(0)[0]\n",
    "        projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "\n",
    "        # The rest of this looks like what we've seen\n",
    "        mean_x = self.mean_module(projected_x)\n",
    "        covar_x = self.covar_module(projected_x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mse_l_love = []\n",
    "time_l_love = []\n",
    "\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    mem_begin = get_mem()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    training_iterations = 40\n",
    "    \n",
    "    \n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Includes GaussianLikelihood parameters\n",
    "    \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    \n",
    "    def train():\n",
    "        iterator = tqdm.tqdm(range(training_iterations))\n",
    "        for i in iterator:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            iterator.set_postfix(loss=loss.item())\n",
    "            vram_usage()\n",
    "            optimizer.step()\n",
    "    \n",
    "    #%time train()\n",
    "    \n",
    "    begin = time.time()\n",
    "    train()\n",
    "    uTime = time.time()-begin\n",
    "    print(\"Time: \", time.time()-begin)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    # with torch.no_grad():\n",
    "    #     for x_batch, y_batch in tqdm.tqdm(test_loader):\n",
    "    #         preds = model(x_batch.cuda())\n",
    "    #         means = torch.cat([means, preds.mean.cpu()])\n",
    "    \n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    #    test_x = torch.linspace(0, 1, 51)\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_love.append(MSE.item())\n",
    "    time_l_love.append(uTime)\n",
    "    print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "\n",
    "print(statistics.mean(mse_l_love))\n",
    "print(statistics.stdev(mse_l_love))\n",
    "\n",
    "print(statistics.mean(time_l_love))\n",
    "print(statistics.stdev(time_l_love))\n",
    "\n",
    "print(round(statistics.mean(mse_l_love),5),round(statistics.stdev(mse_l_love),5), round(statistics.mean(time_l_love),5), round(statistics.stdev(time_l_love),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c952be-27ac-4038-99eb-e670ec4b8b34",
   "metadata": {},
   "source": [
    "# NGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caeed018-ae3b-4b7d-a1cb-990009d4eaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM Usage: 413.69482421875 MB\n",
      "VRAM Usage: 413.69482421875 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [00:53<00:00, 10.73s/it]                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  53.673123836517334\n",
      "Memory Usage: 24.86328125 MB\n",
      "Test MAE: 0.10259094089269638\n",
      "VRAM Usage: 59.634765625 MB\n",
      "VRAM Usage: 59.634765625 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [00:55<00:00, 11.07s/it]                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  55.34487438201904\n",
      "Memory Usage: 3.11328125 MB\n",
      "Test MAE: 0.10325957089662552\n",
      "0.10292525589466095\n",
      "0.00047279280988308153\n",
      "54.50899910926819\n",
      "1.1821061471765684\n",
      "0.10293 0.00047 54.509 1.18211\n"
     ]
    }
   ],
   "source": [
    "my_batch_size = 320\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "mse_l_ngd = []\n",
    "time_l_ngd = []\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    inducing_points = train_x[::100]\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    #variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=train_y.size(0), lr=0.00001)#0.001 for 100k, 0.001 for Dense\n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=train_y.size(0), lr=0.01)#0.001 for 100k, 0.001 for Dense\n",
    "    \n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.1) #0.1 for 100k, 0.1 for Dense\n",
    "    print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    #mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "    num_epochs = 5#15\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "    \n",
    "    begin = time.time()\n",
    "    \n",
    "    for i in epochs_iter:\n",
    "        minibatch_iter = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False, position = 0)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            ### Perform NGD step to optimize variational parameters\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "            hyperparameter_optimizer.step()\n",
    "    \n",
    "    uTime = time.time()-begin\n",
    "    print(\"Time: \",time.time()-begin)\n",
    "    mem_diff = get_mem()-mem_begin\n",
    "    print(\"Memory Usage:\", (mem_diff) / (1024 ** 2), \"MB\")\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_ngd.append(MSE.item())\n",
    "    time_l_ngd.append(uTime)\n",
    "    print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "\n",
    "\n",
    "print(statistics.mean(mse_l_ngd))\n",
    "print(statistics.stdev(mse_l_ngd))\n",
    "\n",
    "print(statistics.mean(time_l_ngd))\n",
    "print(statistics.stdev(time_l_ngd))\n",
    "\n",
    "print(round(statistics.mean(mse_l_ngd),5),round(statistics.stdev(mse_l_ngd),5), round(statistics.mean(time_l_ngd),5), round(statistics.stdev(time_l_ngd),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a445a-b249-48da-be6e-d5937da43568",
   "metadata": {},
   "source": [
    "# SVGP_CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9020718-e748-44ac-b8e4-7f5813f65419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM Usage: 58.0517578125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [00:27<00:00,  2.74s/it]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  27.437535047531128\n",
      "Test MAE: 0.10840844362974167\n",
      "VRAM Usage: 31.8486328125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [00:25<00:00,  2.58s/it]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  25.833882808685303\n",
      "Test MAE: 0.1096784770488739\n",
      "0.10904346033930779\n",
      "0.0008980492430019385\n",
      "26.635708928108215\n",
      "1.133953372752872\n",
      "0.10904 0.0009 26.63571 1.13395\n"
     ]
    }
   ],
   "source": [
    "my_batch_size = 3200\n",
    "smoke_test = False\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "#inducing_points = train_x[torch.randperm(train_x.size(0))[:200]]\n",
    "inducing_points = train_x[::1000]\n",
    "\n",
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.CiqVariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2)\n",
    "        )\n",
    "        self.covar_module.base_kernel.initialize(lengthscale=0.01)  # Specific to the 3droad dataset\n",
    "        print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "\n",
    "mse_l_svgpci = []\n",
    "time_l_svgpci = []\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=train_y.size(0), lr=0.1)\n",
    "    \n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.002) #0.01 for 100k, 0.002 for Dense\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    \n",
    "    num_epochs = 10\n",
    "    \n",
    "    begin = time.time()\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "    for i in epochs_iter:\n",
    "        minibatch_iter = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False, position = 0)\n",
    "    \n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "            vram_usage()\n",
    "            hyperparameter_optimizer.step()\n",
    "\n",
    "    uTime = time.time()-begin\n",
    "    print(\"Time: \", time.time()-begin)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    # with torch.no_grad():\n",
    "    #     for x_batch, y_batch in tqdm.tqdm(test_loader):\n",
    "    #         preds = model(x_batch.cuda())\n",
    "    #         means = torch.cat([means, preds.mean.cpu()])\n",
    "    \n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    #    test_x = torch.linspace(0, 1, 51)\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_svgpci.append(MSE.item())\n",
    "    time_l_svgpci.append(uTime)\n",
    "    print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "\n",
    "print(statistics.mean(mse_l_svgpci))\n",
    "print(statistics.stdev(mse_l_svgpci))\n",
    "\n",
    "print(statistics.mean(time_l_svgpci))\n",
    "print(statistics.stdev(time_l_svgpci))\n",
    "\n",
    "print(round(statistics.mean(mse_l_svgpci),5),round(statistics.stdev(mse_l_svgpci),5), round(statistics.mean(time_l_svgpci),5), round(statistics.stdev(time_l_svgpci),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ac0a0-1575-4237-852e-385f23a4f791",
   "metadata": {},
   "source": [
    "# SVGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c48d8af0-7edb-4c15-80a3-e6eb62eda014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replicate: 0\n",
      "RAM:  4.734375\n",
      "RAM:  4.734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:06<02:09,  6.81s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/20 - Loss: 0.817   lengthscale: 0.705   noise: 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:14<02:08,  7.15s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2/20 - Loss: 0.802   lengthscale: 0.709   noise: 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:21<02:05,  7.35s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3/20 - Loss: 0.797   lengthscale: 0.707   noise: 0.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:29<02:00,  7.52s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4/20 - Loss: 0.788   lengthscale: 0.701   noise: 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:37<01:54,  7.66s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5/20 - Loss: 0.783   lengthscale: 0.691   noise: 0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:45<01:49,  7.79s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6/20 - Loss: 0.773   lengthscale: 0.681   noise: 0.621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:53<01:42,  7.90s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7/20 - Loss: 0.765   lengthscale: 0.670   noise: 0.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [01:01<01:35,  7.95s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8/20 - Loss: 0.753   lengthscale: 0.660   noise: 0.598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [01:09<01:28,  8.05s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9/20 - Loss: 0.747   lengthscale: 0.649   noise: 0.586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [01:18<01:20,  8.08s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10/20 - Loss: 0.740   lengthscale: 0.638   noise: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [01:26<01:13,  8.20s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11/20 - Loss: 0.730   lengthscale: 0.628   noise: 0.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [01:34<01:05,  8.24s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12/20 - Loss: 0.725   lengthscale: 0.618   noise: 0.553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [01:43<00:57,  8.26s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13/20 - Loss: 0.718   lengthscale: 0.608   noise: 0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [01:51<00:49,  8.28s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14/20 - Loss: 0.707   lengthscale: 0.598   noise: 0.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [01:59<00:41,  8.31s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15/20 - Loss: 0.704   lengthscale: 0.589   noise: 0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [02:08<00:33,  8.27s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16/20 - Loss: 0.692   lengthscale: 0.581   noise: 0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [02:16<00:24,  8.32s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17/20 - Loss: 0.684   lengthscale: 0.572   noise: 0.502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [02:24<00:16,  8.32s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18/20 - Loss: 0.675   lengthscale: 0.565   noise: 0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [02:33<00:08,  8.37s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 19/20 - Loss: 0.667   lengthscale: 0.557   noise: 0.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20/20 - Loss: 0.656   lengthscale: 0.550   noise: 0.473\n",
      "Time:  161.7269344329834\n",
      "RAM:  3.98828125\n",
      "VRAM:  892.3828125\n",
      "Test MAE: 0.11925528943538666\n",
      "Replicate: 1\n",
      "RAM:  4.88671875\n",
      "RAM:  4.88671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   0%|          | 0/20 [02:41<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:08<02:39,  8.38s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/20 - Loss: 0.817   lengthscale: 0.704   noise: 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:18<02:47,  9.31s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2/20 - Loss: 0.806   lengthscale: 0.709   noise: 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:28<02:44,  9.65s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3/20 - Loss: 0.801   lengthscale: 0.707   noise: 0.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:37<02:30,  9.44s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4/20 - Loss: 0.787   lengthscale: 0.699   noise: 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:46<02:18,  9.21s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5/20 - Loss: 0.783   lengthscale: 0.692   noise: 0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:54<02:06,  9.02s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6/20 - Loss: 0.774   lengthscale: 0.682   noise: 0.621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:03<01:55,  8.88s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7/20 - Loss: 0.763   lengthscale: 0.671   noise: 0.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [01:12<01:45,  8.81s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8/20 - Loss: 0.757   lengthscale: 0.660   noise: 0.598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [01:20<01:35,  8.69s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9/20 - Loss: 0.747   lengthscale: 0.650   noise: 0.586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [01:29<01:28,  8.84s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10/20 - Loss: 0.741   lengthscale: 0.640   noise: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [01:38<01:19,  8.84s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11/20 - Loss: 0.731   lengthscale: 0.629   noise: 0.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [01:47<01:10,  8.87s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12/20 - Loss: 0.728   lengthscale: 0.619   noise: 0.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [01:56<01:01,  8.79s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13/20 - Loss: 0.712   lengthscale: 0.610   noise: 0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [02:04<00:52,  8.73s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14/20 - Loss: 0.707   lengthscale: 0.600   noise: 0.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [02:13<00:43,  8.75s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15/20 - Loss: 0.703   lengthscale: 0.591   noise: 0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [02:22<00:34,  8.68s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16/20 - Loss: 0.693   lengthscale: 0.583   noise: 0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [02:30<00:25,  8.64s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17/20 - Loss: 0.684   lengthscale: 0.574   noise: 0.502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [02:39<00:17,  8.72s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 18/20 - Loss: 0.674   lengthscale: 0.568   noise: 0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [02:48<00:08,  8.72s/it]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 19/20 - Loss: 0.665   lengthscale: 0.560   noise: 0.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20/20 - Loss: 0.661   lengthscale: 0.552   noise: 0.473\n",
      "Time:  176.8529760837555\n",
      "RAM:  5.15625\n",
      "VRAM:  892.3828125\n",
      "Test MAE: 0.11922081559896469\n",
      "0.11923805251717567\n",
      "2.4376683507488335e-05\n",
      "169.28995525836945\n",
      "10.695726623771108\n",
      "0.11924 2e-05 169.28996 10.69573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "my_batch_size = 3200\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=False)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5))\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        #print(\"RAM: \", (get_mem() - mem_begin) / (1024 ** 2))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        #print(\"RAM: \", (get_mem() - mem_begin) / (1024 ** 2))\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "mse_l_svgp = []\n",
    "time_l_svgp = []\n",
    "\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    print(\"Replicate:\" ,i)\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    inducing_points = train_x[::100]\n",
    "    #inducing_points = train_x\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    #likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"RAM: \", mem_diff / (1024 ** 2))\n",
    "    \n",
    "    #num_epochs = 3# if smoke_test else 4\n",
    "    num_epochs = 20\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.001)\n",
    "    \n",
    "    #optimizer = torch.optim.SGD([\n",
    "    #    {'params': model.parameters()},\n",
    "    #    {'params': likelihood.parameters()},\n",
    "    #], lr=1)\n",
    "    \n",
    "    # Our loss object. We're using the VariationalELBO\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"RAM: \", mem_diff / (1024 ** 2))\n",
    "    \n",
    "    \n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "    \n",
    "    begin = time.time()\n",
    "    #for i in epochs_iter:\n",
    "    for i in tqdm.tqdm(range(num_epochs), leave = False, position = 0):\n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        minibatch_iter = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False, position = 0)\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "        #for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            max_ram = max(max_ram, (get_mem() - mem_begin))\n",
    "            optimizer.step()\n",
    "            if gpu:\n",
    "                max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "            i + 1, num_epochs, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item(),\n",
    "            likelihood.noise.item()\n",
    "        ))\n",
    "    uTime = time.time()-begin\n",
    "    print(\"Time: \", time.time() - begin)\n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"RAM: \", max_ram / (1024 ** 2))\n",
    "    print(\"VRAM: \", max_vram / (1024 ** 2))\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    # with torch.no_grad():\n",
    "    #     for x_batch, y_batch in tqdm.tqdm(test_loader):\n",
    "    #         preds = model(x_batch.cuda())\n",
    "    #         means = torch.cat([means, preds.mean.cpu()])\n",
    "     \n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    #    test_x = torch.linspace(0, 1, 51)\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_svgp.append(MSE.item())\n",
    "    time_l_svgp.append(uTime)\n",
    "    print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "\n",
    "print(statistics.mean(mse_l_svgp))\n",
    "print(statistics.stdev(mse_l_svgp))\n",
    "\n",
    "print(statistics.mean(time_l_svgp))\n",
    "print(statistics.stdev(time_l_svgp))\n",
    "\n",
    "print(round(statistics.mean(mse_l_svgp),5),round(statistics.stdev(mse_l_svgp),5), round(statistics.mean(time_l_svgp),5), round(statistics.stdev(time_l_svgp),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4ccda-39fa-4f20-8b0c-eb815d86c99f",
   "metadata": {},
   "source": [
    "# SKI - Can only handle up to 40,000 datapoints before running out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85087eb9-8b3b-4dad-9125-4ebefd372189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = None\n",
    "likelihood = None\n",
    "\n",
    "if gpu:\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb226a15-2d04-4e1c-9e88-1733fb104f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_ski = train_x[::2]\n",
    "train_y_ski = train_y[::2]\n",
    "\n",
    "if gpu:\n",
    "    train_x_ski, train_y_ski = train_x_ski.cuda(), train_y_ski.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24869027-c361-400b-a950-33cdab17cac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.6911289691925\n",
      "RAM:  5758.4609375\n",
      "VRAM:  892.3828125\n",
      "Test MAE: 0.12249397486448288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.61305618286133\n",
      "RAM:  -1933.140625\n",
      "VRAM:  892.3828125\n",
      "Test MAE: 0.12249375879764557\n",
      "0.12249386683106422\n",
      "1.5278232585199085e-07\n",
      "87.65209257602692\n",
      "2.8836329213870937\n",
      "0.12249 0.0 87.65209 2.88363\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        # SKI requires a grid size hyperparameter. This util can help with that\n",
    "        grid_size = gpytorch.utils.grid.choose_grid_size(train_x, 1) #1/50\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.RBFKernel(), grid_size=grid_size, num_dims=2\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iterations = 32\n",
    "\n",
    "\n",
    "mse_l_ski = []\n",
    "time_l_ski = []\n",
    "\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x_ski, train_y_ski, likelihood)\n",
    "    \n",
    "    \n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)  #0.001 Includes GaussianLikelihood parameters\n",
    "    \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model).cuda()\n",
    "    \n",
    "    # # Find optimal model hyperparameters\n",
    "    # model.train()\n",
    "    # likelihood.train()\n",
    "    \n",
    "    # # Use the adam optimizer\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Includes GaussianLikelihood parameters\n",
    "    \n",
    "    # # \"Loss\" for GPs - the marginal log likelihood\n",
    "    # mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    training_iterations = 15\n",
    "    begin = time.time()\n",
    "    \n",
    "    for i in tqdm.tqdm(range(training_iterations), desc=\"Train\", leave = False, position = 0 ):\n",
    "        optimizer.zero_grad()\n",
    "        if gpu:\n",
    "            max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        output = model(train_x_ski)\n",
    "        loss = -mll(output, train_y_ski)\n",
    "        loss.backward()\n",
    "        if gpu:\n",
    "            max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        optimizer.step()\n",
    "\n",
    "    uTime = time.time()-begin\n",
    "    print(time.time()-begin)\n",
    "    print(\"RAM: \",(get_mem() - mem_begin)/(1024**2))\n",
    "    print(\"VRAM: \", max_vram / (1024 ** 2))\n",
    "    \n",
    "    model.eval()\n",
    "    with gpytorch.settings.prior_mode():\n",
    "        output = (model(test_x))\n",
    "    means = output.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    mse_l_ski.append(MSE.item())\n",
    "    time_l_ski.append(uTime)\n",
    "    print('Test MAE: {}'.format(torch.mean((means.cpu() - test_y.cpu())*(means.cpu() - test_y.cpu()))))\n",
    "\n",
    "\n",
    "\n",
    "print(statistics.mean(mse_l_ski))\n",
    "print(statistics.stdev(mse_l_ski))\n",
    "\n",
    "print(statistics.mean(time_l_ski))\n",
    "print(statistics.stdev(time_l_ski))\n",
    "\n",
    "print(round(statistics.mean(mse_l_ski),5),round(statistics.stdev(mse_l_ski),5), round(statistics.mean(time_l_ski),5), round(statistics.stdev(time_l_ski),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc3f645-92de-4bed-86da-6c77d005c177",
   "metadata": {},
   "source": [
    "# VNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84aecce4-4e59-48d5-a6ab-b89d65c77184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replicate:  0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m likelihood \u001b[38;5;241m=\u001b[39m gpytorch\u001b[38;5;241m.\u001b[39mlikelihoods\u001b[38;5;241m.\u001b[39mGaussianLikelihood()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Note: one should use full training set as inducing points!\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minducing_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gpu:\n\u001b[0;32m     79\u001b[0m     likelihood \u001b[38;5;241m=\u001b[39m likelihood\u001b[38;5;241m.\u001b[39mcuda()\n",
      "Cell \u001b[1;32mIn[14], line 30\u001b[0m, in \u001b[0;36mGPModel.__init__\u001b[1;34m(self, inducing_points, likelihood, k, training_batch_size)\u001b[0m\n\u001b[0;32m     27\u001b[0m     inducing_points \u001b[38;5;241m=\u001b[39m inducing_points\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m variational_strategy \u001b[38;5;241m=\u001b[39m \u001b[43mNNVariationalStrategy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minducing_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariational_distribution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mtraining_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m21\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28msuper\u001b[39m(GPModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(variational_strategy)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gpytorch\\variational\\nearest_neighbor_variational_strategy.py:112\u001b[0m, in \u001b[0;36mNNVariationalStrategy.__init__\u001b[1;34m(self, model, inducing_points, variational_distribution, k, training_batch_size, jitter_val)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_shape: torch\u001b[38;5;241m.\u001b[39mSize \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_shapes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inducing_batch_shape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_batch_shape)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_util: NNUtil \u001b[38;5;241m=\u001b[39m NNUtil(\n\u001b[0;32m    110\u001b[0m     k, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD, batch_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inducing_batch_shape, device\u001b[38;5;241m=\u001b[39minducing_points\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    111\u001b[0m )\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_batch_size \u001b[38;5;241m=\u001b[39m training_batch_size\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_training_iterator()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gpytorch\\variational\\nearest_neighbor_variational_strategy.py:362\u001b[0m, in \u001b[0;36mNNVariationalStrategy._compute_nn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    360\u001b[0m     inducing_points_fl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minducing_points\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_util\u001b[38;5;241m.\u001b[39mset_nn_idx(inducing_points_fl)\n\u001b[1;32m--> 362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_xinduce_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_sequential_nn_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43minducing_points_fl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gpytorch\\utils\\nearest_neighbors.py:199\u001b[0m, in \u001b[0;36mNNUtil.build_sequential_nn_idx\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_shape\u001b[38;5;241m.\u001b[39mnumel()):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# finding k nearest neighbors in the first k\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, N):\n\u001b[1;32m--> 199\u001b[0m         train_neighbors \u001b[38;5;241m=\u001b[39m \u001b[43mNearestNeighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_np\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m         nn_idx_i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\n\u001b[0;32m    201\u001b[0m             train_neighbors\u001b[38;5;241m.\u001b[39mkneighbors(\n\u001b[0;32m    202\u001b[0m                 x_np[bi][i][\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m             )[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    206\u001b[0m         )\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    208\u001b[0m         nn_idx[bi][i \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk]\u001b[38;5;241m.\u001b[39mcopy_(nn_idx_i)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_unsupervised.py:175\u001b[0m, in \u001b[0;36mNearestNeighbors.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the nearest neighbors estimator from the training dataset.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m        The fitted nearest neighbors estimator.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_base.py:590\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminkowski\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_params_\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    583\u001b[0m     ):\n\u001b[0;32m    584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    585\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkd_tree\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not valid for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    586\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with a weight parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtry algorithm=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mball_tree\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    588\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor algorithm=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    589\u001b[0m         )\n\u001b[1;32m--> 590\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tree \u001b[38;5;241m=\u001b[39m KDTree(\n\u001b[0;32m    591\u001b[0m         X,\n\u001b[0;32m    592\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleaf_size,\n\u001b[0;32m    593\u001b[0m         metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_,\n\u001b[0;32m    594\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_params_,\n\u001b[0;32m    595\u001b[0m     )\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32msklearn\\neighbors\\_binary_tree.pxi:833\u001b[0m, in \u001b[0;36msklearn.neighbors._kd_tree.BinaryTree.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    893\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    894\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    895\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    896\u001b[0m         )\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 899\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    907\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:108\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space np.isfinite to prevent\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# false positives from overflow in sum method. The sum is also calculated\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# safely to reduce dtype induced overflows.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m is_float \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_float \u001b[38;5;129;01mand\u001b[39;00m (np\u001b[38;5;241m.\u001b[39misfinite(\u001b[43m_safe_accumulator_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)):\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_float:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\extmath.py:897\u001b[0m, in \u001b[0;36m_safe_accumulator_op\u001b[1;34m(op, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    895\u001b[0m     result \u001b[38;5;241m=\u001b[39m op(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 897\u001b[0m     result \u001b[38;5;241m=\u001b[39m op(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_batch_size = 32\n",
    "smoke_test = False\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points, likelihood, k=256, training_batch_size=256):\n",
    "\n",
    "        m, d = inducing_points.shape\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "        print(1)\n",
    "\n",
    "        variational_distribution = gpytorch.variational.MeanFieldVariationalDistribution(m)\n",
    "\n",
    "        if gpu:\n",
    "            inducing_points = inducing_points.cuda()\n",
    "        print(2)\n",
    "\n",
    "        variational_strategy = NNVariationalStrategy(self, inducing_points, variational_distribution, k=k,\n",
    "                                                     training_batch_size=training_batch_size)\n",
    "        print(21)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        print(22)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        print(23)\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=d))\n",
    "        print(3)\n",
    "        \n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def __call__(self, x, prior=False, **kwargs):\n",
    "        if x is not None:\n",
    "            if x.dim() == 1:\n",
    "                x = x.unsqueeze(-1)\n",
    "        return self.variational_strategy(x=x, prior=False, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "begin = time.time()\n",
    "if smoke_test:\n",
    "    k = 32\n",
    "    training_batch_size = 32\n",
    "else:\n",
    "    k = 256\n",
    "    training_batch_size = 64\n",
    "\n",
    "k = 160#320\n",
    "training_batch_size = 320*4\n",
    "\n",
    "mse_l_vnn = []\n",
    "time_l_vnn = []\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    print(\"Replicate: \",i)\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    # Note: one should use full training set as inducing points!\n",
    "    model = GPModel(inducing_points=train_x[::1].contiguous(), likelihood=likelihood, k=k, training_batch_size=training_batch_size)\n",
    "    \n",
    "    if gpu:\n",
    "        likelihood = likelihood.cuda()\n",
    "        model = model.cuda()\n",
    "    \n",
    "    print(time.time()-begin)\n",
    "    \n",
    "    #torch.cuda.empty_cache()\n",
    "    \n",
    "    num_epochs = 1 if smoke_test else 20\n",
    "    num_epochs = 10#30\n",
    "    num_batches = model.variational_strategy._total_training_batches\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "    \n",
    "    # optimizer = torch.optim.Adam([\n",
    "    #     {'params': model.parameters()},\n",
    "    #     {'params': likelihood.parameters()},\n",
    "    # ], lr=0.05)\n",
    "    \n",
    "    # Our loss object. We're using the VariationalELBO\n",
    "    #mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    begin = time.time()\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\", leave=True, position = 0)\n",
    "    for epoch in epochs_iter:\n",
    "        minibatch_iter = tqdm.tqdm(range(num_batches), leave=True, position = 0)\n",
    "    \n",
    "        for i in minibatch_iter:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x=None)\n",
    "            # Obtain the indices for mini-batch data\n",
    "            current_training_indices = model.variational_strategy.current_training_indices\n",
    "            # Obtain the y_batch using indices. It is important to keep the same order of train_x and train_y\n",
    "            y_batch = train_y[...,current_training_indices]\n",
    "            if gpu:\n",
    "                y_batch = y_batch.cuda()\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            vram_usage()\n",
    "            optimizer.step()\n",
    "    uTime = time.time() - begin\n",
    "    print(\"Time: \", time.time() - begin)\n",
    "    print(\"VRAM: \", max_vram/(1024 ** 2))\n",
    "    print(\"RAM: \", (get_mem() - mem_begin)/(1024**2))\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_vnn.append(MSE.item())\n",
    "    time_l_vnn.append(uTime)\n",
    "\n",
    "    model = None\n",
    "    likelihood = None\n",
    "    mll = None\n",
    "    optimizer = None\n",
    "    epochs_iter = None\n",
    "    if gpu:\n",
    "        gc.collect()\n",
    "    \n",
    "    print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "\n",
    "\n",
    "\n",
    "print(statistics.mean(mse_l_vnn))\n",
    "print(statistics.stdev(mse_l_vnn))\n",
    "\n",
    "print(statistics.mean(time_l_vnn))\n",
    "print(statistics.stdev(time_l_vnn))\n",
    "\n",
    "print(round(statistics.mean(mse_l_vnn),5),round(statistics.stdev(mse_l_vnn),5), round(statistics.mean(time_l_vnn),5), round(statistics.stdev(time_l_vnn),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d76261-c467-43ea-a72b-e0ec5a412fe1",
   "metadata": {},
   "source": [
    "# Compile Table (MSE and Time only)\n",
    "\n",
    "SKI\n",
    "SGPR\n",
    "LOVE\n",
    "DKL\n",
    "SVGP-CI\n",
    "SVGP\n",
    "NGD\n",
    "VNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f11037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKI     --- MSE: 0.12249386683106422 ( 1.5278232585199085e-07 )  Time: 87.65209257602692 ( 2.8836329213870937 )\n",
      "SGPR    --- MSE: 0.11041893064975739 ( 0.0 )  Time: 0.0805734395980835 ( 0.002783209288265398 )\n",
      "LOVE    --- MSE: 0.11497940123081207 ( 0.001891445194047647 )  Time: 12.987598299980164 ( 0.3426783235657048 )\n",
      "DKL     --- MSE: 0.1151503287255764 ( 0.0002179993055665355 )  Time: 18.796062707901 ( 0.1451446425522407 )\n",
      "SVGP-CI --- MSE: 0.10904346033930779 ( 0.0008980492430019385 )  Time: 26.635708928108215 ( 1.133953372752872 )\n",
      "SVGP    --- MSE: 0.11923805251717567 ( 2.4376683507488335e-05 )  Time: 169.28995525836945 ( 10.695726623771108 )\n",
      "NGD     --- MSE: 0.10292525589466095 ( 0.00047279280988308153 )  Time: 54.50899910926819 ( 1.1821061471765684 )\n"
     ]
    },
    {
     "ename": "StatisticsError",
     "evalue": "mean requires at least one data point",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStatisticsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVGP    --- MSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m,statistics\u001b[38;5;241m.\u001b[39mmean(mse_l_svgp), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m,statistics\u001b[38;5;241m.\u001b[39mstdev(mse_l_svgp),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)  Time:\u001b[39m\u001b[38;5;124m\"\u001b[39m, statistics\u001b[38;5;241m.\u001b[39mmean(time_l_svgp), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m,statistics\u001b[38;5;241m.\u001b[39mstdev(time_l_svgp),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNGD     --- MSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m,statistics\u001b[38;5;241m.\u001b[39mmean(mse_l_ngd), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m,statistics\u001b[38;5;241m.\u001b[39mstdev(mse_l_ngd),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)  Time:\u001b[39m\u001b[38;5;124m\"\u001b[39m, statistics\u001b[38;5;241m.\u001b[39mmean(time_l_ngd), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m,statistics\u001b[38;5;241m.\u001b[39mstdev(time_l_ngd),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVNN     --- MSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mstatistics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmse_l_vnn\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m,statistics\u001b[38;5;241m.\u001b[39mstdev(mse_l_vnn),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)  Time:\u001b[39m\u001b[38;5;124m\"\u001b[39m, statistics\u001b[38;5;241m.\u001b[39mmean(time_l_vnn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m,statistics\u001b[38;5;241m.\u001b[39mstdev(time_l_vnn),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\statistics.py:328\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    326\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StatisticsError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean requires at least one data point\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    329\u001b[0m T, total, count \u001b[38;5;241m=\u001b[39m _sum(data)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m count \u001b[38;5;241m==\u001b[39m n\n",
      "\u001b[1;31mStatisticsError\u001b[0m: mean requires at least one data point"
     ]
    }
   ],
   "source": [
    "print(\"SKI     --- MSE:\",statistics.mean(mse_l_ski), \"(\",statistics.stdev(mse_l_ski),\")  Time:\", statistics.mean(time_l_ski), \"(\",statistics.stdev(time_l_ski),\")\")\n",
    "print(\"SGPR    --- MSE:\",statistics.mean(mse_l_sgpr), \"(\",statistics.stdev(mse_l_sgpr),\")  Time:\", statistics.mean(time_l_sgpr), \"(\",statistics.stdev(time_l_sgpr),\")\")\n",
    "print(\"LOVE    --- MSE:\",statistics.mean(mse_l_love), \"(\",statistics.stdev(mse_l_love),\")  Time:\", statistics.mean(time_l_love), \"(\",statistics.stdev(time_l_love),\")\")\n",
    "print(\"DKL     --- MSE:\",statistics.mean(mse_l_dkl), \"(\",statistics.stdev(mse_l_dkl),\")  Time:\", statistics.mean(time_l_dkl), \"(\",statistics.stdev(time_l_dkl),\")\")\n",
    "print(\"SVGP-CI --- MSE:\",statistics.mean(mse_l_svgpci), \"(\",statistics.stdev(mse_l_svgpci),\")  Time:\", statistics.mean(time_l_svgpci), \"(\",statistics.stdev(time_l_svgpci),\")\")\n",
    "print(\"SVGP    --- MSE:\",statistics.mean(mse_l_svgp), \"(\",statistics.stdev(mse_l_svgp),\")  Time:\", statistics.mean(time_l_svgp), \"(\",statistics.stdev(time_l_svgp),\")\")\n",
    "print(\"NGD     --- MSE:\",statistics.mean(mse_l_ngd), \"(\",statistics.stdev(mse_l_ngd),\")  Time:\", statistics.mean(time_l_ngd), \"(\",statistics.stdev(time_l_ngd),\")\")\n",
    "print(\"VNN     --- MSE:\",statistics.mean(mse_l_vnn), \"(\",statistics.stdev(mse_l_vnn),\")  Time:\", statistics.mean(time_l_vnn), \"(\",statistics.stdev(time_l_vnn),\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c0f4f-d07f-4601-b8ed-f0f6965ec735",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reordering\n",
    "SVGP\n",
    "SVGP-CI\n",
    "VNN\n",
    "NGD\n",
    "DKL\n",
    "SGPR\n",
    "SKI\n",
    "LOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b5546-a5ea-4818-ba7d-37e7fdd2ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVGP    --- MSE:\",statistics.mean(mse_l_svgp), \"(\",statistics.stdev(mse_l_svgp),\")  Time:\", statistics.mean(time_l_svgp), \"(\",statistics.stdev(time_l_svgp),\")\")\n",
    "print(\"SVGP-CI --- MSE:\",statistics.mean(mse_l_svgpci), \"(\",statistics.stdev(mse_l_svgpci),\")  Time:\", statistics.mean(time_l_svgpci), \"(\",statistics.stdev(time_l_svgpci),\")\")\n",
    "print(\"VNN     --- MSE:\",statistics.mean(mse_l_vnn), \"(\",statistics.stdev(mse_l_vnn),\")  Time:\", statistics.mean(time_l_vnn), \"(\",statistics.stdev(time_l_vnn),\")\")\n",
    "print(\"NGD     --- MSE:\",statistics.mean(mse_l_ngd), \"(\",statistics.stdev(mse_l_ngd),\")  Time:\", statistics.mean(time_l_ngd), \"(\",statistics.stdev(time_l_ngd),\")\")\n",
    "print(\"DKL     --- MSE:\",statistics.mean(mse_l_dkl), \"(\",statistics.stdev(mse_l_dkl),\")  Time:\", statistics.mean(time_l_dkl), \"(\",statistics.stdev(time_l_dkl),\")\")\n",
    "print(\"SGPR    --- MSE:\",statistics.mean(mse_l_sgpr), \"(\",statistics.stdev(mse_l_sgpr),\")  Time:\", statistics.mean(time_l_sgpr), \"(\",statistics.stdev(time_l_sgpr),\")\")\n",
    "print(\"SKI     --- MSE:\",statistics.mean(mse_l_ski), \"(\",statistics.stdev(mse_l_ski),\")  Time:\", statistics.mean(time_l_ski), \"(\",statistics.stdev(time_l_ski),\")\")\n",
    "print(\"LOVE    --- MSE:\",statistics.mean(mse_l_love), \"(\",statistics.stdev(mse_l_love),\")  Time:\", statistics.mean(time_l_love), \"(\",statistics.stdev(time_l_love),\")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
