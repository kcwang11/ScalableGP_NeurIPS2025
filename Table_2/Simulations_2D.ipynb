{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b37894c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b0d9a",
   "metadata": {},
   "source": [
    "# Important Note\n",
    "RAM and VRAM measurements are dependent on the computer state, and should only be interpreted relative to each other. In order to obtain RAM and VRAM measurements, perform the following steps:\n",
    "\n",
    "1 - Restart the Kernel\n",
    "\n",
    "2 - Run the \"Loading Required Packages and Helper Functions\" cell\n",
    "\n",
    "3 - Run the \"Loading Data\" cell\n",
    "\n",
    "4 - Run ONLY ONE iteration of the desired method, and read the RAM and VRAM usage reports printed by the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6bfc9",
   "metadata": {},
   "source": [
    "# Loading Required Packages and Helper Functions\n",
    "If you would like to use Cuda, set gpu = True. Otherwise set gpu = False. \n",
    "\n",
    "Step 1: Run the following cell to import the required packages and helper functions. Set the number of replicates desired.\n",
    "\n",
    "Step 2: Load the Data\n",
    "\n",
    "Step 3: Execute the cells under the method you wish to replicate.\n",
    "\n",
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a4ade5-38b2-4f05-8626-7fb74d9fe5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = True\n",
    "n_replicates = 11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6605e5-f791-44f8-ada3-9efb2237ed1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Core Imports ---\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import urllib.request\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "import psutil\n",
    "import tqdm\n",
    "import faiss\n",
    "from math import floor\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from memory_profiler import memory_usage\n",
    "import pynvml\n",
    "\n",
    "# --- GPyTorch Imports ---\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.mlls import DeepApproximateMLL\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "# --- Memory and GPU Utilities ---\n",
    "def clear_gpu():\n",
    "    for obj in ['model', 'likelihood', 'observed_pred', 'preds', 'output']:\n",
    "        if obj in globals():\n",
    "            del globals()[obj]\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def get_mem():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 ** 2)  # Return MB\n",
    "\n",
    "max_vram = 0\n",
    "def vram_usage():\n",
    "    global max_vram\n",
    "    max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "\n",
    "def log_memory():\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    max_allocated = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "    max_reserved = torch.cuda.max_memory_reserved() / 1024**2    # MB\n",
    "    gpu_used = meminfo.used / 1024**2                            # MB\n",
    "    sys_used = psutil.virtual_memory().used / 1024**3            # GB\n",
    "    print(f\"[PyTorch] Max Allocated: {max_allocated:.2f} MB | Max Reserved: {max_reserved:.2f} MB\")\n",
    "    print(f\"[GPU VRAM] Used (nvidia-smi): {gpu_used:.2f} MB | [System RAM]: {sys_used:.2f} GB\")\n",
    "    return max_allocated, max_reserved, gpu_used, sys_used\n",
    "\n",
    "# --- IPython Magics (should only be run in notebooks) ---\n",
    "# These will throw errors outside of Jupyter; include only if running interactively\n",
    "try:\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "    get_ipython().run_line_magic('autoreload', '2')\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7e28d-807c-40e4-8868-4146cd1c5911",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "Step 2: Load the data (note: must run the DataGenerator.Rmd file first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = pd.read_csv('Data/data_2d.csv', header = None, dtype=float, delimiter=',')\n",
    "all_data = torch.tensor(np.array(csvfile)).float()\n",
    "\n",
    "def splitter(all_data, n_train=80_000, n_test=20_000, random_state=42, move_to_gpu=True):\n",
    "    assert all_data.ndim == 2 and all_data.shape[1] == 3, \\\n",
    "        \"all_data must be [N,3]\"\n",
    "    total_samples = all_data.shape[0]\n",
    "    assert n_train + n_test <= total_samples, \"Not enough samples to split\"\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "    indices = rng.permutation(total_samples)\n",
    "    train_idx = indices[:n_train]\n",
    "    test_idx  = indices[n_train:n_train + n_test]\n",
    "    train = all_data[train_idx]\n",
    "    test  = all_data[test_idx]\n",
    "\n",
    "    train_x = train[:, :2].contiguous()\n",
    "    train_y = train[:,  2].contiguous()\n",
    "    test_x  = test[:,  :2].contiguous()\n",
    "    test_y  = test[:,   2].contiguous()\n",
    "\n",
    "    if move_to_gpu and torch.cuda.is_available():\n",
    "        train_x = train_x.cuda()\n",
    "        train_y = train_y.cuda()\n",
    "        test_x  = test_x.cuda()\n",
    "        test_y  = test_y.cuda()\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "train_x, train_y, test_x, test_y = splitter(all_data, n_train= 80_000, n_test = 20_000, random_state=42, move_to_gpu=True)\n",
    "print(train_x.shape)\n",
    "print(train_x.size(-1))\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f697e43",
   "metadata": {},
   "source": [
    "# Simulations\n",
    "Step 3: Execute the simulations to be reproduced. If all simulations are run, there is a summarizer at the end. Otherwise, the relevant statistics are printed at the end of each method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db075e72",
   "metadata": {},
   "source": [
    "# Deep Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf4690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]c:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\utils\\sparse.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if nonzero_indices.storage():\n",
      "c:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\utils\\sparse.py:66: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:80.)\n",
      "  res = cls(index_tensor, value_tensor, interp_size)\n",
      "c:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\utils\\sparse.py:66: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:646.)\n",
      "  res = cls(index_tensor, value_tensor, interp_size)\n",
      "  7%|â–‹         | 4/60 [00:03<00:51,  1.08it/s, loss=0.803]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 88\u001b[0m\n\u001b[0;32m     86\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     87\u001b[0m output \u001b[38;5;241m=\u001b[39m model(tx)\n\u001b[1;32m---> 88\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     90\u001b[0m iterator\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\mlls\\exact_marginal_log_likelihood.py:82\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[1;34m(self, function_dist, target, *params, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN observation policy \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported by ExactMarginalLogLikelihood!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_other_terms(res, params)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py:193\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[0;32m    192\u001b[0m covar \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39mevaluate_kernel()\n\u001b[1;32m--> 193\u001b[0m inv_quad, logdet \u001b[38;5;241m=\u001b[39m \u001b[43mcovar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_quad_logdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m([inv_quad, logdet, diff\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)])\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:1772\u001b[0m, in \u001b[0;36mLinearOperator.inv_quad_logdet\u001b[1;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[0;32m   1769\u001b[0m probe_vectors, probe_vector_norms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probe_vectors_and_norms()\n\u001b[0;32m   1771\u001b[0m func \u001b[38;5;241m=\u001b[39m InvQuadLogdet\u001b[38;5;241m.\u001b[39mapply\n\u001b[1;32m-> 1772\u001b[0m inv_quad_term, pinvk_logdet \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1773\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecond_lt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreconditioner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprecond_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1777\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobe_vectors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobe_vector_norms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprecond_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1782\u001b[0m logdet_term \u001b[38;5;241m=\u001b[39m pinvk_logdet\n\u001b[0;32m   1783\u001b[0m logdet_term \u001b[38;5;241m=\u001b[39m logdet_term \u001b[38;5;241m+\u001b[39m logdet_p\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\torch\\autograd\\function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\functions\\_inv_quad_logdet.py:132\u001b[0m, in \u001b[0;36mInvQuadLogdet.forward\u001b[1;34m(ctx, representation_tree, precond_representation_tree, preconditioner, num_precond_args, inv_quad, probe_vectors, probe_vector_norms, *args)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Perform solves (for inv_quad) and tridiagonalization (for estimating logdet)\u001b[39;00m\n\u001b[0;32m    131\u001b[0m rhs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(rhs_list, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m solves, t_mat \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_solve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreconditioner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_tridiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_random_probes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Final values to return\u001b[39;00m\n\u001b[0;32m    135\u001b[0m logdet_term \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(linear_op\u001b[38;5;241m.\u001b[39mbatch_shape, dtype\u001b[38;5;241m=\u001b[39mctx\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mctx\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:795\u001b[0m, in \u001b[0;36mLinearOperator._solve\u001b[1;34m(self, rhs, preconditioner, num_tridiag)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_solve\u001b[39m(\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... N N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    782\u001b[0m     rhs: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... N C\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m     ],\n\u001b[0;32m    791\u001b[0m ]:\n\u001b[0;32m    792\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;124;03m    TODO\u001b[39;00m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_cg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_tridiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_tridiag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_cg_iterations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tridiag_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_lanczos_quadrature_iterations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreconditioner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreconditioner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\utils\\linear_cg.py:248\u001b[0m, in \u001b[0;36mlinear_cg\u001b[1;34m(matmul_closure, rhs, n_tridiag, tolerance, eps, stop_updating_after, max_iter, max_tridiag_iter, initial_guess, preconditioner)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# Start the iteration\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;66;03m# Get next alpha\u001b[39;00m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;66;03m# alpha_{k} = (residual_{k-1}^T precon_residual{k-1}) / (p_vec_{k-1}^T mat p_vec_{k-1})\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m     mvms \u001b[38;5;241m=\u001b[39m \u001b[43mmatmul_closure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_conjugate_vec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m precond:\n\u001b[0;32m    250\u001b[0m         torch\u001b[38;5;241m.\u001b[39mmul(curr_conjugate_vec, mvms, out\u001b[38;5;241m=\u001b[39mmul_storage)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\operators\\added_diag_linear_operator.py:77\u001b[0m, in \u001b[0;36mAddedDiagLinearOperator._matmul\u001b[1;34m(self, rhs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_matmul\u001b[39m(\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     75\u001b[0m     rhs: Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[0;32m     76\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M\u001b[39m\u001b[38;5;124m\"\u001b[39m]]:\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39maddcmul(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_linear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diag_tensor\u001b[38;5;241m.\u001b[39m_diag\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), rhs)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\operators\\interpolated_linear_operator.py:213\u001b[0m, in \u001b[0;36mInterpolatedLinearOperator._matmul\u001b[1;34m(self, rhs)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# left_interp * base_linear_op * right_interp^T * rhs\u001b[39;00m\n\u001b[0;32m    212\u001b[0m left_interp_mat \u001b[38;5;241m=\u001b[39m left_interp_t\u001b[38;5;241m.\u001b[39mmT\n\u001b[1;32m--> 213\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbdsmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_interp_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Squeeze if necessary\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vector:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\utils\\sparse.py:130\u001b[0m, in \u001b[0;36mbdsmm\u001b[1;34m(sparse, dense)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdsmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import statistics\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_SIZE, TEST_SIZE = 400_000, 20_000\n",
    "N_REPLICATES = n_replicates  # defined elsewhere\n",
    "TRAIN_ITERS = 60\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "def make_loader(x, y, shuffle=False):\n",
    "    return DataLoader(TensorDataset(x, y), batch_size=BATCH_SIZE, shuffle=shuffle)\n",
    "\n",
    "# MLP feature extractor\n",
    "class FeatureNet(torch.nn.Sequential):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__(\n",
    "            torch.nn.Linear(in_dim, 1000), torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 500),   torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 50),     torch.nn.ReLU(),\n",
    "            torch.nn.Linear(50, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, x, y, likelihood, feat_net):\n",
    "        super().__init__(x, y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5)),\n",
    "            num_dims=1, grid_size=100\n",
    "        )\n",
    "        self.feat_net = feat_net\n",
    "        self.scale = gpytorch.utils.grid.ScaleToBounds(-1.0, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        proj = self.scale(self.feat_net(x))\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(\n",
    "            self.mean_module(proj),\n",
    "            self.covar_module(proj)\n",
    "        )\n",
    "\n",
    "\n",
    "mse_l_dkl, time_l_dkl = [], []\n",
    "\n",
    "for i in range(N_REPLICATES):\n",
    "    tx, ty, vx, vy = splitter(\n",
    "        all_data, n_train=TRAIN_SIZE, n_test=TEST_SIZE,\n",
    "        random_state=42 + i, move_to_gpu=gpu\n",
    "    )\n",
    "    if USE_GPU:\n",
    "        tx, ty, vx, vy = tx.cuda(), ty.cuda(), vx.cuda(), vy.cuda()\n",
    "\n",
    "    mem_begin = get_mem()\n",
    "\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPModel(tx, ty, likelihood, FeatureNet(tx.size(-1)))\n",
    "    if USE_GPU:\n",
    "        model, likelihood = model.cuda(), likelihood.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(model.feat_net.parameters()) +\n",
    "        list(model.covar_module.parameters()) +\n",
    "        list(model.mean_module.parameters()) +\n",
    "        list(likelihood.parameters()),\n",
    "        lr=0.02\n",
    "    )\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "    model.train(); likelihood.train()\n",
    "    iterator = tqdm.tqdm(range(TRAIN_ITERS), leave=True)\n",
    "    start = time.time()\n",
    "    for _ in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(tx)\n",
    "        loss = -mll(output, ty)\n",
    "        loss.backward()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "        vram_usage()\n",
    "        optimizer.step()\n",
    "    uTime = time.time() - start\n",
    "    print(uTime)\n",
    "\n",
    "\n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"Memory Usage:\", mem_diff, \"MB\")\n",
    "\n",
    "\n",
    "    model.eval(); likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        preds = likelihood(model(vx))\n",
    "    peak_alloc, peak_reserved, gpu_used, sys_used = log_memory()\n",
    "\n",
    "    means = preds.mean.cpu()\n",
    "    MSE = torch.mean((means - vy.cpu())**2)\n",
    "    mse_l_dkl.append(MSE.item())\n",
    "    time_l_dkl.append(uTime)\n",
    "\n",
    "    print(\n",
    "        f\"DKL: Rep {i+1}: \"\n",
    "        f\"MSE={MSE:.4f}, \"\n",
    "        f\"Time={uTime:.2f}s, \"\n",
    "        f\"RAM Î”={mem_diff:.2f}MB, \"\n",
    "        f\"VRAM peak={peak_alloc:.2f}MB\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\n",
    "    round(statistics.mean(mse_l_dkl), 5),\n",
    "    round(statistics.stdev(mse_l_dkl), 5),\n",
    "    round(statistics.mean(time_l_dkl), 5),\n",
    "    round(statistics.stdev(time_l_dkl), 5)\n",
    ")\n",
    "clear_gpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f447ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6aae2815-2c3a-4ea5-85bc-e7d80e7c52c9",
   "metadata": {},
   "source": [
    "# Sparse GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe2fcd-83d3-4459-b120-9fe92575fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        base = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2))\n",
    "        ip = train_x[::1500].clone()\n",
    "        self.covar_module = gpytorch.kernels.InducingPointKernel(base, inducing_points=ip, likelihood=likelihood)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gpytorch.distributions.MultivariateNormal(self.mean_module(x), self.covar_module(x))\n",
    "\n",
    "my_batch_size = 320\n",
    "smoke_test = False\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_SIZE, TEST_SIZE = 400_000, 20_000\n",
    "N_REPLICATES = n_replicates\n",
    "TRAIN_ITERS = 60\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "\n",
    "mse_l_sgpr = []\n",
    "time_l_sgpr = []\n",
    "\n",
    "for i in range(N_REPLICATES):\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_data, n_train=TRAIN_SIZE, n_test=TEST_SIZE,\n",
    "        random_state=42 + i, move_to_gpu=gpu\n",
    "    )\n",
    "    test_loader = DataLoader(TensorDataset(test_x, test_y), batch_size=my_batch_size, shuffle=False)\n",
    "    mem_begin = get_mem()\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood().double()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood).double()\n",
    "    if gpu:\n",
    "        model, likelihood = model.cuda(), likelihood.cuda()\n",
    "\n",
    "    training_iterations = 350\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    def train():\n",
    "        for _ in tqdm.tqdm(range(training_iterations), desc=\"Train\"):\n",
    "            optimizer.zero_grad()\n",
    "            loss = -mll(model(train_x), train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            vram_usage()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    begin = time.time()\n",
    "    train()\n",
    "    uTime = time.time() - begin\n",
    "    print(\"Time:\", uTime)\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, _ in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "\n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    peak_alloc, peak_reserved, gpu_used, sys_used = log_memory()\n",
    "\n",
    "    MSE = torch.mean((means - test_y.cpu()) ** 2)\n",
    "    mse_l_sgpr.append(MSE.item())\n",
    "    time_l_sgpr.append(uTime)\n",
    "\n",
    "    print(\n",
    "        f\"SGPR: Rep {i+1}: MSE={MSE:.4f}, Time={uTime:.2f}s, \"\n",
    "        f\"RAM Î”={mem_diff:.2f}MB, VRAM peak={peak_alloc:.2f}MB\"\n",
    "    )\n",
    "    clear_gpu()\n",
    "\n",
    "print(statistics.mean(mse_l_sgpr), statistics.stdev(mse_l_sgpr))\n",
    "print(statistics.mean(time_l_sgpr), statistics.stdev(time_l_sgpr))\n",
    "print(\n",
    "    round(statistics.mean(mse_l_sgpr), 5),\n",
    "    round(statistics.stdev(mse_l_sgpr), 5),\n",
    "    round(statistics.mean(time_l_sgpr), 5),\n",
    "    round(statistics.stdev(time_l_sgpr), 5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aacf9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f63e92cd-510c-4dd4-8577-bf9ad4d0dfe7",
   "metadata": {},
   "source": [
    "# LOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91503e-88f6-4084-adfe-79aadc7fb7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch_size = 3200\n",
    "smoke_test = False\n",
    "train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=my_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_x, test_y), batch_size=320, shuffle=False)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(input_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 1))\n",
    "        print(\"VRAM Usage:\", torch.cuda.memory_allocated() / (1024**2), \"MB\")\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=1)),\n",
    "            grid_size=100, num_dims=1\n",
    "        )\n",
    "        self.feature_extractor = LargeFeatureExtractor(input_dim=train_x.size(-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected_x = self.feature_extractor(x)\n",
    "        projected_x = projected_x - projected_x.min(0)[0]\n",
    "        projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "        mean_x = self.mean_module(projected_x)\n",
    "        covar_x = self.covar_module(projected_x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "mse_l_love = []\n",
    "time_l_love = []\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_SIZE, TEST_SIZE = 400_000, 20_000\n",
    "\n",
    "for i in range(n_replicates):\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_data, n_train=TRAIN_SIZE, n_test=TEST_SIZE,\n",
    "        random_state=42 + i, move_to_gpu=gpu\n",
    "    )\n",
    "    test_loader = DataLoader(TensorDataset(test_x, test_y), batch_size=320, shuffle=False)\n",
    "    mem_begin = get_mem()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    if gpu:\n",
    "        model, likelihood = model.cuda(), likelihood.cuda()\n",
    "    training_iterations = 40\n",
    "    model.train(); likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    def train():\n",
    "        for _ in tqdm.tqdm(range(training_iterations)):\n",
    "            optimizer.zero_grad()\n",
    "            loss = -mll(model(train_x), train_y)\n",
    "            loss.backward()\n",
    "            vram_usage()\n",
    "            optimizer.step()\n",
    "\n",
    "    begin = time.time(); train(); uTime = time.time() - begin\n",
    "    print(\"Time:\", uTime)\n",
    "    model.eval(); likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x.to('cuda') if gpu else test_x))\n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    peak_alloc, peak_reserved, gpu_used, sys_used = log_memory()\n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu()) ** 2)\n",
    "    mse_l_love.append(MSE.item()); time_l_love.append(uTime)\n",
    "    print(\n",
    "        f\"LOVE: Rep {i+1}: MSE={MSE:.4f}, Time={uTime:.2f}s, \"\n",
    "        f\"RAM Î”={mem_diff:.2f}MB, VRAM peak={peak_alloc:.2f}MB\"\n",
    "    )\n",
    "    clear_gpu()\n",
    "\n",
    "print(statistics.mean(mse_l_love), statistics.stdev(mse_l_love))\n",
    "print(statistics.mean(time_l_love), statistics.stdev(time_l_love))\n",
    "print(\n",
    "    round(statistics.mean(mse_l_love), 5),\n",
    "    round(statistics.stdev(mse_l_love), 5),\n",
    "    round(statistics.mean(time_l_love), 5),\n",
    "    round(statistics.stdev(time_l_love), 5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c952be-27ac-4038-99eb-e670ec4b8b34",
   "metadata": {},
   "source": [
    "# NGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeed018-ae3b-4b7d-a1cb-990009d4eaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM Usage: 8.6396484375 MB\n",
      "VRAM Usage: 8.6396484375 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:15<?, ?it/s]                             \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m variational_ngd_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     79\u001b[0m hyperparameter_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 80\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, y_batch)\n\u001b[0;32m     82\u001b[0m minibatch_iter\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\models\\approximate_gp.py:114\u001b[0m, in \u001b[0;36mApproximateGP.__call__\u001b[1;34m(self, inputs, prior, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    113\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariational_strategy(inputs, prior\u001b[38;5;241m=\u001b[39mprior, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\variational\\variational_strategy.py:239\u001b[0m, in \u001b[0;36mVariationalStrategy.__call__\u001b[1;34m(self, x, prior, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, prior: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MultivariateNormal:\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdated_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prior:\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    241\u001b[0m             \u001b[38;5;66;03m# Get unwhitened p(u)\u001b[39;00m\n\u001b[0;32m    242\u001b[0m             prior_function_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minducing_points, prior\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_batch_size = 320\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=my_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_x, test_y), batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "mse_l_ngd = []\n",
    "time_l_ngd = []\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_SIZE, TEST_SIZE = 400_000, 20_000\n",
    "\n",
    "for i in range(n_replicates):\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_data, n_train=TRAIN_SIZE, n_test=TEST_SIZE,\n",
    "        random_state=42 + i, move_to_gpu=gpu\n",
    "    )\n",
    "    train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=my_batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(test_x, test_y), batch_size=my_batch_size, shuffle=False)\n",
    "    mem_begin = get_mem()\n",
    "\n",
    "    inducing_points = train_x[::400]\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "    if gpu:\n",
    "        model, likelihood = model.cuda(), likelihood.cuda()\n",
    "\n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(\n",
    "        model.variational_parameters(), num_data=train_y.size(0), lr=0.01\n",
    "    )\n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.1)\n",
    "\n",
    "    print(\"VRAM Usage:\", torch.cuda.memory_allocated() / (1024**2), \"MB\")\n",
    "\n",
    "    model.train(); likelihood.train()\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "    print(\"VRAM Usage:\", torch.cuda.memory_allocated() / (1024**2), \"MB\")\n",
    "\n",
    "    num_epochs = 1\n",
    "    begin = time.time()\n",
    "\n",
    "    for _ in tqdm.tqdm(range(num_epochs), desc=\"Epoch\"):\n",
    "        minibatch = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "        for x_batch, y_batch in minibatch:\n",
    "            variational_ngd_optimizer.zero_grad(); hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step(); hyperparameter_optimizer.step()\n",
    "\n",
    "    uTime = time.time() - begin\n",
    "    print(\"Time:\", uTime)\n",
    "\n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"Memory Usage:\", mem_diff / (1024 ** 2), \"MB\")\n",
    "\n",
    "    model.eval(); likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, _ in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "\n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    peak_alloc, peak_reserved, gpu_used, sys_used = log_memory()\n",
    "\n",
    "    MSE = torch.mean((means - test_y.cpu()) ** 2)\n",
    "    mse_l_ngd.append(MSE.item()); time_l_ngd.append(uTime)\n",
    "\n",
    "    print(\n",
    "        f\"LOVE: Rep {i+1}: MSE={MSE:.4f}, Time={uTime:.2f}s, \"\n",
    "        f\"RAM Î”={mem_diff:.2f}MB, VRAM peak={peak_alloc:.2f}MB\"\n",
    "    )\n",
    "    clear_gpu()\n",
    "\n",
    "print(statistics.mean(mse_l_ngd), statistics.stdev(mse_l_ngd))\n",
    "print(statistics.mean(time_l_ngd), statistics.stdev(time_l_ngd))\n",
    "print(\n",
    "    round(statistics.mean(mse_l_ngd), 5),\n",
    "    round(statistics.stdev(mse_l_ngd), 5),\n",
    "    round(statistics.mean(time_l_ngd), 5),\n",
    "    round(statistics.stdev(time_l_ngd), 5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a445a-b249-48da-be6e-d5937da43568",
   "metadata": {},
   "source": [
    "# SVGP_CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9020718-e748-44ac-b8e4-7f5813f65419",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch_size = 3200\n",
    "smoke_test = False\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "inducing_points = train_x[::5000]\n",
    "\n",
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.CiqVariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2)\n",
    "        )\n",
    "        self.covar_module.base_kernel.initialize(lengthscale=0.01)  # Specific to the 3droad dataset\n",
    "        print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "mse_l_svgpci = []\n",
    "time_l_svgpci = []\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "my_batch_size = 3200\n",
    "TRAIN_SIZE, TEST_SIZE = 400_000, 20_000\n",
    "for i in np.arange(0,n_replicates):\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_data, n_train=TRAIN_SIZE, n_test=TEST_SIZE,\n",
    "        random_state=42 + i, move_to_gpu=gpu\n",
    "    )\n",
    "    train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=320, shuffle=False)\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=train_y.size(0), lr=0.1)\n",
    "    \n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.002) #0.01 for 100k, 0.002 for Dense\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    \n",
    "    num_epochs = 4#10\n",
    "    \n",
    "    begin = time.time()\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "    for i in epochs_iter:\n",
    "        minibatch_iter = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False, position = 0)\n",
    "    \n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "            vram_usage()\n",
    "            hyperparameter_optimizer.step()\n",
    "\n",
    "    uTime = time.time()-begin\n",
    "    print(\"Time: \", time.time()-begin)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    mem_diff = get_mem()-mem_begin\n",
    "    peak_alloc, peak_reserved, gpu_used, sys_used = log_memory()\n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    mse_l_svgpci.append(MSE.item())\n",
    "    time_l_svgpci.append(uTime)\n",
    "    print(\n",
    "        f\"LOVE: Rep {i+1}: \"\n",
    "        f\"MSE={MSE:.4f}, \"\n",
    "        f\"Time={uTime:.2f}s, \"\n",
    "        f\"RAM Î”={mem_diff:.2f}MB, \"\n",
    "        f\"VRAM peak={peak_alloc:.2f}MB\"\n",
    "    )\n",
    "    clear_gpu()\n",
    "\n",
    "print(statistics.mean(mse_l_svgpci))\n",
    "print(statistics.stdev(mse_l_svgpci))\n",
    "\n",
    "print(statistics.mean(time_l_svgpci))\n",
    "print(statistics.stdev(time_l_svgpci))\n",
    "\n",
    "print(round(statistics.mean(mse_l_svgpci),5),round(statistics.stdev(mse_l_svgpci),5), round(statistics.mean(time_l_svgpci),5), round(statistics.stdev(time_l_svgpci),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ac0a0-1575-4237-852e-385f23a4f791",
   "metadata": {},
   "source": [
    "# SVGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d8af0-7edb-4c15-80a3-e6eb62eda014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM:  2.391636371612549e-06\n",
      "RAM:  2.391636371612549e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs), leave \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, position \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# Within each iteration, we will go over each minibatch of data\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     minibatch_iter \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinibatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, position \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m minibatch_iter:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m#for x_batch, y_batch in train_loader:\u001b[39;00m\n\u001b[0;32m     94\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     95\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(x_batch)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_batch_size = 3200\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=False)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5))\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "mse_l_svgp = []\n",
    "time_l_svgp = []\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_SIZE, TEST_SIZE = 400_000, 20_000\n",
    "for i in np.arange(0,n_replicates):\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_data, n_train=TRAIN_SIZE, n_test=TEST_SIZE,\n",
    "        random_state=42 + i, move_to_gpu=gpu\n",
    "    )\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=320, shuffle=False)\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    inducing_points = train_x[::500]\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"RAM: \", mem_diff / (1024 ** 2))\n",
    "    num_epochs = 5\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.001)\n",
    "\n",
    "    #mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"RAM: \", mem_diff / (1024 ** 2))\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "    \n",
    "    begin = time.time()\n",
    "    for i in tqdm.tqdm(range(num_epochs), leave = False, position = 0):\n",
    "        minibatch_iter = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False, position = 0)\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            max_ram = max(max_ram, (get_mem() - mem_begin))\n",
    "            optimizer.step()\n",
    "            if gpu:\n",
    "                max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "            i + 1, num_epochs, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item(),\n",
    "            likelihood.noise.item()\n",
    "        ))\n",
    "    uTime = time.time()-begin\n",
    "    print(\"Time: \", time.time() - begin)\n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"RAM: \", max_ram / (1024 ** 2))\n",
    "    print(\"VRAM: \", max_vram / (1024 ** 2))\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "\n",
    "     \n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    mem_diff = get_mem()-mem_begin\n",
    "    peak_alloc, peak_reserved, gpu_used, sys_used = log_memory()\n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    mse_l_svgp.append(MSE.item())\n",
    "    time_l_svgp.append(uTime)\n",
    "    print(\n",
    "        f\"LOVE: Rep {i+1}: \"\n",
    "        f\"MSE={MSE:.4f}, \"\n",
    "        f\"Time={uTime:.2f}s, \"\n",
    "        f\"RAM Î”={mem_diff:.2f}MB, \"\n",
    "        f\"VRAM peak={peak_alloc:.2f}MB\"\n",
    "    )\n",
    "    clear_gpu()\n",
    "\n",
    "print(statistics.mean(mse_l_svgp))\n",
    "print(statistics.stdev(mse_l_svgp))\n",
    "\n",
    "print(statistics.mean(time_l_svgp))\n",
    "print(statistics.stdev(time_l_svgp))\n",
    "\n",
    "print(round(statistics.mean(mse_l_svgp),5),round(statistics.stdev(mse_l_svgp),5), round(statistics.mean(time_l_svgp),5), round(statistics.stdev(time_l_svgp),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4ccda-39fa-4f20-8b0c-eb815d86c99f",
   "metadata": {},
   "source": [
    "# SKI - Can only handle up to 40,000 datapoints before running out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85087eb9-8b3b-4dad-9125-4ebefd372189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = None\n",
    "likelihood = None\n",
    "\n",
    "if gpu:\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb226a15-2d04-4e1c-9e88-1733fb104f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_ski = train_x[::2]\n",
    "train_y_ski = train_y[::2]\n",
    "\n",
    "if gpu:\n",
    "    train_x_ski, train_y_ski = train_x_ski.cuda(), train_y_ski.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24869027-c361-400b-a950-33cdab17cac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/15 [00:00<?, ?it/s]c:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\utils\\sparse.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if nonzero_indices.storage():\n",
      "c:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\utils\\sparse.py:66: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:80.)\n",
      "  res = cls(index_tensor, value_tensor, interp_size)\n",
      "c:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\utils\\sparse.py:66: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:646.)\n",
      "  res = cls(index_tensor, value_tensor, interp_size)\n",
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.958143711090088\n",
      "RAM:  0.0004656873643398285\n",
      "VRAM:  32.3515625\n",
      "[PyTorch] Max Allocated: 8291.03 MB | Max Reserved: 10416.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 12057.15 MB | [System RAM]: 23.40 GB\n",
      "LOVE: Rep 15: MSE=0.1282, Time=7.96s, RAM Î”=488.31MB, VRAM peak=8291.03MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.547093391418457\n",
      "RAM:  1.4156103134155273e-07\n",
      "VRAM:  32.81884765625\n",
      "[PyTorch] Max Allocated: 8291.50 MB | Max Reserved: 10412.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 12015.09 MB | [System RAM]: 23.40 GB\n",
      "LOVE: Rep 15: MSE=0.1275, Time=7.55s, RAM Î”=0.15MB, VRAM peak=8291.50MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m     max_vram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_vram, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated())\n\u001b[0;32m     88\u001b[0m output \u001b[38;5;241m=\u001b[39m model(train_x)\n\u001b[1;32m---> 89\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gpu:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\mlls\\exact_marginal_log_likelihood.py:82\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[1;34m(self, function_dist, target, *params, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN observation policy \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported by ExactMarginalLogLikelihood!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_other_terms(res, params)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py:192\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    185\u001b[0m         covar \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39mrepeat(\n\u001b[0;32m    186\u001b[0m             \u001b[38;5;241m*\u001b[39m(diff_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m covar_size \u001b[38;5;28;01mfor\u001b[39;00m diff_size, covar_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(diff\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], padded_batch_shape)),\n\u001b[0;32m    187\u001b[0m             \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    189\u001b[0m         )\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m covar \u001b[38;5;241m=\u001b[39m \u001b[43mcovar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m inv_quad, logdet \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39minv_quad_logdet(inv_quad_rhs\u001b[38;5;241m=\u001b[39mdiff\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), logdet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    195\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m([inv_quad, logdet, diff\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)])\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\operators\\added_diag_linear_operator.py:209\u001b[0m, in \u001b[0;36mAddedDiagLinearOperator.evaluate_kernel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_kernel\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 209\u001b[0m     added_diag_linear_op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation())\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m added_diag_linear_op\u001b[38;5;241m.\u001b[39m_linear_op \u001b[38;5;241m+\u001b[39m added_diag_linear_op\u001b[38;5;241m.\u001b[39m_diag_tensor\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:2072\u001b[0m, in \u001b[0;36mLinearOperator.representation_tree\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2062\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepresentation_tree\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LinearOperatorRepresentationTree:\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2064\u001b[0m \u001b[38;5;124;03m    Returns a\u001b[39;00m\n\u001b[0;32m   2065\u001b[0m \u001b[38;5;124;03m    :obj:`linear_operator.operators.LinearOperatorRepresentationTree` tree\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;124;03m    including all subobjects. This is used internally.\u001b[39;00m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2072\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLinearOperatorRepresentationTree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\operators\\linear_operator_representation_tree.py:15\u001b[0m, in \u001b[0;36mLinearOperatorRepresentationTree.__init__\u001b[1;34m(self, linear_op)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(linear_op\u001b[38;5;241m.\u001b[39m_args, linear_op\u001b[38;5;241m.\u001b[39m_differentiable_kwargs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepresentation\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(arg\u001b[38;5;241m.\u001b[39mrepresentation):  \u001b[38;5;66;03m# Is it a lazy tensor?\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m         representation_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mslice\u001b[39m(counter, counter \u001b[38;5;241m+\u001b[39m representation_size, \u001b[38;5;28;01mNone\u001b[39;00m), arg\u001b[38;5;241m.\u001b[39mrepresentation_tree()))\n\u001b[0;32m     17\u001b[0m         counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m representation_size\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:397\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.representation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrepresentation()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# Otherwise, we'll evaluate the kernel (or at least its LinearOperator representation) and use its\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;66;03m# representation\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrepresentation()\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:25\u001b[0m, in \u001b[0;36mrecall_grad_state.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_grad_enabled):\n\u001b[1;32m---> 25\u001b[0m         output \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:355\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m     temp_active_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel(\n\u001b[0;32m    356\u001b[0m         x1,\n\u001b[0;32m    357\u001b[0m         x2,\n\u001b[0;32m    358\u001b[0m         diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    359\u001b[0m         last_dim_is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_dim_is_batch,\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams,\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m temp_active_dims\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Check the size of the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\kernels\\kernel.py:539\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m    536\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    538\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[1;32m--> 539\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Kernel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x1_, x2_, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    540\u001b[0m     )\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\kernels\\scale_kernel.py:109\u001b[0m, in \u001b[0;36mScaleKernel.forward\u001b[1;34m(self, x1, x2, last_dim_is_batch, diag, **params)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, last_dim_is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m--> 109\u001b[0m     orig_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_kernel\u001b[38;5;241m.\u001b[39mforward(x1, x2, diag\u001b[38;5;241m=\u001b[39mdiag, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    110\u001b[0m     outputscales \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputscale\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m last_dim_is_batch:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\kernels\\grid_interpolation_kernel.py:155\u001b[0m, in \u001b[0;36mGridInterpolationKernel.forward\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x1\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dims), x2\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dims)])\n\u001b[1;32m--> 155\u001b[0m x_maxs \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m x_mins \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmin(\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# We need to update the grid if\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# 1) it hasn't ever been initialized, or\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# 2) if any of the grid points are \"out of bounds\"\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, GridInterpolationKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        grid_size = gpytorch.utils.grid.choose_grid_size(train_x, 1)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.MaternKernel(nu=1.5), grid_size=grid_size, num_dims=2\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iterations = 32\n",
    "\n",
    "\n",
    "mse_l_ski = []\n",
    "time_l_ski = []\n",
    "\n",
    "# Config\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_SIZE, TEST_SIZE = 40_000, 20_000\n",
    "for i in np.arange(0,n_replicates):\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_data, n_train=TRAIN_SIZE, n_test=TEST_SIZE,\n",
    "        random_state=42 + i, move_to_gpu=gpu\n",
    "    )\n",
    "    train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=320, shuffle=False)\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    if gpu:\n",
    "        mll = mll.cuda()\n",
    "\n",
    "    \n",
    "    training_iterations = 15\n",
    "    begin = time.time()\n",
    "    \n",
    "    for i in tqdm.tqdm(range(training_iterations), desc=\"Train\", leave = False, position = 0 ):\n",
    "        optimizer.zero_grad()\n",
    "        if gpu:\n",
    "            max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        if gpu:\n",
    "            max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        optimizer.step()\n",
    "\n",
    "    uTime = time.time()-begin\n",
    "    print(time.time()-begin)\n",
    "    print(\"RAM: \",(get_mem() - mem_begin)/(1024**2))\n",
    "    print(\"VRAM: \", max_vram / (1024 ** 2))\n",
    "    \n",
    "    model.eval()\n",
    "    with gpytorch.settings.prior_mode():\n",
    "        output = (model(test_x))\n",
    "    mem_diff = get_mem()-mem_begin\n",
    "    peak_alloc, peak_reserved, gpu_used, sys_used = log_memory()\n",
    "    means = output.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    mse_l_ski.append(MSE.item())\n",
    "    time_l_ski.append(uTime)\n",
    "    print(\n",
    "        f\"LOVE: Rep {i+1}: \"\n",
    "        f\"MSE={MSE:.4f}, \"\n",
    "        f\"Time={uTime:.2f}s, \"\n",
    "        f\"RAM Î”={mem_diff:.2f}MB, \"\n",
    "        f\"VRAM peak={peak_alloc:.2f}MB\"\n",
    "    )\n",
    "    clear_gpu()\n",
    "\n",
    "print(statistics.mean(mse_l_ski))\n",
    "print(statistics.stdev(mse_l_ski))\n",
    "print(statistics.mean(time_l_ski))\n",
    "print(statistics.stdev(time_l_ski))\n",
    "print(round(statistics.mean(mse_l_ski),5),round(statistics.stdev(mse_l_ski),5), round(statistics.mean(time_l_ski),5), round(statistics.stdev(time_l_ski),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc3f645-92de-4bed-86da-6c77d005c177",
   "metadata": {},
   "source": [
    "# VNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea0983ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aecce4-4e59-48d5-a6ab-b89d65c77184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replicate:  0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\faiss\\contrib\\torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  x.storage().data_ptr() + x.storage_offset() * 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "22\n",
      "23\n",
      "3\n",
      "561.1702280044556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 50.03it/s, loss=0.327]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 49.18it/s, loss=0.0305]   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 48.42it/s, loss=-0.376]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:05<00:00, 54.12it/s, loss=0.168]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:07<00:00, 43.79it/s, loss=-0.175]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 52.22it/s, loss=-0.148] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 45.76it/s, loss=-0.11]  \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 47.13it/s, loss=-0.183] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:05<00:00, 53.39it/s, loss=-0.0609]  \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:07<00:00, 41.18it/s, loss=-0.127]\n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:05<00:00,  6.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  65.19755530357361\n",
      "VRAM:  1673.62890625\n",
      "RAM:  0.0006361007690429688\n",
      "[PyTorch] Max Allocated: 1676.71 MB | Max Reserved: 1708.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 5176.22 MB | [System RAM]: 24.06 GB\n",
      "VNN: Rep 314: MSE=0.1021, Time=65.20s, RAM Î”=662.16MB, VRAM peak=1676.71MB\n",
      "Replicate:  1\n",
      "1\n",
      "2\n",
      "21\n",
      "22\n",
      "23\n",
      "3\n",
      "616.663957118988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 51.84it/s, loss=0.36] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:05<00:00, 52.73it/s, loss=-0.011]   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 51.31it/s, loss=-0.407]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 50.79it/s, loss=0.188]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 52.25it/s, loss=-0.254]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 48.91it/s, loss=-0.142] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 50.57it/s, loss=-0.0883] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 50.77it/s, loss=-0.152] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 50.55it/s, loss=-0.0171]  \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 51.33it/s, loss=-0.226]\n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:01<00:00,  6.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  61.50834941864014\n",
      "VRAM:  1673.62890625\n",
      "RAM:  0.00023777782917022705\n",
      "[PyTorch] Max Allocated: 1676.72 MB | Max Reserved: 1708.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 5123.16 MB | [System RAM]: 24.11 GB\n",
      "VNN: Rep 314: MSE=0.1002, Time=61.51s, RAM Î”=245.66MB, VRAM peak=1676.72MB\n",
      "Replicate:  2\n",
      "1\n",
      "2\n",
      "21\n",
      "22\n",
      "23\n",
      "3\n",
      "576.545111656189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 50.40it/s, loss=0.367]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 47.55it/s, loss=0.0081]   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 50.65it/s, loss=-0.332]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:07<00:00, 43.15it/s, loss=0.179]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:05<00:00, 53.15it/s, loss=-0.213]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 47.18it/s, loss=-0.184] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 50.16it/s, loss=-0.0966] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:05<00:00, 53.70it/s, loss=-0.159] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:07<00:00, 43.89it/s, loss=-0.00476] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:05<00:00, 53.96it/s, loss=-0.169]\n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:03<00:00,  6.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  63.982200384140015\n",
      "VRAM:  1673.712890625\n",
      "RAM:  0.00024169310927391052\n",
      "[PyTorch] Max Allocated: 1676.86 MB | Max Reserved: 1708.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 5094.04 MB | [System RAM]: 24.05 GB\n",
      "VNN: Rep 314: MSE=0.1001, Time=63.98s, RAM Î”=250.78MB, VRAM peak=1676.86MB\n",
      "Replicate:  3\n",
      "1\n",
      "2\n",
      "21\n",
      "22\n",
      "23\n",
      "3\n",
      "579.282312631607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:07<00:00, 43.91it/s, loss=0.452]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:05<00:00, 52.35it/s, loss=-0.0248]  \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:07<00:00, 43.69it/s, loss=-0.404]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 50.97it/s, loss=0.191]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 51.10it/s, loss=-0.139]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 47.49it/s, loss=-0.123]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:05<00:00, 54.75it/s, loss=-0.0166] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 47.05it/s, loss=-0.128]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 51.07it/s, loss=-0.00026] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 46.81it/s, loss=-0.193]\n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:04<00:00,  6.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  64.55292177200317\n",
      "VRAM:  1673.712890625\n",
      "RAM:  0.0002416856586933136\n",
      "[PyTorch] Max Allocated: 1676.69 MB | Max Reserved: 1708.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 5101.71 MB | [System RAM]: 24.11 GB\n",
      "VNN: Rep 314: MSE=0.1014, Time=64.55s, RAM Î”=250.68MB, VRAM peak=1676.69MB\n",
      "Replicate:  4\n",
      "1\n",
      "2\n",
      "21\n",
      "22\n",
      "23\n",
      "3\n",
      "580.2057065963745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 46.05it/s, loss=0.306]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:05<00:00, 55.14it/s, loss=0.0431]   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 45.54it/s, loss=-0.395]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 51.10it/s, loss=0.196]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 50.16it/s, loss=-0.202]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 47.11it/s, loss=-0.168] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:05<00:00, 55.53it/s, loss=-0.0774]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:07<00:00, 44.10it/s, loss=-0.136]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 49.31it/s, loss=-0.0497]  \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 45.92it/s, loss=-0.137]\n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:04<00:00,  6.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  64.51261234283447\n",
      "VRAM:  1673.712890625\n",
      "RAM:  0.0002418719232082367\n",
      "[PyTorch] Max Allocated: 1676.86 MB | Max Reserved: 1708.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 5096.64 MB | [System RAM]: 24.05 GB\n",
      "VNN: Rep 314: MSE=0.1028, Time=64.51s, RAM Î”=250.89MB, VRAM peak=1676.86MB\n",
      "Replicate:  5\n",
      "1\n",
      "2\n",
      "21\n",
      "22\n",
      "23\n",
      "3\n",
      "585.2091274261475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:07<00:00, 43.99it/s, loss=0.315]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:05<00:00, 53.78it/s, loss=-0.0033]  \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 49.54it/s, loss=-0.428]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 48.41it/s, loss=0.171]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:05<00:00, 56.78it/s, loss=-0.207]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 49.02it/s, loss=-0.0909]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 52.25it/s, loss=-0.107] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 49.29it/s, loss=-0.138] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:06<00:00, 45.56it/s, loss=-0.0254]  \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:05<00:00, 58.01it/s, loss=-0.22] \n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:02<00:00,  6.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  62.447962284088135\n",
      "VRAM:  1673.71484375\n",
      "RAM:  0.0002416558563709259\n",
      "[PyTorch] Max Allocated: 1676.86 MB | Max Reserved: 1708.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 5059.91 MB | [System RAM]: 24.08 GB\n",
      "VNN: Rep 314: MSE=0.1008, Time=62.45s, RAM Î”=250.65MB, VRAM peak=1676.86MB\n",
      "Replicate:  6\n",
      "1\n",
      "2\n",
      "21\n",
      "22\n",
      "23\n",
      "3\n",
      "585.0397419929504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:04<00:00, 66.92it/s, loss=0.348]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:04<00:00, 69.36it/s, loss=-0.000761]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:04<00:00, 68.61it/s, loss=-0.363]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:04<00:00, 67.47it/s, loss=0.179]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:04<00:00, 68.49it/s, loss=-0.208]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:04<00:00, 69.30it/s, loss=-0.187] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:04<00:00, 69.47it/s, loss=-0.118] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:04<00:00, 69.05it/s, loss=-0.194] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:04<00:00, 67.46it/s, loss=-0.0166] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314/314 [00:04<00:00, 69.42it/s, loss=-0.176]\n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:45<00:00,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  45.82531785964966\n",
      "VRAM:  1673.71484375\n",
      "RAM:  0.0002415888011455536\n",
      "[PyTorch] Max Allocated: 1676.77 MB | Max Reserved: 1706.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 5241.78 MB | [System RAM]: 24.34 GB\n",
      "VNN: Rep 314: MSE=0.1021, Time=45.83s, RAM Î”=250.58MB, VRAM peak=1676.77MB\n",
      "Replicate:  7\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "my_batch_size = 32\n",
    "smoke_test = False\n",
    "\n",
    "import faiss\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points, likelihood, k=256, training_batch_size=256):\n",
    "        m, d = inducing_points.shape\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "        variational_distribution = gpytorch.variational.MeanFieldVariationalDistribution(m)\n",
    "        if gpu:\n",
    "            inducing_points = inducing_points.cuda()\n",
    "        variational_strategy = NNVariationalStrategy(self, inducing_points, variational_distribution, k=k,\n",
    "                                                     training_batch_size=training_batch_size)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=d))\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def __call__(self, x, prior=False, **kwargs):\n",
    "        if x is not None:\n",
    "            if x.dim() == 1:\n",
    "                x = x.unsqueeze(-1)\n",
    "        return self.variational_strategy(x=x, prior=False, **kwargs)\n",
    "\n",
    "begin = time.time()\n",
    "if smoke_test:\n",
    "    k = 32\n",
    "    training_batch_size = 32\n",
    "else:\n",
    "    k = 256\n",
    "    training_batch_size = 64\n",
    "k = 160\n",
    "training_batch_size = 320*4\n",
    "mse_l_vnn = []\n",
    "time_l_vnn = []\n",
    "my_batch_size = 32\n",
    "TRAIN_SIZE, TEST_SIZE = 400_000, 20_000\n",
    "for i in np.arange(0,n_replicates):\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_data, n_train=TRAIN_SIZE, n_test=TEST_SIZE,\n",
    "        random_state=42 + i, move_to_gpu=gpu\n",
    "    )\n",
    "    train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "    print(\"Replicate: \",i)\n",
    "    mem_begin = get_mem()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPModel(inducing_points=train_x[::1].contiguous(), likelihood=likelihood, k=k, training_batch_size=training_batch_size)\n",
    "    if gpu:\n",
    "        likelihood = likelihood.cuda()\n",
    "        model = model.cuda()\n",
    "    print(time.time()-begin)\n",
    "    num_epochs = 1 if smoke_test else 20\n",
    "    num_epochs = 10#30\n",
    "    num_batches = model.variational_strategy._total_training_batches\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    begin = time.time()\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\", leave=True, position = 0)\n",
    "    for epoch in epochs_iter:\n",
    "        minibatch_iter = tqdm.tqdm(range(num_batches), leave=True, position = 0)\n",
    "    \n",
    "        for i in minibatch_iter:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x=None)\n",
    "            current_training_indices = model.variational_strategy.current_training_indices\n",
    "            y_batch = train_y[...,current_training_indices]\n",
    "            if gpu:\n",
    "                y_batch = y_batch.cuda()\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            vram_usage()\n",
    "            optimizer.step()\n",
    "    uTime = time.time() - begin\n",
    "    print(\"Time: \", time.time() - begin)\n",
    "    print(\"VRAM: \", max_vram/(1024 ** 2))\n",
    "    print(\"RAM: \", (get_mem() - mem_begin)/(1024**2))\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    mem_diff = get_mem()-mem_begin\n",
    "    peak_alloc, peak_reserved, gpu_used, sys_used = log_memory()\n",
    "    means = means[1:]\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    mse_l_vnn.append(MSE.item())\n",
    "    time_l_vnn.append(uTime)\n",
    "\n",
    "    model = None\n",
    "    likelihood = None\n",
    "    mll = None\n",
    "    optimizer = None\n",
    "    epochs_iter = None\n",
    "    if gpu:\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\n",
    "        f\"VNN: Rep {i+1}: \"\n",
    "        f\"MSE={MSE:.4f}, \"\n",
    "        f\"Time={uTime:.2f}s, \"\n",
    "        f\"RAM Î”={mem_diff:.2f}MB, \"\n",
    "        f\"VRAM peak={peak_alloc:.2f}MB\"\n",
    "    )\n",
    "    clear_gpu()\n",
    "\n",
    "print(statistics.mean(mse_l_vnn))\n",
    "print(statistics.stdev(mse_l_vnn))\n",
    "print(statistics.mean(time_l_vnn))\n",
    "print(statistics.stdev(time_l_vnn))\n",
    "print(round(statistics.mean(mse_l_vnn),5),round(statistics.stdev(mse_l_vnn),5), round(statistics.mean(time_l_vnn),5), round(statistics.stdev(time_l_vnn),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d76261-c467-43ea-a72b-e0ec5a412fe1",
   "metadata": {},
   "source": [
    "# Compile Table (MSE and Time only)\n",
    "\n",
    "SKI\n",
    "SGPR\n",
    "LOVE\n",
    "DKL\n",
    "SVGP-CI\n",
    "SVGP\n",
    "NGD\n",
    "VNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SKI     --- MSE:\",statistics.mean(mse_l_ski), \"(\",statistics.stdev(mse_l_ski),\")  Time:\", statistics.mean(time_l_ski), \"(\",statistics.stdev(time_l_ski),\")\")\n",
    "print(\"SGPR    --- MSE:\",statistics.mean(mse_l_sgpr), \"(\",statistics.stdev(mse_l_sgpr),\")  Time:\", statistics.mean(time_l_sgpr), \"(\",statistics.stdev(time_l_sgpr),\")\")\n",
    "print(\"LOVE    --- MSE:\",statistics.mean(mse_l_love), \"(\",statistics.stdev(mse_l_love),\")  Time:\", statistics.mean(time_l_love), \"(\",statistics.stdev(time_l_love),\")\")\n",
    "print(\"DKL     --- MSE:\",statistics.mean(mse_l_dkl), \"(\",statistics.stdev(mse_l_dkl),\")  Time:\", statistics.mean(time_l_dkl), \"(\",statistics.stdev(time_l_dkl),\")\")\n",
    "print(\"SVGP-CI --- MSE:\",statistics.mean(mse_l_svgpci), \"(\",statistics.stdev(mse_l_svgpci),\")  Time:\", statistics.mean(time_l_svgpci), \"(\",statistics.stdev(time_l_svgpci),\")\")\n",
    "print(\"SVGP    --- MSE:\",statistics.mean(mse_l_svgp), \"(\",statistics.stdev(mse_l_svgp),\")  Time:\", statistics.mean(time_l_svgp), \"(\",statistics.stdev(time_l_svgp),\")\")\n",
    "print(\"NGD     --- MSE:\",statistics.mean(mse_l_ngd), \"(\",statistics.stdev(mse_l_ngd),\")  Time:\", statistics.mean(time_l_ngd), \"(\",statistics.stdev(time_l_ngd),\")\")\n",
    "print(\"VNN     --- MSE:\",statistics.mean(mse_l_vnn), \"(\",statistics.stdev(mse_l_vnn),\")  Time:\", statistics.mean(time_l_vnn), \"(\",statistics.stdev(time_l_vnn),\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b1b7aa",
   "metadata": {},
   "source": [
    "Reordering\n",
    "SVGP\n",
    "SVGP-CI\n",
    "VNN\n",
    "NGD\n",
    "DKL\n",
    "SGPR\n",
    "SKI\n",
    "LOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b5546-a5ea-4818-ba7d-37e7fdd2ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVGP    --- MSE:\",statistics.mean(mse_l_svgp), \"(\",statistics.stdev(mse_l_svgp),\")  Time:\", statistics.mean(time_l_svgp), \"(\",statistics.stdev(time_l_svgp),\")\")\n",
    "print(\"SVGP-CI --- MSE:\",statistics.mean(mse_l_svgpci), \"(\",statistics.stdev(mse_l_svgpci),\")  Time:\", statistics.mean(time_l_svgpci), \"(\",statistics.stdev(time_l_svgpci),\")\")\n",
    "print(\"VNN     --- MSE:\",statistics.mean(mse_l_vnn), \"(\",statistics.stdev(mse_l_vnn),\")  Time:\", statistics.mean(time_l_vnn), \"(\",statistics.stdev(time_l_vnn),\")\")\n",
    "print(\"NGD     --- MSE:\",statistics.mean(mse_l_ngd), \"(\",statistics.stdev(mse_l_ngd),\")  Time:\", statistics.mean(time_l_ngd), \"(\",statistics.stdev(time_l_ngd),\")\")\n",
    "print(\"DKL     --- MSE:\",statistics.mean(mse_l_dkl), \"(\",statistics.stdev(mse_l_dkl),\")  Time:\", statistics.mean(time_l_dkl), \"(\",statistics.stdev(time_l_dkl),\")\")\n",
    "print(\"SGPR    --- MSE:\",statistics.mean(mse_l_sgpr), \"(\",statistics.stdev(mse_l_sgpr),\")  Time:\", statistics.mean(time_l_sgpr), \"(\",statistics.stdev(time_l_sgpr),\")\")\n",
    "print(\"SKI     --- MSE:\",statistics.mean(mse_l_ski), \"(\",statistics.stdev(mse_l_ski),\")  Time:\", statistics.mean(time_l_ski), \"(\",statistics.stdev(time_l_ski),\")\")\n",
    "print(\"LOVE    --- MSE:\",statistics.mean(mse_l_love), \"(\",statistics.stdev(mse_l_love),\")  Time:\", statistics.mean(time_l_love), \"(\",statistics.stdev(time_l_love),\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf821f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "methods = [\n",
    "    (\"SVGP\",   mse_l_svgp,   time_l_svgp),\n",
    "    (\"SVGP-CI\",mse_l_svgpci, time_l_svgpci),\n",
    "    (\"NGD\",    mse_l_ngd,    time_l_ngd),\n",
    "    (\"DKL\",    mse_l_dkl,    time_l_dkl),\n",
    "    (\"SGPR\",   mse_l_sgpr,   time_l_sgpr),\n",
    "    (\"SKI\",    mse_l_ski,    time_l_ski),\n",
    "    (\"LOVE\",   mse_l_love,   time_l_love),\n",
    "]\n",
    "\n",
    "for name, mse_list, time_list in methods:\n",
    "    data_mse  = mse_list[1:]\n",
    "    data_time = time_list[1:]\n",
    "\n",
    "    mean_mse  = statistics.mean(data_mse)\n",
    "    sd_mse    = statistics.stdev(data_mse)\n",
    "    mean_time = statistics.mean(data_time)\n",
    "    sd_time   = statistics.stdev(data_time)\n",
    "    print(f\"{name:<8} & {mean_mse:.4f}  & ({sd_mse:.4f} )  & {mean_time:.4f}  & ({sd_time:.4f} )\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scaleGP)",
   "language": "python",
   "name": "scalegp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
