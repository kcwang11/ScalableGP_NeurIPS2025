{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b37894c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efacb388",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "To run this experiment, first run the Setup chunk, then run the RFF section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6605e5-f791-44f8-ada3-9efb2237ed1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80000, 2])\n",
      "2\n",
      "torch.Size([80000])\n",
      "torch.Size([20000, 2])\n",
      "torch.Size([20000])\n"
     ]
    }
   ],
   "source": [
    "gpu = True\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import urllib.request\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "import psutil\n",
    "import tqdm\n",
    "import faiss\n",
    "from math import floor\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from memory_profiler import memory_usage\n",
    "import pynvml\n",
    "\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.mlls import DeepApproximateMLL\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "def clear_gpu():\n",
    "    for obj in ['model', 'likelihood', 'observed_pred', 'preds', 'output']:\n",
    "        if obj in globals():\n",
    "            del globals()[obj]\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def get_mem():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 ** 2)  # Return MB\n",
    "\n",
    "max_vram = 0\n",
    "def vram_usage():\n",
    "    global max_vram\n",
    "    max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "\n",
    "def log_memory():\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    max_allocated = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "    max_reserved = torch.cuda.max_memory_reserved() / 1024**2    # MB\n",
    "    gpu_used = meminfo.used / 1024**2                            # MB\n",
    "    sys_used = psutil.virtual_memory().used / 1024**3            # GB\n",
    "    print(f\"[PyTorch] Max Allocated: {max_allocated:.2f} MB | Max Reserved: {max_reserved:.2f} MB\")\n",
    "    print(f\"[GPU VRAM] Used (nvidia-smi): {gpu_used:.2f} MB | [System RAM]: {sys_used:.2f} GB\")\n",
    "    return max_allocated, max_reserved, gpu_used, sys_used\n",
    "\n",
    "\n",
    "try:\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "    get_ipython().run_line_magic('autoreload', '2')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "csvfile = pd.read_csv('Data/data_2d.csv', header = None, dtype=float, delimiter=',')\n",
    "all_data = torch.tensor(np.array(csvfile)).float()\n",
    "\n",
    "def splitter(all_data, n_train=80_000, n_test=20_000, random_state=42, move_to_gpu=True):\n",
    "    assert all_data.ndim == 2 and all_data.shape[1] == 3, \\\n",
    "        \"all_data must be [N,3]\"\n",
    "    total_samples = all_data.shape[0]\n",
    "    assert n_train + n_test <= total_samples, \"Not enough samples to split\"\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "    indices = rng.permutation(total_samples)\n",
    "    train_idx = indices[:n_train]\n",
    "    test_idx  = indices[n_train:n_train + n_test]\n",
    "    train = all_data[train_idx]\n",
    "    test  = all_data[test_idx]\n",
    "\n",
    "    train_x = train[:, :2].contiguous()\n",
    "    train_y = train[:,  2].contiguous()\n",
    "    test_x  = test[:,  :2].contiguous()\n",
    "    test_y  = test[:,   2].contiguous()\n",
    "\n",
    "    if move_to_gpu and torch.cuda.is_available():\n",
    "        train_x = train_x.cuda()\n",
    "        train_y = train_y.cuda()\n",
    "        test_x  = test_x.cuda()\n",
    "        test_y  = test_y.cuda()\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "train_x, train_y, test_x, test_y = splitter(all_data, n_train= 80_000, n_test = 20_000, random_state=42, move_to_gpu=True)\n",
    "print(train_x.shape)\n",
    "print(train_x.size(-1))\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f697e43",
   "metadata": {},
   "source": [
    "# RFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fdf6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Replicate 1/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 1: MSE=0.1001, Time=10.69s, RAM before=0.0MB, peak=828.4MB (Δ=828.4MB), VRAM peak=12689.6MB\n",
      "\n",
      "=== Replicate 2/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 2: MSE=0.1009, Time=10.50s, RAM before=0.0MB, peak=978.1MB (Δ=978.1MB), VRAM peak=12814.9MB\n",
      "\n",
      "=== Replicate 3/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 3: MSE=0.1017, Time=10.50s, RAM before=0.0MB, peak=978.3MB (Δ=978.3MB), VRAM peak=12814.9MB\n",
      "\n",
      "=== Replicate 4/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     56\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 57\u001b[0m peak_ram \u001b[38;5;241m=\u001b[39m \u001b[43mmemory_usage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\n\u001b[0;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     64\u001b[0m vram_peak \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_allocated() \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\memory_profiler.py:379\u001b[0m, in \u001b[0;36mmemory_usage\u001b[1;34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend, max_iterations)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# When there is an exception in the \"proc\" - the (spawned) monitoring processes don't get killed.\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# Therefore, the whole process hangs indefinitely. Here, we are ensuring that the process gets killed!\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     returned \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    380\u001b[0m     parent_conn\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# finish timing\u001b[39;00m\n\u001b[0;32m    381\u001b[0m     ret \u001b[38;5;241m=\u001b[39m parent_conn\u001b[38;5;241m.\u001b[39mrecv()\n",
      "Cell \u001b[1;32mIn[2], line 50\u001b[0m, in \u001b[0;36mtrain_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     49\u001b[0m output \u001b[38;5;241m=\u001b[39m model(train_x)\n\u001b[1;32m---> 50\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\mlls\\exact_marginal_log_likelihood.py:82\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[1;34m(self, function_dist, target, *params, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN observation policy \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported by ExactMarginalLogLikelihood!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_other_terms(res, params)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py:177\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[0;32m    176\u001b[0m mean, covar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_covariance_matrix\n\u001b[1;32m--> 177\u001b[0m diff \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Repeat the covar to match the batch shape of diff\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m diff\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m covar\u001b[38;5;241m.\u001b[39mbatch_shape:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm, time, gc\n",
    "import torch, gpytorch\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RFFKernel(\n",
    "                num_samples=200,  # Adjust this line to choose the number of Fourier Features\n",
    "                num_dims=2\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gpytorch.distributions.MultivariateNormal(\n",
    "            self.mean_module(x),\n",
    "            self.covar_module(x)\n",
    "        )\n",
    "\n",
    "n_replicates = 11\n",
    "training_iterations = 64\n",
    "n_train, n_test = 40_000, 20_000\n",
    "random_state = 42\n",
    "mse_l_rff, time_l_rff = [], []\n",
    "\n",
    "for rep in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {rep + 1}/{n_replicates} ===\")\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_data, n_train=n_train, n_test=n_test,\n",
    "        random_state=42 + rep, move_to_gpu=gpu\n",
    "    )\n",
    "    ram_before = get_mem() / (1024**2)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train(); likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.25)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    if torch.cuda.is_available():\n",
    "        mll = mll.cuda()\n",
    "\n",
    "    def train_fn():\n",
    "        progress = tqdm.trange(training_iterations, desc=f\"Training (rep {rep+1})\", leave=False)\n",
    "        for _ in progress:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            progress.set_postfix(loss=loss.item())\n",
    "        return None\n",
    "\n",
    "    start_time = time.time()\n",
    "    peak_ram = memory_usage(\n",
    "        (train_fn,),\n",
    "        max_usage=True,\n",
    "        retval=False,\n",
    "        interval=0.01\n",
    "    )\n",
    "    elapsed = time.time() - start_time\n",
    "    vram_peak = torch.cuda.max_memory_allocated() / (1024**2) if torch.cuda.is_available() else None\n",
    "    ram_delta = peak_ram - ram_before\n",
    "    model.eval(); likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        pred = likelihood(model(test_x)).mean.cpu()\n",
    "    mse = torch.mean((pred - test_y.cpu()) ** 2).item()\n",
    "    mse_l_rff.append(mse)\n",
    "    time_l_rff.append(elapsed)\n",
    "    print(\n",
    "        f\"Rep {rep+1}: MSE={mse:.4f}, Time={elapsed:.2f}s, \"\n",
    "        f\"RAM before={ram_before:.1f}MB, peak={peak_ram:.1f}MB (Δ={ram_delta:.1f}MB)\"\n",
    "        + (f\", VRAM peak={vram_peak:.1f}MB\" if vram_peak is not None else \"\")\n",
    "    )\n",
    "    del model, likelihood\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f87af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10186869353055954\n",
      "0.000967189484150289\n",
      "10.747753953933715\n",
      "0.12633148455706028\n"
     ]
    }
   ],
   "source": [
    "print(statistics.mean(mse_l_rff[1:]))\n",
    "print(statistics.stdev(mse_l_rff[1:]))\n",
    "\n",
    "print(statistics.mean(time_l_rff[1:]))\n",
    "print(statistics.stdev(time_l_rff[1:]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scaleGP)",
   "language": "python",
   "name": "scalegp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
