{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7511799-a297-4192-8eab-f89cdc78dd2c",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "To run this experiment, first run the Setup chunk, then run the RFF section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e238afb",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6605e5-f791-44f8-ada3-9efb2237ed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU availability:  True\n",
      "28549.08984375\n",
      "all_x shape: torch.Size([1000000, 1])\n",
      "all_y shape: torch.Size([1000000])\n",
      "torch.Size([80000, 1]) torch.Size([80000])\n",
      "torch.Size([20000, 1]) torch.Size([20000])\n"
     ]
    }
   ],
   "source": [
    "gpu = True\n",
    "n_replicates = 2\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import gc\n",
    "import statistics\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import psutil\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "import gpytorch\n",
    "import pynvml\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import (\n",
    "    NNVariationalStrategy,\n",
    "    CholeskyVariationalDistribution,\n",
    "    VariationalStrategy,\n",
    ")\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.mlls import DeepApproximateMLL\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def log_memory():\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    max_allocated = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "    max_reserved = torch.cuda.max_memory_reserved() / 1024**2    # MB\n",
    "    gpu_used = meminfo.used / 1024**2                            # MB\n",
    "    sys_used = psutil.virtual_memory().used / 1024**3            # GB\n",
    "    print(f\"[PyTorch] Max Allocated: {max_allocated:.2f} MB | Max Reserved: {max_reserved:.2f} MB\")\n",
    "    print(f\"[GPU VRAM] Used (nvidia-smi): {gpu_used:.2f} MB | [System RAM]: {sys_used:.2f} GB\")\n",
    "    return max_allocated, max_reserved, gpu_used, sys_used\n",
    "\n",
    "\n",
    "max_vram = 0\n",
    "max_ram = 0\n",
    "def get_mem():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss/(1024**2)\n",
    "\n",
    "max_vram = 0\n",
    "def vram_usage():\n",
    "    global max_vram\n",
    "    if gpu:\n",
    "        max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "\n",
    "\n",
    "print(\"GPU availability: \", torch.cuda.is_available())\n",
    "print(psutil.virtual_memory().used / (1024 ** 2))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "x = pd.read_csv('Data/x_100k.csv', header=None).values.squeeze()\n",
    "y = pd.read_csv('Data/y_100k.csv', header=None).values.squeeze()\n",
    "all_x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "all_y = torch.tensor(y, dtype=torch.float32)\n",
    "all_x = all_x.contiguous()\n",
    "all_y = all_y.contiguous()\n",
    "print(\"all_x shape:\", all_x.shape)\n",
    "print(\"all_y shape:\", all_y.shape)\n",
    "\n",
    "\n",
    "def splitter(x_cpu, y_cpu, n_train=80000, n_test=20000, random_state=42, move_to_gpu=True):\n",
    "    assert x_cpu.shape[0] == y_cpu.shape[0], \"Mismatch in number of samples\"\n",
    "    total_samples = x_cpu.shape[0]\n",
    "    assert n_train + n_test <= total_samples, \"Not enough samples to split\"\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "    indices = rng.permutation(total_samples)\n",
    "    train_idx = indices[:n_train]\n",
    "    test_idx  = indices[n_train:n_train + n_test]\n",
    "    train_x = x_cpu[train_idx].contiguous()\n",
    "    train_y = y_cpu[train_idx].contiguous()\n",
    "    test_x  = x_cpu[test_idx].contiguous()\n",
    "    test_y  = y_cpu[test_idx].contiguous()\n",
    "    if move_to_gpu and torch.cuda.is_available():\n",
    "        train_x = train_x.cuda()\n",
    "        train_y = train_y.cuda()\n",
    "        test_x = test_x.cuda()\n",
    "        test_y = test_y.cuda()\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "train_x, train_y, test_x, test_y = splitter(all_x, all_y, n_train=80000, n_test=20000)\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955df80-1c9d-4055-8e5e-3ea753501029",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# RFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b5818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Replicate 1/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 1: MSE=0.3005, Time=10.83s, RAM before=0.0MB, peak=790.7MB (Δ=790.7MB), VRAM peak=12689.3MB\n",
      "\n",
      "=== Replicate 2/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 2: MSE=0.2941, Time=10.62s, RAM before=0.0MB, peak=935.7MB (Δ=935.7MB), VRAM peak=12814.4MB\n",
      "\n",
      "=== Replicate 3/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 3: MSE=0.3066, Time=10.59s, RAM before=0.0MB, peak=936.3MB (Δ=936.3MB), VRAM peak=12814.4MB\n",
      "\n",
      "=== Replicate 4/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 4: MSE=0.3028, Time=10.63s, RAM before=0.0MB, peak=936.3MB (Δ=936.3MB), VRAM peak=12814.4MB\n",
      "\n",
      "=== Replicate 5/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 5: MSE=0.2973, Time=10.66s, RAM before=0.0MB, peak=936.3MB (Δ=936.3MB), VRAM peak=12814.4MB\n",
      "\n",
      "=== Replicate 6/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 6: MSE=0.3048, Time=10.66s, RAM before=0.0MB, peak=936.3MB (Δ=936.3MB), VRAM peak=12814.4MB\n",
      "\n",
      "=== Replicate 7/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 7: MSE=0.3002, Time=10.71s, RAM before=0.0MB, peak=936.2MB (Δ=936.2MB), VRAM peak=12814.4MB\n",
      "\n",
      "=== Replicate 8/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 8: MSE=0.4594, Time=10.49s, RAM before=0.0MB, peak=936.2MB (Δ=936.2MB), VRAM peak=12814.4MB\n",
      "\n",
      "=== Replicate 9/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 9: MSE=0.2972, Time=10.64s, RAM before=0.0MB, peak=936.2MB (Δ=936.2MB), VRAM peak=12814.4MB\n",
      "\n",
      "=== Replicate 10/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 10: MSE=0.9770, Time=10.53s, RAM before=0.0MB, peak=936.2MB (Δ=936.2MB), VRAM peak=12814.4MB\n"
     ]
    }
   ],
   "source": [
    "import tqdm, time, gc\n",
    "import torch, gpytorch\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RFFKernel(\n",
    "                num_samples=200, # Adjust this line to choose the number of Fourier Features\n",
    "                num_dims=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gpytorch.distributions.MultivariateNormal(\n",
    "            self.mean_module(x),\n",
    "            self.covar_module(x)\n",
    "        )\n",
    "\n",
    "n_replicates = 10\n",
    "training_iterations = 64\n",
    "n_train, n_test = 40_000, 10_000\n",
    "random_state = 42\n",
    "mse_l_rff, time_l_rff = [], []\n",
    "\n",
    "for rep in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {rep + 1}/{n_replicates} ===\")\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_x, all_y,\n",
    "        n_train=n_train, n_test=n_test,\n",
    "        random_state=random_state + rep,\n",
    "        move_to_gpu=torch.cuda.is_available()\n",
    "    )\n",
    "    ram_before = get_mem() / (1024**2)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train(); likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.25)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    if torch.cuda.is_available():\n",
    "        mll = mll.cuda()\n",
    "\n",
    "    def train_fn():\n",
    "        progress = trange(training_iterations, desc=f\"Training (rep {rep+1})\", leave=False)\n",
    "        for _ in progress:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            progress.set_postfix(loss=loss.item())\n",
    "        return None\n",
    "\n",
    "    start_time = time.time()\n",
    "    peak_ram = memory_usage(\n",
    "        (train_fn,),\n",
    "        max_usage=True,\n",
    "        retval=False,\n",
    "        interval=0.01\n",
    "    )\n",
    "    elapsed = time.time() - start_time\n",
    "    vram_peak = torch.cuda.max_memory_allocated() / (1024**2) if torch.cuda.is_available() else None\n",
    "    ram_delta = peak_ram - ram_before\n",
    "    model.eval(); likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        pred = likelihood(model(test_x)).mean.cpu()\n",
    "    mse = torch.mean((pred - test_y.cpu()) ** 2).item()\n",
    "    mse_l_rff.append(mse)\n",
    "    time_l_rff.append(elapsed)\n",
    "    print(\n",
    "        f\"Rep {rep+1}: MSE={mse:.4f}, Time={elapsed:.2f}s, \"\n",
    "        f\"RAM before={ram_before:.1f}MB, peak={peak_ram:.1f}MB (Δ={ram_delta:.1f}MB)\"\n",
    "        + (f\", VRAM peak={vram_peak:.1f}MB\" if vram_peak is not None else \"\")\n",
    "    )\n",
    "    del model, likelihood\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa31965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.393264373143514\n",
      "0.22516770588452842\n",
      "10.61574641863505\n",
      "0.06786572604004579\n"
     ]
    }
   ],
   "source": [
    "print(statistics.mean(mse_l_rff[1:]))\n",
    "print(statistics.stdev(mse_l_rff[1:]))\n",
    "\n",
    "print(statistics.mean(time_l_rff[1:]))\n",
    "print(statistics.stdev(time_l_rff[1:]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scaleGP)",
   "language": "python",
   "name": "scalegp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
