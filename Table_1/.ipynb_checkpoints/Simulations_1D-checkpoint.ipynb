{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7511799-a297-4192-8eab-f89cdc78dd2c",
   "metadata": {},
   "source": [
    "# Important Note\n",
    "RAM and VRAM measurements are dependent on the computer state, and should only be interpreted relative to each other. In order to obtain RAM and VRAM measurements, perform the following steps:\n",
    "\n",
    "1 - Restart the Kernel\n",
    "\n",
    "2 - Run the \"Loading Required Packages and Helper Functions\" cell\n",
    "\n",
    "3 - Run the \"Loading Data\" cell\n",
    "\n",
    "4 - Run ONLY ONE iteration of the desired method, and read the RAM and VRAM usage reports printed by the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531dca75-5931-4d1c-a9c7-f4b266b6ec42",
   "metadata": {},
   "source": [
    "# Loading Required Packages and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e07126f-2559-4249-9221-117c4c53ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6605e5-f791-44f8-ada3-9efb2237ed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU availability:  True\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 59\u001b[0m\n\u001b[0;32m     54\u001b[0m         max_vram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_vram, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated())\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU availability: \u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "import statistics\n",
    "import numpy as np\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import psutil\n",
    "def get_mem():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss\n",
    "\n",
    "import tqdm\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.mlls import DeepApproximateMLL\n",
    "\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.mlls import DeepApproximateMLL\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "\n",
    "max_vram = 0\n",
    "max_ram = 0\n",
    "\n",
    "\n",
    "max_vram = 0\n",
    "def vram_usage():\n",
    "    global max_vram\n",
    "    if gpu:\n",
    "        max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "\n",
    "\n",
    "print(\"GPU availability: \", torch.cuda.is_available())\n",
    "\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7e28d-807c-40e4-8868-4146cd1c5911",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d29fd88-72ae-43bf-9292-e9f77f51d552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80000, 1])\n",
      "torch.Size([80000])\n",
      "torch.Size([20000])\n",
      "torch.Size([20000])\n"
     ]
    }
   ],
   "source": [
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "csvfile = pd.read_csv('train_x_100k.csv', header = None)\n",
    "train_x = torch.tensor(csvfile[0]).float()\n",
    "train_x = torch.reshape(train_x,[80000,1])\n",
    "csvfile = pd.read_csv('train_y_100k.csv', header = None)\n",
    "train_y = torch.tensor(csvfile[0]).float()\n",
    "csvfile = pd.read_csv('test_x_100k.csv', header = None)\n",
    "test_x = torch.tensor(csvfile[0]).float()\n",
    "csvfile = pd.read_csv('test_y_100k.csv', header = None)\n",
    "test_y = torch.tensor(csvfile[0]).float()\n",
    "\n",
    "\n",
    "# csvfile = pd.read_csv('train_x_Dense.csv', header = None)\n",
    "# train_x = torch.tensor(csvfile[0]).float()\n",
    "# train_x = torch.reshape(train_x,[80000,1])\n",
    "# csvfile = pd.read_csv('train_y_Dense.csv', header = None)\n",
    "# train_y = torch.tensor(csvfile[0]).float()\n",
    "# csvfile = pd.read_csv('test_x_Dense.csv', header = None)\n",
    "# test_x = torch.tensor(csvfile[0]).float()\n",
    "# csvfile = pd.read_csv('test_y_Dense.csv', header = None)\n",
    "# test_y = torch.tensor(csvfile[0]).float()\n",
    "\n",
    "\n",
    "train_n = int(len(train_x))\n",
    "train_x = train_x.contiguous()\n",
    "train_y = train_y.contiguous()\n",
    "\n",
    "test_x = test_x.contiguous()\n",
    "test_y = test_y.contiguous()\n",
    "\n",
    "if gpu:\n",
    "    train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "\n",
    "\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6ca619-2934-42e4-8d62-9c5ae1a6c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955df80-1c9d-4055-8e5e-3ea753501029",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# SKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47ccb967-8318-4395-8578-2b7a3d2105c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/32 [00:00<?, ?it/s]C:\\Users\\kevwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\utils\\sparse.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if nonzero_indices.storage():\n",
      "C:\\Users\\kevwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\utils\\sparse.py:66: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:85.)\n",
      "  res = cls(index_tensor, value_tensor, interp_size)\n",
      "C:\\Users\\kevwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\utils\\sparse.py:66: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:607.)\n",
      "  res = cls(index_tensor, value_tensor, interp_size)\n",
      "Train: 100%|██████████| 32/32 [00:06<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKI: MSE &Time &RAM &VRAM: 0.30744537711143494 & 6.76 & 648.36 & 35.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:06<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKI: MSE &Time &RAM &VRAM: 0.307567834854126 & 6.22 & -0.74 & 54.56\n"
     ]
    }
   ],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        grid_size = gpytorch.utils.grid.choose_grid_size(train_x,1.0/25.0)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=1), grid_size=grid_size, num_dims=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "smoke_test = False\n",
    "\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iterations = 32\n",
    "\n",
    "mem_begin = get_mem()\n",
    "\n",
    "\n",
    "mse_l_ski = []\n",
    "time_l_ski = []\n",
    "\n",
    "#for i in np.arange(0,10):\n",
    "for i in np.arange(0,2):\n",
    "    mem_begin = get_mem()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)#.cuda()\n",
    "    if gpu:\n",
    "        mll = mll.cuda()\n",
    "\n",
    "    \n",
    "    begin = time.time()\n",
    "\n",
    "    for i in tqdm.tqdm(range(training_iterations), desc=\"Train\"):\n",
    "        optimizer.zero_grad()\n",
    "        max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        optimizer.step()\n",
    "\n",
    "    uTime = time.time()-begin\n",
    "    uRAM = (get_mem() - mem_begin)/(1024**2)\n",
    "    uVRAM = max_vram / (1024 ** 2)\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "\n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    mse_l_ski.append(MSE.item())\n",
    "    time_l_ski.append(uTime)\n",
    "    print('SKI: MSE &Time &RAM &VRAM: {}'.format(MSE), \"&\", round(uTime,2), \"&\", round(uRAM,2) , \"&\" , round(uVRAM,2) )\n",
    "    likelihood = None\n",
    "    model = None\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa31965",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(mse_l_ski))\n",
    "print(statistics.stdev(mse_l_ski))\n",
    "\n",
    "print(statistics.mean(time_l_ski))\n",
    "print(statistics.stdev(time_l_ski))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce0dd39-3dc7-4cec-96a5-d068a05c3752",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfad5eb3-9eb7-44ba-a7bd-c680b3716264",
   "metadata": {},
   "source": [
    "# Sparse GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e5399d-22be-42e9-ad0d-4db2b746f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "max_vram = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d229e74-5c6b-401b-8a77-a2361bf99247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/250 [00:00<?, ?it/s]C:\\Users\\kevwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "C:\\Users\\kevwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "Train: 100%|██████████| 250/250 [00:19<00:00, 13.00it/s, loss=0.839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKI: MSE &Time &RAM &VRAM: 0.31073513627052307 & 19.23 & 10.91 & 135.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  18%|█▊        | 44/250 [00:02<00:11, 17.94it/s, loss=1.31]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m#%time train()\u001b[39;00m\n\u001b[0;32m     66\u001b[0m begin \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 67\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m uTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mbegin\n\u001b[0;32m     69\u001b[0m uRAM \u001b[38;5;241m=\u001b[39m (get_mem() \u001b[38;5;241m-\u001b[39m mem_begin)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 58\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, train_y)\n\u001b[0;32m     57\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 58\u001b[0m iterator\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     60\u001b[0m vram_usage()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.base_covar_module = ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2))\n",
    "        self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=train_x[::300].clone(), likelihood=likelihood)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "\n",
    "mse_l_sgpr = []\n",
    "time_l_sgpr = []\n",
    "#for i in np.arange(0,10):\n",
    "for i in np.arange(0,2):\n",
    "    mem_begin = get_mem()\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "\n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "\n",
    "    training_iterations = 250\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    def train():\n",
    "        iterator = tqdm.tqdm(range(training_iterations), desc=\"Train\")\n",
    "\n",
    "        for i in iterator:\n",
    "            # Zero backprop gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Get output from model\n",
    "            output = model(train_x)\n",
    "            # Calc loss and backprop derivatives\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            iterator.set_postfix(loss=loss.item())\n",
    "            optimizer.step()\n",
    "            vram_usage()\n",
    "            if gpu:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    #%time train()\n",
    "\n",
    "    begin = time.time()\n",
    "    train()\n",
    "    uTime = time.time()-begin\n",
    "    uRAM = (get_mem() - mem_begin)/(1024**2)\n",
    "    uVRAM = max_vram / (1024 ** 2)\n",
    "    # print(time.time()-begin)\n",
    "    # print(\"RAM: \",(get_mem() - mem_begin)/(1024**2))\n",
    "    # print(\"VRAM: \", max_vram / (1024 ** 2))\n",
    "\n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    #    test_x = torch.linspace(0, 1, 51)\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "\n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_sgpr.append(MSE.item())\n",
    "    time_l_sgpr.append(uTime)\n",
    "    print('SKI: MSE &Time &RAM &VRAM: {}'.format(MSE), \"&\", round(uTime,2), \"&\", round(uRAM,2) , \"&\" , round(uVRAM,2) )\n",
    "    likelihood = None\n",
    "    model = None\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d17fa-8cbd-4bc5-92cd-a6e51066d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(statistics.mean(mse_l_sgpr))\n",
    "print(statistics.stdev(mse_l_sgpr))\n",
    "\n",
    "print(statistics.mean(time_l_sgpr))\n",
    "print(statistics.stdev(time_l_sgpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b424e738-8f9e-4027-b302-a7a6eb2a4784",
   "metadata": {},
   "source": [
    "# LOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520eddc5-3859-4dd7-b92d-67304487353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lanczos Variance Estimates (LOVE)\n",
    "max_vram = 0\n",
    "gc.collect()\n",
    "if gpu:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ed2886-f6e2-42b7-b5c8-72a61f4bc513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM Usage: 1648.70751953125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:32<00:00,  3.12it/s, loss=0.818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKI: MSE &Time &RAM &VRAM: 0.29914700984954834 & 32.09 & 32.24 & 2147.16\n",
      "VRAM Usage: 148.7763671875 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:01<00:39,  2.43it/s, loss=1.02]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m#%time train()\u001b[39;00m\n\u001b[0;32m     96\u001b[0m begin \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 97\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m uTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mbegin\n\u001b[0;32m     99\u001b[0m uRAM \u001b[38;5;241m=\u001b[39m (get_mem() \u001b[38;5;241m-\u001b[39m mem_begin)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 90\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     88\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, train_y)\n\u001b[0;32m     89\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 90\u001b[0m iterator\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     91\u001b[0m vram_usage()\n\u001b[0;32m     92\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "\n",
    "# my_batch_size = 3200\n",
    "# smoke_test = False\n",
    "\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "# train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "# test_dataset = TensorDataset(test_x, test_y)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(input_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 1))\n",
    "        print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        \n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=1)),\n",
    "            grid_size=100, num_dims=1,\n",
    "        )\n",
    "\n",
    "        # Also add the deep net\n",
    "        self.feature_extractor = LargeFeatureExtractor(input_dim=train_x.size(-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We're first putting our data through a deep net (feature extractor)\n",
    "        # We're also scaling the features so that they're nice values\n",
    "        projected_x = self.feature_extractor(x)\n",
    "        projected_x = projected_x - projected_x.min(0)[0]\n",
    "        projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "\n",
    "        # The rest of this looks like what we've seen\n",
    "        mean_x = self.mean_module(projected_x)\n",
    "        covar_x = self.covar_module(projected_x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "mse_l_love = []\n",
    "time_l_love = []\n",
    "\n",
    "\n",
    "#for i in np.arange(0,10):\n",
    "for i in np.arange(0,2):\n",
    "    mem_begin = get_mem()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    training_iterations = 100\n",
    "    \n",
    "    \n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "    \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    \n",
    "    def train():\n",
    "        iterator = tqdm.tqdm(range(training_iterations))\n",
    "        for i in iterator:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            iterator.set_postfix(loss=loss.item())\n",
    "            vram_usage()\n",
    "            optimizer.step()\n",
    "    \n",
    "    #%time train()\n",
    "    \n",
    "    begin = time.time()\n",
    "    train()\n",
    "    uTime = time.time()-begin\n",
    "    uRAM = (get_mem() - mem_begin)/(1024**2)\n",
    "    uVRAM = max_vram / (1024 ** 2)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    mse_l_love.append(MSE.item())\n",
    "    time_l_love.append(uTime)\n",
    "    print('SKI: MSE &Time &RAM &VRAM: {}'.format(MSE), \"&\", round(uTime,2), \"&\", round(uRAM,2) , \"&\" , round(uVRAM,2) )\n",
    "    likelihood = None\n",
    "    model = None\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e43d83-1b30-4d8b-8058-cf4c8d840b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(mse_l_love))\n",
    "print(statistics.stdev(mse_l_love))\n",
    "\n",
    "print(statistics.mean(time_l_love))\n",
    "print(statistics.stdev(time_l_love))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90f64c-2426-45ef-be6f-94cd98b9f010",
   "metadata": {},
   "source": [
    "# DKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae0c28c-fdeb-42ac-9255-752e4c809f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Kernel Learning\n",
    "max_vram = 0\n",
    "gc.collect()\n",
    "if gpu:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7cc8db-cfc4-4ef2-8e2f-fb03fabf92bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 1))\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()\n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.ScaleKernel(  gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=1)  ),\n",
    "                num_dims=1, grid_size=100\n",
    "            )\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "            # This module will scale the NN features so that they're nice values\n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\"\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            vram_usage()\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "mse_l_dkl = []\n",
    "time_l_dkl = []\n",
    "\n",
    "for i in np.arange(0,2):\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    training_iterations = 80\n",
    "    \n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.feature_extractor.parameters()},\n",
    "        {'params': model.covar_module.parameters()},\n",
    "        {'params': model.mean_module.parameters()},\n",
    "        {'params': model.likelihood.parameters()},\n",
    "    ], lr=0.02)# 0.02 for 100k, 0.02 of Dense\n",
    "    \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    def train():\n",
    "        iterator = tqdm.tqdm(range(training_iterations), leave = True)\n",
    "        for i in iterator:\n",
    "            # Zero backprop gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Get output from model\n",
    "            output = model(train_x)\n",
    "            # Calc loss and backprop derivatives\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            iterator.set_postfix(loss=loss.item())\n",
    "            vram_usage()\n",
    "            optimizer.step()\n",
    "    \n",
    "    \n",
    "    begin=time.time()\n",
    "    train()\n",
    "    uTime = time.time()-begin\n",
    "    uRAM = (get_mem() - mem_begin)/(1024**2)\n",
    "    uVRAM = max_vram / (1024 ** 2)\n",
    "    \n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    #test_loader = DataLoader(test_dataset, batch_size=320, shuffle=False)\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_dkl.append(MSE.item())\n",
    "    time_l_dkl.append(uTime)\n",
    "    print('SKI: MSE &Time &RAM &VRAM: {}'.format(MSE), \"&\", round(uTime,2), \"&\", round(uRAM,2) , \"&\" , round(uVRAM,2) )\n",
    "    likelihood = None\n",
    "    model = None\n",
    "    gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51aa14-6510-4d76-9fcd-4a025320087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(mse_l_dkl))\n",
    "print(statistics.stdev(mse_l_dkl))\n",
    "\n",
    "print(statistics.mean(time_l_dkl))\n",
    "print(statistics.stdev(time_l_dkl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eab984-9b64-4e7f-9532-039bbb00be33",
   "metadata": {},
   "source": [
    "# SVGP CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b464d4-2ff1-4d2f-a573-ae6822d71849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVGP_CI\n",
    "max_vram = 0\n",
    "gc.collect()\n",
    "if gpu:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0611cb-3119-4842-a668-3c2acd3fcf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch_size = 3200\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "inducing_points = train_x[torch.randperm(train_x.size(0))[:200]]\n",
    "inducing_points = train_x[::1000]\n",
    "\n",
    "\n",
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.CiqVariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2)\n",
    "        )\n",
    "        self.covar_module.base_kernel.initialize(lengthscale=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "mse_l_svgpci = []\n",
    "time_l_svgpci = []\n",
    "\n",
    "for i in np.arange(0,2):\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=train_y.size(0), lr=0.1)\n",
    "    \n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.002) #0.01 for 100k, 0.002 for Dense\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    \n",
    "    num_epochs = 15\n",
    "    \n",
    "    begin = time.time()\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "    for i in epochs_iter:\n",
    "        minibatch_iter = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False, position = 0)\n",
    "    \n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "            vram_usage()\n",
    "            hyperparameter_optimizer.step()\n",
    "    \n",
    "    uTime = time.time()-begin\n",
    "    uRAM = (get_mem() - mem_begin)/(1024**2)\n",
    "    uVRAM = max_vram / (1024 ** 2)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=320, shuffle=False)\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    mse_l_svgpci.append(MSE.item())\n",
    "    time_l_svgpci.append(uTime)\n",
    "    print('SKI: MSE &Time &RAM &VRAM: {}'.format(MSE), \"&\", round(uTime,2), \"&\", round(uRAM,2) , \"&\" , round(uVRAM,2) )\n",
    "    likelihood = None\n",
    "    model = None\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1f02c-431f-48d7-9da1-7d52ff797bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(mse_l_svgpci))\n",
    "print(statistics.stdev(mse_l_svgpci))\n",
    "\n",
    "print(statistics.mean(time_l_svgpci))\n",
    "print(statistics.stdev(time_l_svgpci))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28302cc4-1b8d-4544-a4a6-8146d8d0dba6",
   "metadata": {},
   "source": [
    "# SVGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e35f6-e4f9-4e8b-bb46-5cd07f7f9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVGP\n",
    "max_vram = 0\n",
    "gc.collect()\n",
    "if gpu:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834b3b8-9790-4eb1-851e-f0ff48390b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch_size = 32\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=False)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "mse_l_svgp = []\n",
    "time_l_svgp = []\n",
    "\n",
    "for i in np.arange(0,2):\n",
    "    mem_begin = get_mem()\n",
    "    inducing_points = train_x[::1000]\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"RAM: \", mem_diff / (1024 ** 2))\n",
    "    \n",
    "    \n",
    "    # inducing_points = train_x[::1000]\n",
    "    # model = GPModel(inducing_points=inducing_points)\n",
    "    # likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    # if torch.cuda.is_available():\n",
    "    #     model = model.cuda()\n",
    "    #     likelihood = likelihood.cuda()\n",
    "    # mem_diff = get_mem() - mem_begin\n",
    "    # print(\"RAM: \", mem_diff / (1024 ** 2))\n",
    "    \n",
    "    \n",
    "    num_epochs = 6\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.005)\n",
    "        \n",
    "\n",
    "    #mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"RAM: \", mem_diff / (1024 ** 2))\n",
    "    \n",
    "    \n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "    \n",
    "    begin = time.time()\n",
    "    for i in tqdm.tqdm(range(num_epochs), leave = False, position = 0):\n",
    "        minibatch_iter = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False, position = 0)\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            max_ram = max(max_ram, (get_mem() - mem_begin))\n",
    "            optimizer.step()\n",
    "            max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "            i + 1, num_epochs, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item(),\n",
    "            likelihood.noise.item()\n",
    "        ))\n",
    "    uTime = time.time()-begin\n",
    "    uRAM = (get_mem() - mem_begin)/(1024**2)\n",
    "    uVRAM = max_vram / (1024 ** 2)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=320, shuffle=False)\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "\n",
    "    mse_l_svgp.append(MSE.item())\n",
    "    time_l_svgp.append(uTime)\n",
    "    print('SKI: MSE &Time &RAM &VRAM: {}'.format(MSE), \"&\", round(uTime,2), \"&\", round(uRAM,2) , \"&\" , round(uVRAM,2) )\n",
    "    likelihood = None\n",
    "    model = None\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57924823-1f7c-46cf-8e3f-73daf54ccc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(mse_l_svgp))\n",
    "print(statistics.stdev(mse_l_svgp))\n",
    "\n",
    "print(statistics.mean(time_l_svgp))\n",
    "print(statistics.stdev(time_l_svgp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174305f1-9eca-4bfb-8b45-55dcb521db47",
   "metadata": {},
   "source": [
    "# NGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec3b4b-9332-4954-8e4e-6c688b0fcac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGD\n",
    "max_vram = 0\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41490c0b-1b29-4b3f-882d-0835174f149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch_size = 3200\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "mse_l_ngd = []\n",
    "time_l_ngd = []\n",
    "\n",
    "for i in np.arange(0,2):\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    inducing_points = train_x[::100]\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=train_y.size(0), lr=0.001)#0.001 for 100k, 0.001 for Dense\n",
    "    \n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.1) #0.1 for 100k, 0.1 for Dense\n",
    "    #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    #mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    num_epochs = 60 #100k = 20, Dense = 60\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\", leave = False, position = 0)\n",
    "    \n",
    "    begin = time.time()\n",
    "    \n",
    "    for i in epochs_iter:\n",
    "        minibatch_iter = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False, position = 0)\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "            hyperparameter_optimizer.step()\n",
    "    \n",
    "    uTime = time.time()-begin\n",
    "    uRAM = (get_mem() - mem_begin)/(1024**2)\n",
    "    uVRAM = max_vram / (1024 ** 2)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=320, shuffle=False)\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    mse_l_ngd.append(MSE.item())\n",
    "    time_l_ngd.append(uTime)\n",
    "    print('SKI: MSE &Time &RAM &VRAM: {}'.format(MSE), \"&\", round(uTime,2), \"&\", round(uRAM,2) , \"&\" , round(uVRAM,2) )\n",
    "    likelihood = None\n",
    "    model = None\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44415b-9236-403d-ab39-c8e3fbfa28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(mse_l_ngd))\n",
    "print(statistics.stdev(mse_l_ngd))\n",
    "\n",
    "print(statistics.mean(time_l_ngd))\n",
    "print(statistics.stdev(time_l_ngd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b8e294-2c10-447c-b1dd-41f6a4ea12f9",
   "metadata": {},
   "source": [
    "# VNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87cd68-1d06-41cf-9e3b-4eb7a992502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VNN\n",
    "max_vram = 0\n",
    "gc.collect()\n",
    "if gpu:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856a26a-5d71-4a08-9f5c-91ed27786be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch_size = 32\n",
    "smoke_test = False\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points, likelihood, k=256, training_batch_size=256):\n",
    "\n",
    "        m, d = inducing_points.shape\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "        print(1)\n",
    "\n",
    "        variational_distribution = gpytorch.variational.MeanFieldVariationalDistribution(m)\n",
    "\n",
    "        if gpu:\n",
    "            inducing_points = inducing_points.cuda()\n",
    "        print(2)\n",
    "\n",
    "        variational_strategy = NNVariationalStrategy(self, inducing_points, variational_distribution, k=k,\n",
    "                                                     training_batch_size=training_batch_size)\n",
    "        print(21)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        print(22)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        print(23)\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=d))\n",
    "        print(3)\n",
    "        \n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def __call__(self, x, prior=False, **kwargs):\n",
    "        if x is not None:\n",
    "            if x.dim() == 1:\n",
    "                x = x.unsqueeze(-1)\n",
    "        return self.variational_strategy(x=x, prior=False, **kwargs)\n",
    "\n",
    "begin = time.time()\n",
    "if smoke_test:\n",
    "    k = 32\n",
    "    training_batch_size = 32\n",
    "else:\n",
    "    k = 256\n",
    "    training_batch_size = 64\n",
    "\n",
    "k = 320\n",
    "training_batch_size = 320*4\n",
    "\n",
    "\n",
    "mse_l_vnn = []\n",
    "time_l_vnn = []\n",
    "\n",
    "for i in np.arange(0,2):\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    # Note: one should use full training set as inducing points!\n",
    "    model = GPModel(inducing_points=train_x[::1].contiguous(), likelihood=likelihood, k=k, training_batch_size=training_batch_size)\n",
    "    \n",
    "    if gpu:\n",
    "        likelihood = likelihood.cuda()\n",
    "        model = model.cuda()\n",
    "    \n",
    "    print(time.time()-begin)\n",
    "    \n",
    "    num_epochs = 30\n",
    "    num_batches = model.variational_strategy._total_training_batches\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "    \n",
    "    # optimizer = torch.optim.Adam([\n",
    "    #     {'params': model.parameters()},\n",
    "    #     {'params': likelihood.parameters()},\n",
    "    # ], lr=0.05)\n",
    "    \n",
    "    #mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    begin = time.time()\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\", leave=True, position = 0)\n",
    "    for epoch in epochs_iter:\n",
    "        minibatch_iter = tqdm.tqdm(range(num_batches), leave=True, position = 0)\n",
    "    \n",
    "        for i in minibatch_iter:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x=None)\n",
    "            current_training_indices = model.variational_strategy.current_training_indices\n",
    "            y_batch = train_y[...,current_training_indices]\n",
    "            if gpu:\n",
    "                y_batch = y_batch.cuda()\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "            optimizer.step()\n",
    "    \n",
    "    uTime = time.time()-begin\n",
    "    uRAM = (get_mem() - mem_begin)/(1024**2)\n",
    "    uVRAM = max_vram / (1024 ** 2)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    mse_l_vnn.append(MSE.item())\n",
    "    time_l_vnn.append(uTime)\n",
    "    print('SKI: MSE &Time &RAM &VRAM: {}'.format(MSE), \"&\", round(uTime,2), \"&\", round(uRAM,2) , \"&\" , round(uVRAM,2) )\n",
    "    likelihood = None\n",
    "    model = None\n",
    "    gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887db61e-ec92-4906-bfe3-2352d74479d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(mse_l_vnn))\n",
    "print(statistics.stdev(mse_l_vnn))\n",
    "\n",
    "print(statistics.mean(time_l_vnn))\n",
    "print(statistics.stdev(time_l_vnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2777a-12b2-4376-8511-e9ff2af69a6e",
   "metadata": {},
   "source": [
    "# Compile Table (MSE and Time only)\n",
    "\n",
    "SKI\n",
    "SGPR\n",
    "LOVE\n",
    "DKL\n",
    "SVGP-CI\n",
    "SVGP\n",
    "NGD\n",
    "VNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddbd90b-6fdf-48d9-a45a-ff205754ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SKI     --- MSE:\",statistics.mean(mse_l_ski), \"(\",statistics.stdev(mse_l_ski),\")  Time:\", statistics.mean(time_l_ski), \"(\",statistics.stdev(time_l_ski),\")\")\n",
    "print(\"SGPR    --- MSE:\",statistics.mean(mse_l_sgpr), \"(\",statistics.stdev(mse_l_sgpr),\")  Time:\", statistics.mean(time_l_sgpr), \"(\",statistics.stdev(time_l_sgpr),\")\")\n",
    "print(\"LOVE    --- MSE:\",statistics.mean(mse_l_love), \"(\",statistics.stdev(mse_l_love),\")  Time:\", statistics.mean(time_l_love), \"(\",statistics.stdev(time_l_love),\")\")\n",
    "print(\"DKL     --- MSE:\",statistics.mean(mse_l_dkl), \"(\",statistics.stdev(mse_l_dkl),\")  Time:\", statistics.mean(time_l_dkl), \"(\",statistics.stdev(time_l_dkl),\")\")\n",
    "print(\"SVGP-CI --- MSE:\",statistics.mean(mse_l_svgpci), \"(\",statistics.stdev(mse_l_svgpci),\")  Time:\", statistics.mean(time_l_svgpci), \"(\",statistics.stdev(time_l_svgpci),\")\")\n",
    "print(\"SVGP    --- MSE:\",statistics.mean(mse_l_svgp), \"(\",statistics.stdev(mse_l_svgp),\")  Time:\", statistics.mean(time_l_svgp), \"(\",statistics.stdev(time_l_svgp),\")\")\n",
    "print(\"NGD     --- MSE:\",statistics.mean(mse_l_ngd), \"(\",statistics.stdev(mse_l_ngd),\")  Time:\", statistics.mean(time_l_ngd), \"(\",statistics.stdev(time_l_ngd),\")\")\n",
    "print(\"VNN     --- MSE:\",statistics.mean(mse_l_vnn), \"(\",statistics.stdev(mse_l_vnn),\")  Time:\", statistics.mean(time_l_vnn), \"(\",statistics.stdev(time_l_vnn),\")\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ecab9-b529-4142-af05-eede13567719",
   "metadata": {},
   "source": [
    "Reordering\n",
    "SVGP\n",
    "SVGP-CI\n",
    "VNN\n",
    "NGD\n",
    "DKL\n",
    "SGPR\n",
    "SKI\n",
    "LOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713418b4-3d7c-4b1a-92ac-a6d31d0ddec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVGP    --- MSE:\",statistics.mean(mse_l_svgp), \"(\",statistics.stdev(mse_l_svgp),\")  Time:\", statistics.mean(time_l_svgp), \"(\",statistics.stdev(time_l_svgp),\")\")\n",
    "print(\"SVGP-CI --- MSE:\",statistics.mean(mse_l_svgpci), \"(\",statistics.stdev(mse_l_svgpci),\")  Time:\", statistics.mean(time_l_svgpci), \"(\",statistics.stdev(time_l_svgpci),\")\")\n",
    "print(\"VNN     --- MSE:\",statistics.mean(mse_l_vnn), \"(\",statistics.stdev(mse_l_vnn),\")  Time:\", statistics.mean(time_l_vnn), \"(\",statistics.stdev(time_l_vnn),\")\")\n",
    "print(\"NGD     --- MSE:\",statistics.mean(mse_l_ngd), \"(\",statistics.stdev(mse_l_ngd),\")  Time:\", statistics.mean(time_l_ngd), \"(\",statistics.stdev(time_l_ngd),\")\")\n",
    "print(\"DKL     --- MSE:\",statistics.mean(mse_l_dkl), \"(\",statistics.stdev(mse_l_dkl),\")  Time:\", statistics.mean(time_l_dkl), \"(\",statistics.stdev(time_l_dkl),\")\")\n",
    "print(\"SGPR    --- MSE:\",statistics.mean(mse_l_sgpr), \"(\",statistics.stdev(mse_l_sgpr),\")  Time:\", statistics.mean(time_l_sgpr), \"(\",statistics.stdev(time_l_sgpr),\")\")\n",
    "print(\"SKI     --- MSE:\",statistics.mean(mse_l_ski), \"(\",statistics.stdev(mse_l_ski),\")  Time:\", statistics.mean(time_l_ski), \"(\",statistics.stdev(time_l_ski),\")\")\n",
    "print(\"LOVE    --- MSE:\",statistics.mean(mse_l_love), \"(\",statistics.stdev(mse_l_love),\")  Time:\", statistics.mean(time_l_love), \"(\",statistics.stdev(time_l_love),\")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
