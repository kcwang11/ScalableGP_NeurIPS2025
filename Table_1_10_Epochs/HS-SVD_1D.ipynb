{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416b0d9a",
   "metadata": {},
   "source": [
    "# Important Note\n",
    "RAM and VRAM measurements are dependent on the computer state, and should only be interpreted relative to each other. In order to obtain RAM and VRAM measurements, perform the following steps as normal but only use the first ram and vram measurements. Additionally, the Datagenerator.rmd file must be run to generate the data first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6bfc9",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "This notebook provides code to implement and benchmark the HSSVD method using the compact Matern kernel in 1D. To run this benchmark, perform the following steps:\n",
    "\n",
    "Step 1: Run the following cell to import the required packages and helper functions. Set the number of replicates desired.\n",
    "\n",
    "Step 2: Load the Data and implement the method\n",
    "\n",
    "Step 3: Execute the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d5218",
   "metadata": {},
   "source": [
    "# Step 1: Loading Required Packages and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a4ade5-38b2-4f05-8626-7fb74d9fe5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "1.26.4\n",
      "3.9.2\n",
      "5.9.0\n",
      "1.13\n"
     ]
    }
   ],
   "source": [
    "gpu = True\n",
    "n_replicates = 2\n",
    "\n",
    "import torch, numpy, matplotlib, psutil, gpytorch, time, pynvml, statistics, gc, numpy as np, tqdm, os, urllib.request, pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from math import floor, ceil\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(torch.__version__)\n",
    "print(numpy.__version__)\n",
    "print(matplotlib.__version__)\n",
    "print(psutil.__version__)\n",
    "print(gpytorch.__version__)\n",
    "\n",
    "def log_memory():\n",
    "    pynvml.nvmlInit()\n",
    "    h = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    m = pynvml.nvmlDeviceGetMemoryInfo(h)\n",
    "    g = m.used / 1024**2\n",
    "    t = torch.cuda.max_memory_allocated() / 1024**2\n",
    "    s = psutil.virtual_memory().used / 1024**3\n",
    "    print(f\"[GPU] Used: {g:.2f} MB\")\n",
    "    print(f\"[PyTorch] Max Allocated: {t:.2f} MB\")\n",
    "    print(f\"[System RAM] Used: {s:.2f} GB\")\n",
    "    return g, t, s\n",
    "\n",
    "def get_mem():\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / 1024**2\n",
    "\n",
    "max_vram = 0\n",
    "def vram_usage():\n",
    "    global max_vram\n",
    "    max_vram = max(max_vram, torch.cuda.memory_allocated())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7e28d-807c-40e4-8868-4146cd1c5911",
   "metadata": {},
   "source": [
    "# Step 2: Load the Data and implement the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "838e1174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and transferred to device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "data = pd.read_csv(\"Data/data_1D_100k_full.csv\")\n",
    "x = torch.tensor(data['x'].values, dtype=torch.float64)\n",
    "y = torch.tensor(data['y'].values, dtype=torch.float64)\n",
    "y_true = torch.tensor(data['y_true'].values, dtype=torch.float64)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "y_true = y_true.to(device)\n",
    "\n",
    "print(\"Data loaded and transferred to device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a646da89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train x shape: torch.Size([300000, 1])\n",
      "Train y shape: torch.Size([300000])\n",
      "Test x shape: torch.Size([20000, 1])\n",
      "Test y shape: torch.Size([20000])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"Data/data_1D_100k_full.csv\")\n",
    "x = torch.tensor(data['x'].values, dtype=torch.float64).unsqueeze(1)\n",
    "y = torch.tensor(data['y'].values, dtype=torch.float64)\n",
    "\n",
    "\n",
    "def splitter(x_cpu, y_cpu, n_train=80000, n_test=20000, move_to_gpu=True):\n",
    "    assert x_cpu.shape[0] == y_cpu.shape[0], \"Mismatch in number of samples\"\n",
    "    total_samples = x_cpu.shape[0]\n",
    "    assert n_train + n_test <= total_samples, \"Not enough samples to split\"\n",
    "    rng = np.random.default_rng()\n",
    "    indices = rng.permutation(total_samples)\n",
    "    train_idx = indices[:n_train]\n",
    "    test_idx  = indices[n_train:n_train + n_test]\n",
    "    train_x = x_cpu[train_idx].contiguous()\n",
    "    train_y = y_cpu[train_idx].contiguous()\n",
    "    test_x  = x_cpu[test_idx].contiguous()\n",
    "    test_y  = y_cpu[test_idx].contiguous()\n",
    "    if move_to_gpu and torch.cuda.is_available():\n",
    "        train_x = train_x.cuda()\n",
    "        train_y = train_y.cuda()\n",
    "        test_x = test_x.cuda()\n",
    "        test_y = test_y.cuda()\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "train_x, train_y, test_x, test_y = splitter(x, y, n_train=300000, n_test=20000, move_to_gpu=False)\n",
    "print(\"Train x shape:\", train_x.shape)\n",
    "print(\"Train y shape:\", train_y.shape)\n",
    "print(\"Test x shape:\", test_x.shape)\n",
    "print(\"Test y shape:\", test_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f33a6",
   "metadata": {},
   "source": [
    "## Implementing the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e87fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def phifunc(m, x):\n",
    "    device = x.device\n",
    "    dtype = x.dtype\n",
    "    freq = torch.arange(1, m + 1, dtype=dtype, device=device)\n",
    "    sin_values = torch.sin(torch.pi * freq.unsqueeze(1) * x.unsqueeze(0))\n",
    "    phi = sin_values.transpose(0, 1)\n",
    "    return phi\n",
    "\n",
    "def lambdafunc(m, alpha, beta):\n",
    "    device = alpha.device\n",
    "    dtype = alpha.dtype\n",
    "    freq = torch.arange(1, m + 1, dtype=dtype, device=device)\n",
    "    values = (torch.pi**2) * (freq**2) + alpha**2\n",
    "    lam = values**(-beta)\n",
    "    lam = lam / torch.max(lam)\n",
    "    return lam\n",
    "\n",
    "def hssvd_predict_1d(x, y, x_new, init=torch.tensor([0.0, 0.0]), m=30, beta=1, \n",
    "                     train=True, lr=0.01, max_iter=1000, tol=1e-7):\n",
    "    device = x.device\n",
    "    dtype = x.dtype\n",
    "    \n",
    "    init = init.to(device=device, dtype=dtype)\n",
    "    n = x.shape[0]\n",
    "    Phi = phifunc(m, x)\n",
    "    MM = Phi.shape[1]\n",
    "    phi_T_phi = Phi.t() @ Phi\n",
    "    phi_T_y = Phi.t() @ y\n",
    "    sum_y2 = torch.sum(y**2)\n",
    "\n",
    "    def nlik(log_params):\n",
    "        eps = torch.exp(log_params[0])\n",
    "        s2 = torch.exp(log_params[1])\n",
    "        Lambda = lambdafunc(m, eps, beta)\n",
    "        A = s2 * torch.diag(1.0 / Lambda) + phi_T_phi + 1e-10 * torch.eye(MM, device=device, dtype=dtype)\n",
    "        \n",
    "        A_cpu = A.cpu()\n",
    "        L_cpu = torch.linalg.cholesky(A_cpu)\n",
    "        phi_T_y_cpu = phi_T_y.cpu()\n",
    "        sol = torch.cholesky_solve(phi_T_y_cpu.unsqueeze(1), L_cpu).squeeze(1).to(device)\n",
    "        \n",
    "        t1 = (sum_y2 - (phi_T_y @ sol)) / s2\n",
    "        logdetA = 2 * torch.sum(torch.log(torch.diag(L_cpu).to(device)))\n",
    "        t2 = (n - m) * torch.log(s2) + logdetA\n",
    "        t3 = torch.sum(torch.log(Lambda))\n",
    "        return t1 + t2 + t3\n",
    "\n",
    "    if train:\n",
    "        log_params = init.clone().detach().requires_grad_(True)\n",
    "        optimizer = optim.Adam([log_params], lr=lr)\n",
    "        prev_loss = None\n",
    "        for iter_ in range(max_iter):\n",
    "            optimizer.zero_grad()\n",
    "            loss = nlik(log_params)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print(f\"Iter {iter_}: estimate = {log_params.data}, loss = {loss.item():.4f}\", end=\"\\r\")\n",
    "            if prev_loss is not None and torch.abs(loss - prev_loss) < tol * 0:\n",
    "                print(f\"\\nConverged after {iter_} iterations\")\n",
    "                break\n",
    "            prev_loss = loss.item()\n",
    "        out = log_params.detach()\n",
    "        print(\"\\nOptimized parameters (log-space):\", out)\n",
    "    else:\n",
    "        out = init\n",
    "\n",
    "    eps = torch.exp(out[0])\n",
    "    s2 = torch.exp(out[1])\n",
    "    Lambda = lambdafunc(m, eps, beta)\n",
    "    A_final = s2 * torch.diag(1.0 / Lambda) + phi_T_phi + 1e-10 * torch.eye(MM, device=device, dtype=dtype)\n",
    "    \n",
    "    A_final_cpu = A_final.cpu()\n",
    "    L_final_cpu = torch.linalg.cholesky(A_final_cpu)\n",
    "    phi_T_y_cpu = phi_T_y.cpu()\n",
    "    sol_final = torch.cholesky_solve(phi_T_y_cpu.unsqueeze(1), L_final_cpu).squeeze(1).to(device)\n",
    "    \n",
    "    K_inv_y = (1.0 / s2) * (y - Phi @ sol_final)\n",
    "    Phi_new = phifunc(m, x_new)\n",
    "    y_new = Phi_new @ (torch.diag(Lambda) @ (Phi.t() @ K_inv_y))\n",
    "    \n",
    "    return y_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e2edfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c521b69",
   "metadata": {},
   "source": [
    "# Step 3: Execute the benchmark\n",
    "To use GPU, set gpu = True. To use CPU only, set gpu = False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6be595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WARM‑UP ===\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2060, -1.3394])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2060, -1.3394])\n",
      "Warm‑up RAM before=600.0 MB, peak=1122.2 MB (Δ=522.2 MB)\n",
      "Warm‑up VRAM Δ=0.0 MB\n",
      "Warm‑up done.\n",
      "\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2059, -1.3429])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2059, -1.3429])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2064, -1.3410])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2064, -1.3410])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2051, -1.3402])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2051, -1.3402])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2064, -1.3362])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2064, -1.3362])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2051, -1.3432])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2051, -1.3432])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2060, -1.3382])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2060, -1.3382])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2064, -1.3348])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2064, -1.3348])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2065, -1.3401])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2065, -1.3401])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2053, -1.3424])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2053, -1.3424])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2062, -1.3387])\n",
      "\n",
      "Optimized parameters (log-space): tensor([ 6.2062, -1.3387])\n",
      "\n",
      "=== Results ===\n",
      "   Time (s)       MSE  RAM Before (MB)  RAM Peak (MB)  RAM Δ (MB)  VRAM Δ (MB)\n",
      "0  0.070524  0.299076      1276.121094    1278.933594    2.812500          0.0\n",
      "1  0.072519  0.298675      1276.234375    1276.234375    0.000000          0.0\n",
      "2  0.072679  0.297553      1276.234375    1276.234375    0.000000          0.0\n",
      "3  0.074126  0.298639      1276.296875    1276.296875    0.000000          0.0\n",
      "4  0.079829  0.301437      1276.296875    1276.296875    0.000000          0.0\n",
      "5  0.080343  0.299595      1276.296875    1276.300781    0.003906          0.0\n",
      "6  0.061395  0.299464      1276.300781    1276.300781    0.000000          0.0\n",
      "7  0.067559  0.299661      1276.300781    1276.300781    0.000000          0.0\n",
      "8  0.072999  0.301777      1276.363281    1276.367188    0.003906          0.0\n",
      "9  0.080908  0.300155      1276.367188    1276.367188    0.000000          0.0\n",
      "\n",
      "Summary:\n",
      "Avg Time: 0.0733s ± 0.0061s\n",
      "Avg MSE:  0.299603 ± 0.001278\n",
      "Avg RAM Δ: 0.3 MB ± 0.9 MB\n",
      "Avg VRAM Δ: 0.0 MB ± 0.0 MB\n",
      "\n",
      "Results saved to 'hssvd_ram_peak_benchmark.csv'\n"
     ]
    }
   ],
   "source": [
    "gpu = False\n",
    "\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "device = torch.device(\"cuda\" if gpu and torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_vram_usage():\n",
    "    return (torch.cuda.max_memory_allocated() / 1024**2\n",
    "            if torch.cuda.is_available() else None)\n",
    "\n",
    "def to_device(t):\n",
    "    return t.double().to(device)\n",
    "\n",
    "\n",
    "data  = pd.read_csv(\"Data/data_1D_100k_full.csv\")\n",
    "all_x = to_device(torch.tensor(data['x'].values).unsqueeze(1))\n",
    "all_y = to_device(torch.tensor(data['y'].values))\n",
    "\n",
    "\n",
    "train_n, test_n = 4_00_000, 100_000\n",
    "init_vec        = to_device(torch.tensor([0.0, 0.0]))\n",
    "m, beta, lr     = 100, 2.0, 1.0#0.1\n",
    "num_repeats     = 10\n",
    "proc            = psutil.Process()\n",
    "\n",
    "def prepare_split():\n",
    "    tx, ty, vx, vy = splitter(all_x, all_y,\n",
    "                               n_train=train_n, n_test=test_n,\n",
    "                               move_to_gpu=gpu)\n",
    "    return map(to_device, (tx, ty, vx, vy))\n",
    "\n",
    "def single_run(is_warmup=False):\n",
    "    tx, ty, vx, vy = prepare_split()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    ram_before = proc.memory_info().rss / (1024**2)\n",
    "    def target():\n",
    "        return hssvd_predict_1d(\n",
    "            tx.squeeze(), ty.squeeze(), vx.squeeze(),\n",
    "            init=init_vec, m=m, beta=beta, train=True, lr=lr, max_iter=10\n",
    "        )\n",
    "    peak_mem, y_pred = memory_usage(\n",
    "        (target, ),\n",
    "        max_usage=True,\n",
    "        retval=True,\n",
    "        interval=0.01\n",
    "    )\n",
    "    ram_delta = peak_mem - ram_before\n",
    "    vram_before = get_vram_usage()\n",
    "    vram_peak   = get_vram_usage()\n",
    "    vram_delta  = (vram_peak - vram_before) if vram_before is not None else None\n",
    "    start = time.time()\n",
    "    y_pred = target()\n",
    "    elapsed = time.time() - start\n",
    "    if not is_warmup:\n",
    "        y_pred = y_pred.cpu()\n",
    "        mse = torch.mean((y_pred - vy.cpu())**2).item()\n",
    "        return elapsed, mse, ram_before, peak_mem, ram_delta, vram_delta\n",
    "    else:\n",
    "        print(f\"Warm‑up RAM before={ram_before:.1f} MB, peak={peak_mem:.1f} MB (Δ={ram_delta:.1f} MB)\")\n",
    "        if vram_delta is not None:\n",
    "            print(f\"Warm‑up VRAM Δ={vram_delta:.1f} MB\")\n",
    "        print(\"Warm‑up done.\\n\")\n",
    "print(\"\\n=== WARM‑UP ===\")\n",
    "single_run(is_warmup=True)\n",
    "\n",
    "columns = [\"Time (s)\", \"MSE\", \"RAM Before (MB)\", \"RAM Peak (MB)\", \"RAM Δ (MB)\", \"VRAM Δ (MB)\"]\n",
    "results = [single_run() for _ in range(num_repeats)]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "df.to_csv(\"hssvd_ram_peak_benchmark.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== Results ===\")\n",
    "print(df)\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Avg Time: {df['Time (s)'].mean():.4f}s ± {df['Time (s)'].std():.4f}s\")\n",
    "print(f\"Avg MSE:  {df['MSE'].mean():.6f} ± {df['MSE'].std():.6f}\")\n",
    "print(f\"Avg RAM Δ: {df['RAM Δ (MB)'].mean():.1f} MB ± {df['RAM Δ (MB)'].std():.1f} MB\")\n",
    "if df[\"VRAM Δ (MB)\"].notnull().all():\n",
    "    print(f\"Avg VRAM Δ: {df['VRAM Δ (MB)'].mean():.1f} MB ± {df['VRAM Δ (MB)'].std():.1f} MB\")\n",
    "\n",
    "print(\"\\nResults saved to 'hssvd_ram_peak_benchmark.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b56764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: Mean = 0.0733s, SD = 0.0061s\n",
      "MSE: Mean = 0.299603, SD = 0.001278\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "results_array = np.array(results)\n",
    "\n",
    "time_values = results_array[:, 0]\n",
    "mse_values = results_array[:, 1]\n",
    "\n",
    "\n",
    "time_mean = np.mean(time_values)\n",
    "time_sd = np.std(time_values, ddof=1)\n",
    "\n",
    "mse_mean = np.mean(mse_values)\n",
    "mse_sd = np.std(mse_values, ddof=1)\n",
    "\n",
    "print(f\"Time: Mean = {time_mean:.4f}s, SD = {time_sd:.4f}s\")\n",
    "print(f\"MSE: Mean = {mse_mean:.6f}, SD = {mse_sd:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c979ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29907616, 0.29867476, 0.29755316, 0.29863851, 0.30143735,\n",
       "       0.29959526, 0.29946414, 0.2996606 , 0.30177706, 0.30015483])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scaleGP)",
   "language": "python",
   "name": "scalegp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
