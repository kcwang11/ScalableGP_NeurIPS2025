{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7511799-a297-4192-8eab-f89cdc78dd2c",
   "metadata": {},
   "source": [
    "# Important Note\n",
    "RAM and VRAM measurements are dependent on the computer state, and should only be interpreted relative to each other. In order to obtain RAM and VRAM measurements, perform the following steps:\n",
    "\n",
    "1 - Restart the Kernel\n",
    "\n",
    "2 - Run the \"Loading Required Packages and Helper Functions\" cell\n",
    "\n",
    "3 - Run the \"Loading Data\" cell\n",
    "\n",
    "4 - Run ONLY ONE iteration of the desired method, and read the RAM and VRAM usage reports printed by the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531dca75-5931-4d1c-a9c7-f4b266b6ec42",
   "metadata": {},
   "source": [
    "# Loading Required Packages and Helper Functions\n",
    "If you would like to use Cuda, set gpu = True. Otherwise set gpu = False. \n",
    "\n",
    "Step 1: Run the following cell to import the required packages and helper functions. Set the number of replicates desired.\n",
    "\n",
    "Step 2: Load the Data\n",
    "\n",
    "Step 3: Execute the cells under the method you wish to replicate.\n",
    "\n",
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e0d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e07126f-2559-4249-9221-117c4c53ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = True\n",
    "n_replicates = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6605e5-f791-44f8-ada3-9efb2237ed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU availability:  True\n",
      "22408.5390625\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "import statistics\n",
    "import numpy as np\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import psutil\n",
    "def get_mem():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import tqdm\n",
    "import gpytorch\n",
    "import tqdm\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.mlls import DeepApproximateMLL\n",
    "\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.mlls import DeepApproximateMLL\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import gpytorch\n",
    "import pynvml\n",
    "import psutil\n",
    "import statistics\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import pynvml\n",
    "import psutil\n",
    "\n",
    "def log_memory():\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    max_allocated = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "    max_reserved = torch.cuda.max_memory_reserved() / 1024**2    # MB\n",
    "    gpu_used = meminfo.used / 1024**2                            # MB\n",
    "    sys_used = psutil.virtual_memory().used / 1024**3            # GB\n",
    "    print(f\"[PyTorch] Max Allocated: {max_allocated:.2f} MB | Max Reserved: {max_reserved:.2f} MB\")\n",
    "    print(f\"[GPU VRAM] Used (nvidia-smi): {gpu_used:.2f} MB | [System RAM]: {sys_used:.2f} GB\")\n",
    "    return max_allocated, max_reserved, gpu_used, sys_used\n",
    "\n",
    "\n",
    "max_vram = 0\n",
    "max_ram = 0\n",
    "def get_mem():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss/(1024**2)\n",
    "\n",
    "max_vram = 0\n",
    "def vram_usage():\n",
    "    global max_vram\n",
    "    if gpu:\n",
    "        max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "\n",
    "\n",
    "print(\"GPU availability: \", torch.cuda.is_available())\n",
    "print(psutil.virtual_memory().used / (1024 ** 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7e28d-807c-40e4-8868-4146cd1c5911",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "Step 2: Load the data (note: must run the DataGenerator.Rmd file first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d29fd88-72ae-43bf-9292-e9f77f51d552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_x shape: torch.Size([1000000, 1])\n",
      "all_y shape: torch.Size([1000000])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "x = pd.read_csv('Data/x_100k.csv', header=None).values.squeeze()\n",
    "y = pd.read_csv('Data/y_100k.csv', header=None).values.squeeze()\n",
    "all_x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "all_y = torch.tensor(y, dtype=torch.float32)\n",
    "all_x = all_x.contiguous()\n",
    "all_y = all_y.contiguous()\n",
    "print(\"all_x shape:\", all_x.shape)\n",
    "print(\"all_y shape:\", all_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e74326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80000, 1]) torch.Size([80000])\n",
      "torch.Size([20000, 1]) torch.Size([20000])\n"
     ]
    }
   ],
   "source": [
    "def splitter(x_cpu, y_cpu, n_train=80000, n_test=20000, random_state=42, move_to_gpu=True):\n",
    "    assert x_cpu.shape[0] == y_cpu.shape[0], \"Mismatch in number of samples\"\n",
    "    total_samples = x_cpu.shape[0]\n",
    "    assert n_train + n_test <= total_samples, \"Not enough samples to split\"\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "    indices = rng.permutation(total_samples)\n",
    "    train_idx = indices[:n_train]\n",
    "    test_idx  = indices[n_train:n_train + n_test]\n",
    "    train_x = x_cpu[train_idx].contiguous()\n",
    "    train_y = y_cpu[train_idx].contiguous()\n",
    "    test_x  = x_cpu[test_idx].contiguous()\n",
    "    test_y  = y_cpu[test_idx].contiguous()\n",
    "    if move_to_gpu and torch.cuda.is_available():\n",
    "        train_x = train_x.cuda()\n",
    "        train_y = train_y.cuda()\n",
    "        test_x = test_x.cuda()\n",
    "        test_y = test_y.cuda()\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "train_x, train_y, test_x, test_y = splitter(all_x, all_y, n_train=80000, n_test=20000)\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b6082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d6ca619-2934-42e4-8d62-9c5ae1a6c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e5420",
   "metadata": {},
   "source": [
    "# Simulations\n",
    "Step 3: Execute the simulations to be reproduced. If all simulations are run, there is a summarizer at the end. Otherwise, the relevant statistics are printed at the end of each method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955df80-1c9d-4055-8e5e-3ea753501029",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# SKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594b5818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Replicate 1/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\utils\\sparse.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if nonzero_indices.storage():\n",
      "c:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\utils\\sparse.py:66: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:80.)\n",
      "  res = cls(index_tensor, value_tensor, interp_size)\n",
      "c:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\utils\\sparse.py:66: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:646.)\n",
      "  res = cls(index_tensor, value_tensor, interp_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep 1: MSE=0.3269, Time=3.40s, RAM before=0.0MB, peak=1033.9MB (Δ=1033.9MB), VRAM peak=5732.2MB\n",
      "\n",
      "=== Replicate 2/11 ===\n",
      "Rep 2: MSE=0.3249, Time=3.10s, RAM before=0.0MB, peak=1044.3MB (Δ=1044.3MB), VRAM peak=5778.9MB\n",
      "\n",
      "=== Replicate 3/11 ===\n",
      "Rep 3: MSE=0.3252, Time=3.10s, RAM before=0.0MB, peak=1044.8MB (Δ=1044.8MB), VRAM peak=5780.0MB\n",
      "\n",
      "=== Replicate 4/11 ===\n",
      "Rep 4: MSE=0.3286, Time=3.06s, RAM before=0.0MB, peak=1044.8MB (Δ=1044.8MB), VRAM peak=5780.3MB\n",
      "\n",
      "=== Replicate 5/11 ===\n",
      "Rep 5: MSE=0.3288, Time=3.04s, RAM before=0.0MB, peak=1044.8MB (Δ=1044.8MB), VRAM peak=5779.7MB\n",
      "\n",
      "=== Replicate 6/11 ===\n",
      "Rep 6: MSE=0.3307, Time=3.03s, RAM before=0.0MB, peak=1047.8MB (Δ=1047.8MB), VRAM peak=5780.1MB\n",
      "\n",
      "=== Replicate 7/11 ===\n",
      "Rep 7: MSE=0.3267, Time=2.97s, RAM before=0.0MB, peak=1047.8MB (Δ=1047.8MB), VRAM peak=5779.9MB\n",
      "\n",
      "=== Replicate 8/11 ===\n",
      "Rep 8: MSE=0.3266, Time=3.00s, RAM before=0.0MB, peak=1048.1MB (Δ=1048.1MB), VRAM peak=5780.1MB\n",
      "\n",
      "=== Replicate 9/11 ===\n",
      "Rep 9: MSE=0.3298, Time=2.99s, RAM before=0.0MB, peak=1048.1MB (Δ=1048.1MB), VRAM peak=5779.9MB\n",
      "\n",
      "=== Replicate 10/11 ===\n",
      "Rep 10: MSE=0.3299, Time=3.00s, RAM before=0.0MB, peak=1048.1MB (Δ=1048.1MB), VRAM peak=5780.1MB\n",
      "\n",
      "=== Replicate 11/11 ===\n",
      "Rep 11: MSE=0.3262, Time=2.98s, RAM before=0.0MB, peak=1048.1MB (Δ=1048.1MB), VRAM peak=5779.9MB\n"
     ]
    }
   ],
   "source": [
    "import tqdm, time, gc\n",
    "import torch, gpytorch\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        grid_size = gpytorch.utils.grid.choose_grid_size(train_x, 1.0 / 25.0)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=1),\n",
    "                grid_size=grid_size, num_dims=1\n",
    "            )\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return gpytorch.distributions.MultivariateNormal(\n",
    "            self.mean_module(x),\n",
    "            self.covar_module(x)\n",
    "        )\n",
    "\n",
    "n_replicates = 11\n",
    "training_iterations = 10#32\n",
    "n_train, n_test = 400_000, 20_000\n",
    "random_state = 42\n",
    "mse_l_ski, time_l_ski = [], []\n",
    "\n",
    "for rep in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {rep + 1}/{n_replicates} ===\")\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_x, all_y,\n",
    "        n_train=n_train, n_test=n_test,\n",
    "        random_state=random_state + rep,\n",
    "        move_to_gpu=torch.cuda.is_available()\n",
    "    )\n",
    "    ram_before = get_mem() / (1024**2)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    model.train(); likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    if torch.cuda.is_available():\n",
    "        mll = mll.cuda()\n",
    "    def train_fn():\n",
    "        for _ in range(training_iterations):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return None\n",
    "    start_time = time.time()\n",
    "    peak_ram = memory_usage(\n",
    "        (train_fn,),\n",
    "        max_usage=True,\n",
    "        retval=False,\n",
    "        interval=0.01\n",
    "    )\n",
    "    elapsed = time.time() - start_time\n",
    "    vram_peak = torch.cuda.max_memory_allocated() / (1024**2) if torch.cuda.is_available() else None\n",
    "    ram_delta = peak_ram - ram_before\n",
    "    model.eval(); likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        pred = likelihood(model(test_x)).mean.cpu()\n",
    "    mse = torch.mean((pred - test_y.cpu()) ** 2).item()\n",
    "    mse_l_ski.append(mse)\n",
    "    time_l_ski.append(elapsed)\n",
    "    print(\n",
    "        f\"Rep {rep+1}: MSE={mse:.4f}, Time={elapsed:.2f}s, \"\n",
    "        f\"RAM before={ram_before:.1f}MB, peak={peak_ram:.1f}MB (Δ={ram_delta:.1f}MB)\"\n",
    "        + (f\", VRAM peak={vram_peak:.1f}MB\" if vram_peak is not None else \"\")\n",
    "    )\n",
    "    del model, likelihood\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa31965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32774763703346255\n",
      "0.002075905921105616\n",
      "3.0282730579376222\n",
      "0.04694740011788096\n"
     ]
    }
   ],
   "source": [
    "print(statistics.mean(mse_l_ski[1:]))\n",
    "print(statistics.stdev(mse_l_ski[1:]))\n",
    "\n",
    "print(statistics.mean(time_l_ski[1:]))\n",
    "print(statistics.stdev(time_l_ski[1:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfad5eb3-9eb7-44ba-a7bd-c680b3716264",
   "metadata": {},
   "source": [
    "# Sparse GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e5399d-22be-42e9-ad0d-4db2b746f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "max_vram = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d229e74-5c6b-401b-8a77-a2361bf99247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Replicate 1/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1: 100%|██████████| 10/10 [00:00<00:00, 12.54it/s, loss=1.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 5779.93 MB | Max Reserved: 7968.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 3152.32 MB | [System RAM]: 22.39 GB\n",
      "Rep 1: MSE=0.4671, Time=0.80s, VRAM=5779.93MB, RAM diff=12.77MB\n",
      "\n",
      "=== Replicate 2/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2: 100%|██████████| 10/10 [00:00<00:00, 13.08it/s, loss=1.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 5779.93 MB | Max Reserved: 7968.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 3466.93 MB | [System RAM]: 22.39 GB\n",
      "Rep 2: MSE=0.4746, Time=0.76s, VRAM=5779.93MB, RAM diff=0.23MB\n",
      "\n",
      "=== Replicate 3/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3: 100%|██████████| 10/10 [00:00<00:00, 13.06it/s, loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 5779.93 MB | Max Reserved: 7968.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 3466.93 MB | [System RAM]: 22.39 GB\n",
      "Rep 3: MSE=0.4822, Time=0.77s, VRAM=5779.93MB, RAM diff=0.00MB\n",
      "\n",
      "=== Replicate 4/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 4: 100%|██████████| 10/10 [00:00<00:00, 13.26it/s, loss=1.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 5779.93 MB | Max Reserved: 7968.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 3450.93 MB | [System RAM]: 22.39 GB\n",
      "Rep 4: MSE=0.4795, Time=0.76s, VRAM=5779.93MB, RAM diff=0.00MB\n",
      "\n",
      "=== Replicate 5/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 5: 100%|██████████| 10/10 [00:00<00:00, 13.37it/s, loss=1.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 5779.93 MB | Max Reserved: 7968.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 3450.93 MB | [System RAM]: 22.39 GB\n",
      "Rep 5: MSE=0.4785, Time=0.75s, VRAM=5779.93MB, RAM diff=0.00MB\n",
      "\n",
      "=== Replicate 6/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 6: 100%|██████████| 10/10 [00:00<00:00, 13.39it/s, loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 5779.93 MB | Max Reserved: 7968.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 3423.31 MB | [System RAM]: 22.39 GB\n",
      "Rep 6: MSE=0.4734, Time=0.75s, VRAM=5779.93MB, RAM diff=0.00MB\n",
      "\n",
      "=== Replicate 7/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 7: 100%|██████████| 10/10 [00:00<00:00, 13.36it/s, loss=1.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 5779.93 MB | Max Reserved: 7968.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 3423.31 MB | [System RAM]: 22.39 GB\n",
      "Rep 7: MSE=0.4690, Time=0.75s, VRAM=5779.93MB, RAM diff=0.00MB\n",
      "\n",
      "=== Replicate 8/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 8: 100%|██████████| 10/10 [00:00<00:00, 13.46it/s, loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 5779.93 MB | Max Reserved: 7968.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 3423.31 MB | [System RAM]: 22.39 GB\n",
      "Rep 8: MSE=0.4765, Time=0.74s, VRAM=5779.93MB, RAM diff=0.00MB\n",
      "\n",
      "=== Replicate 9/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 9: 100%|██████████| 10/10 [00:00<00:00, 12.86it/s, loss=1.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 5779.93 MB | Max Reserved: 7968.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 3409.31 MB | [System RAM]: 22.39 GB\n",
      "Rep 9: MSE=0.4793, Time=0.78s, VRAM=5779.93MB, RAM diff=0.00MB\n",
      "\n",
      "=== Replicate 10/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 10: 100%|██████████| 10/10 [00:00<00:00, 13.43it/s, loss=1.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 5779.93 MB | Max Reserved: 7968.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 3409.31 MB | [System RAM]: 22.39 GB\n",
      "Rep 10: MSE=0.4751, Time=0.75s, VRAM=5779.93MB, RAM diff=0.00MB\n",
      "\n",
      "=== Replicate 11/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 11: 100%|██████████| 10/10 [00:00<00:00, 13.48it/s, loss=1.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 5779.93 MB | Max Reserved: 7968.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 3409.31 MB | [System RAM]: 22.39 GB\n",
      "Rep 11: MSE=0.4752, Time=0.74s, VRAM=5779.93MB, RAM diff=0.00MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        base_covar = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5)\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.InducingPointKernel(\n",
    "            base_covar,\n",
    "            #inducing_points=train_x[::15000].clone(),\n",
    "            inducing_points=train_x[torch.randperm(train_x.shape[0])[:100]].clone(),\n",
    "            likelihood=likelihood\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.base_covar_module = ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2))\n",
    "        self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=torch.linspace(train_x.min(), train_x.max(), steps=40).unsqueeze(-1), likelihood=likelihood)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "\n",
    "# Experiment Parameters\n",
    "# -----------------------------\n",
    "n_replicates = 11\n",
    "training_iterations = 10#125\n",
    "n_train, n_test = 400000, 20000\n",
    "random_state = 422\n",
    "gpu = torch.cuda.is_available()\n",
    "gpu = True\n",
    "\n",
    "mse_l_sgpr = []\n",
    "time_l_sgpr = []\n",
    "vram_l_sgpr = []\n",
    "\n",
    "# -----------------------------\n",
    "# Replicates Loop\n",
    "# -----------------------------\n",
    "for rep in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {rep + 1}/{n_replicates} ===\")\n",
    "\n",
    "    # ---- Split Data ----\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_x, all_y,\n",
    "        n_train=n_train, n_test=n_test,\n",
    "        random_state=random_state + rep,\n",
    "        move_to_gpu=gpu\n",
    "    )\n",
    "    ram_init = get_mem()\n",
    "    # ---- Initialize ----\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    model = model.double()\n",
    "    likelihood = likelihood.double()\n",
    "\n",
    "    # ---- Train ----\n",
    "    start = time.time()\n",
    "    iterator = tqdm.tqdm(range(training_iterations), desc=f\"Train {rep + 1}\")\n",
    "\n",
    "    for it in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        iterator.set_postfix(loss=loss.item())  # <-- Add this line\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    ram_diff = get_mem() - ram_init\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        pred = likelihood(model(test_x)).mean.cpu()\n",
    "    mse = torch.mean((pred - test_y.cpu()) ** 2).item()\n",
    "    peak_alloc, peak_reserved, gpu_used, sys_used = log_memory()\n",
    "    mse_l_sgpr.append(mse)\n",
    "    time_l_sgpr.append(elapsed)\n",
    "    vram_l_sgpr.append(peak_alloc)\n",
    "    print(f\"Rep {rep + 1}: MSE={mse:.4f}, Time={elapsed:.2f}s, VRAM={peak_alloc:.2f}MB, RAM diff={ram_diff:.2f}MB\")\n",
    "    del model, likelihood\n",
    "    gc.collect()\n",
    "    if gpu:\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "097d17fa-8cbd-4bc5-92cd-a6e51066d107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47633379463561154\n",
      "0.0037449226041651297\n",
      "0.7551281929016114\n",
      "0.01192968158144584\n"
     ]
    }
   ],
   "source": [
    "print(statistics.mean(mse_l_sgpr[1:]))\n",
    "print(statistics.stdev(mse_l_sgpr[1:]))\n",
    "\n",
    "print(statistics.mean(time_l_sgpr[1:]))\n",
    "print(statistics.stdev(time_l_sgpr[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b424e738-8f9e-4027-b302-a7a6eb2a4784",
   "metadata": {},
   "source": [
    "# LOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "520eddc5-3859-4dd7-b92d-67304487353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lanczos Variance Estimates (LOVE)\n",
    "max_vram = 0\n",
    "gc.collect()\n",
    "if gpu:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed2886-f6e2-42b7-b5c8-72a61f4bc513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Replicate 1/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1: 100%|██████████| 10/10 [00:05<00:00,  1.85it/s, loss=0.829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6743.16 MB | Max Reserved: 16652.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 16345.73 MB | [System RAM]: 26.56 GB\n",
      "LoVE Rep 1: MSE=0.2994, Time=5.40s, RAM Δ=25.19MB, VRAM=6743.16MB\n",
      "\n",
      "=== Replicate 2/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2: 100%|██████████| 10/10 [00:08<00:00,  1.21it/s, loss=0.835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6826.47 MB | Max Reserved: 13994.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 15124.86 MB | [System RAM]: 25.65 GB\n",
      "LoVE Rep 2: MSE=0.2950, Time=8.24s, RAM Δ=6.14MB, VRAM=6826.47MB\n",
      "\n",
      "=== Replicate 3/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3: 100%|██████████| 10/10 [00:05<00:00,  1.92it/s, loss=0.831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6827.76 MB | Max Reserved: 14178.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 15309.05 MB | [System RAM]: 25.40 GB\n",
      "LoVE Rep 3: MSE=0.2986, Time=5.22s, RAM Δ=16.01MB, VRAM=6827.76MB\n",
      "\n",
      "=== Replicate 4/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 4: 100%|██████████| 10/10 [00:05<00:00,  1.94it/s, loss=0.858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6829.24 MB | Max Reserved: 13974.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 15105.36 MB | [System RAM]: 25.48 GB\n",
      "LoVE Rep 4: MSE=0.3001, Time=5.15s, RAM Δ=11.27MB, VRAM=6829.24MB\n",
      "\n",
      "=== Replicate 5/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 5: 100%|██████████| 10/10 [00:05<00:00,  1.94it/s, loss=0.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6829.27 MB | Max Reserved: 14158.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 15291.80 MB | [System RAM]: 25.37 GB\n",
      "LoVE Rep 5: MSE=0.3007, Time=5.15s, RAM Δ=-99.93MB, VRAM=6829.27MB\n",
      "\n",
      "=== Replicate 6/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 6: 100%|██████████| 10/10 [00:05<00:00,  1.94it/s, loss=0.827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6830.70 MB | Max Reserved: 14158.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 15291.80 MB | [System RAM]: 25.36 GB\n",
      "LoVE Rep 6: MSE=0.3000, Time=5.15s, RAM Δ=9.57MB, VRAM=6830.70MB\n",
      "\n",
      "=== Replicate 7/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 7: 100%|██████████| 10/10 [00:05<00:00,  1.95it/s, loss=0.833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6827.37 MB | Max Reserved: 14158.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 15291.42 MB | [System RAM]: 25.34 GB\n",
      "LoVE Rep 7: MSE=0.3000, Time=5.13s, RAM Δ=-5.06MB, VRAM=6827.37MB\n",
      "\n",
      "=== Replicate 8/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 8: 100%|██████████| 10/10 [00:05<00:00,  1.94it/s, loss=0.828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6829.40 MB | Max Reserved: 14158.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 15291.48 MB | [System RAM]: 25.32 GB\n",
      "LoVE Rep 8: MSE=0.2980, Time=5.16s, RAM Δ=-2.70MB, VRAM=6829.40MB\n",
      "\n",
      "=== Replicate 9/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 9: 100%|██████████| 10/10 [00:05<00:00,  1.93it/s, loss=0.828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6828.76 MB | Max Reserved: 14158.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 15294.23 MB | [System RAM]: 25.37 GB\n",
      "LoVE Rep 9: MSE=0.3010, Time=5.17s, RAM Δ=-3.19MB, VRAM=6828.76MB\n",
      "\n",
      "=== Replicate 10/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 10: 100%|██████████| 10/10 [00:05<00:00,  1.90it/s, loss=0.831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6829.68 MB | Max Reserved: 14158.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 15296.67 MB | [System RAM]: 25.32 GB\n",
      "LoVE Rep 10: MSE=0.2997, Time=5.25s, RAM Δ=-66.79MB, VRAM=6829.68MB\n",
      "\n",
      "=== Replicate 11/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 11: 100%|██████████| 10/10 [00:05<00:00,  1.93it/s, loss=0.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6829.37 MB | Max Reserved: 14158.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 15296.67 MB | [System RAM]: 25.29 GB\n",
      "LoVE Rep 11: MSE=0.2997, Time=5.17s, RAM Δ=-3.50MB, VRAM=6829.37MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import psutil\n",
    "\n",
    "# -----------------------------\n",
    "# Feature Extractor\n",
    "# -----------------------------\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(input_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 1))\n",
    "\n",
    "# -----------------------------\n",
    "# GPRegressionModel\n",
    "# -----------------------------\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=1)\n",
    "            ),\n",
    "            grid_size=100, num_dims=1\n",
    "        )\n",
    "        self.feature_extractor = LargeFeatureExtractor(input_dim=train_x.size(-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected_x = self.feature_extractor(x)\n",
    "        projected_x = projected_x - projected_x.min(0)[0]\n",
    "        projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "        mean_x = self.mean_module(projected_x)\n",
    "        covar_x = self.covar_module(projected_x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# -----------------------------\n",
    "# Experiment Parameters\n",
    "# -----------------------------\n",
    "n_replicates = 11\n",
    "# training_iterations = 10#100\n",
    "# n_train, n_test = 80000, 20000\n",
    "training_iterations = 10#25\n",
    "n_train, n_test = 400000, 100000\n",
    "random_state = 42\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "mse_l_love = []\n",
    "time_l_love = []\n",
    "vram_l_love = []\n",
    "ram_l_love = []\n",
    "\n",
    "# -----------------------------\n",
    "# Replicates Loop\n",
    "# -----------------------------\n",
    "for rep in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {rep + 1}/{n_replicates} ===\")\n",
    "\n",
    "    # ---- Split Data ----\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_x, all_y,\n",
    "        n_train=n_train, n_test=n_test,\n",
    "        random_state=random_state + rep,\n",
    "        move_to_gpu=gpu\n",
    "    )\n",
    "\n",
    "    # ---- RAM before model ----\n",
    "    mem_begin = psutil.virtual_memory().used / (1024 ** 2)  # in MB\n",
    "\n",
    "    # ---- Initialize ----\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    if gpu:\n",
    "        mll = mll.cuda()\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # ---- Train ----\n",
    "    start = time.time()\n",
    "    iterator = tqdm.tqdm(range(training_iterations), desc=f\"Train {rep + 1}\")\n",
    "    for it in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # ---- RAM after ----\n",
    "    mem_end = psutil.virtual_memory().used / (1024 ** 2)  # in MB\n",
    "    delta_ram = mem_end - mem_begin\n",
    "\n",
    "    # ---- Evaluate ----\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "    means = observed_pred.mean.cpu()\n",
    "    mse = torch.mean((means - test_y.cpu()) ** 2).item()\n",
    "\n",
    "    # ---- Log Resources ----\n",
    "    peak_alloc, peak_reserved, gpu_used, sys_used = log_memory()\n",
    "\n",
    "    mse_l_love.append(mse)\n",
    "    time_l_love.append(elapsed)\n",
    "    vram_l_love.append(peak_alloc)\n",
    "    ram_l_love.append(delta_ram)\n",
    "\n",
    "    print(f\"LoVE Rep {rep + 1}: MSE={mse:.4f}, Time={elapsed:.2f}s, RAM Δ={delta_ram:.2f}MB, VRAM={peak_alloc:.2f}MB\")\n",
    "\n",
    "    # ---- Cleanup ----\n",
    "    del model, likelihood\n",
    "    gc.collect()\n",
    "    if gpu:\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61e43d83-1b30-4d8b-8058-cf4c8d840b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2992790758609772\n",
      "0.0017231493648240308\n",
      "5.47954113483429\n",
      "0.9719836564032389\n"
     ]
    }
   ],
   "source": [
    "print(statistics.mean(mse_l_love[1:]))\n",
    "print(statistics.stdev(mse_l_love[1:]))\n",
    "\n",
    "print(statistics.mean(time_l_love[1:]))\n",
    "print(statistics.stdev(time_l_love[1:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90f64c-2426-45ef-be6f-94cd98b9f010",
   "metadata": {},
   "source": [
    "# DKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ae0c28c-fdeb-42ac-9255-752e4c809f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Kernel Learning\n",
    "max_vram = 0\n",
    "gc.collect()\n",
    "if gpu:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de7cc8db-cfc4-4ef2-8e2f-fb03fabf92bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Replicate 1/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1: 100%|██████████| 10/10 [00:05<00:00,  1.86it/s, loss=0.979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6822.24 MB | Max Reserved: 13550.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 14969.09 MB | [System RAM]: 23.44 GB\n",
      "DKL Rep 1: MSE=0.3624, Time=5.39s, RAM Δ=32.34MB, VRAM=6822.24MB\n",
      "\n",
      "=== Replicate 2/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2: 100%|██████████| 10/10 [00:05<00:00,  1.84it/s, loss=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6784.74 MB | Max Reserved: 13552.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 14979.32 MB | [System RAM]: 23.55 GB\n",
      "DKL Rep 2: MSE=0.7973, Time=5.45s, RAM Δ=87.16MB, VRAM=6784.74MB\n",
      "\n",
      "=== Replicate 3/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3: 100%|██████████| 10/10 [00:05<00:00,  1.88it/s, loss=0.995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6784.74 MB | Max Reserved: 13368.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 14812.01 MB | [System RAM]: 23.48 GB\n",
      "DKL Rep 3: MSE=0.3661, Time=5.32s, RAM Δ=-62.34MB, VRAM=6784.74MB\n",
      "\n",
      "=== Replicate 4/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 4: 100%|██████████| 10/10 [00:05<00:00,  1.88it/s, loss=0.997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6785.18 MB | Max Reserved: 13092.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 14537.60 MB | [System RAM]: 23.47 GB\n",
      "DKL Rep 4: MSE=0.6384, Time=5.32s, RAM Δ=35.53MB, VRAM=6785.18MB\n",
      "\n",
      "=== Replicate 5/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 5: 100%|██████████| 10/10 [00:05<00:00,  1.88it/s, loss=0.994]\n",
      "c:\\ProgramData\\miniconda3\\envs\\scaleGP\\lib\\site-packages\\linear_operator\\operators\\added_diag_linear_operator.py:128: NumericalWarning: NaNs encountered in preconditioner computation. Attempting to continue without preconditioning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6783.89 MB | Max Reserved: 13092.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 14551.52 MB | [System RAM]: 23.50 GB\n",
      "DKL Rep 5: MSE=1.2815, Time=5.32s, RAM Δ=8.20MB, VRAM=6783.89MB\n",
      "\n",
      "=== Replicate 6/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 6: 100%|██████████| 10/10 [00:05<00:00,  1.86it/s, loss=0.993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6777.20 MB | Max Reserved: 13460.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 14919.08 MB | [System RAM]: 23.40 GB\n",
      "DKL Rep 6: MSE=0.5832, Time=5.37s, RAM Δ=-92.55MB, VRAM=6777.20MB\n",
      "\n",
      "=== Replicate 7/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 7: 100%|██████████| 10/10 [00:05<00:00,  1.94it/s, loss=1.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6785.01 MB | Max Reserved: 13460.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 15245.32 MB | [System RAM]: 23.43 GB\n",
      "DKL Rep 7: MSE=1.2984, Time=5.16s, RAM Δ=16.16MB, VRAM=6785.01MB\n",
      "\n",
      "=== Replicate 8/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 8: 100%|██████████| 10/10 [00:05<00:00,  1.83it/s, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6776.99 MB | Max Reserved: 13458.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 15216.32 MB | [System RAM]: 23.41 GB\n",
      "DKL Rep 8: MSE=1.0244, Time=5.47s, RAM Δ=17.54MB, VRAM=6776.99MB\n",
      "\n",
      "=== Replicate 9/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 9: 100%|██████████| 10/10 [00:05<00:00,  1.90it/s, loss=0.993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6782.92 MB | Max Reserved: 13182.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 14950.12 MB | [System RAM]: 23.66 GB\n",
      "DKL Rep 9: MSE=1.1792, Time=5.26s, RAM Δ=274.23MB, VRAM=6782.92MB\n",
      "\n",
      "=== Replicate 10/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 10: 100%|██████████| 10/10 [00:05<00:00,  1.90it/s, loss=0.996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6784.72 MB | Max Reserved: 13460.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 15248.17 MB | [System RAM]: 23.75 GB\n",
      "DKL Rep 10: MSE=0.5893, Time=5.26s, RAM Δ=66.23MB, VRAM=6784.72MB\n",
      "\n",
      "=== Replicate 11/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 11: 100%|██████████| 10/10 [00:05<00:00,  1.87it/s, loss=0.971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 6785.19 MB | Max Reserved: 13460.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 15359.80 MB | [System RAM]: 23.71 GB\n",
      "DKL Rep 11: MSE=1.1768, Time=5.34s, RAM Δ=-3.68MB, VRAM=6785.19MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import psutil\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DKL Feature Extractor\n",
    "# -----------------------------\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(input_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 1))\n",
    "\n",
    "# -----------------------------\n",
    "# GPRegressionModel\n",
    "# -----------------------------\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, input_dim):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=1)\n",
    "            ),\n",
    "            num_dims=1, grid_size=100\n",
    "        )\n",
    "        self.feature_extractor = LargeFeatureExtractor(input_dim=input_dim)\n",
    "        self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected_x = self.feature_extractor(x)\n",
    "        projected_x = self.scale_to_bounds(projected_x)\n",
    "        mean_x = self.mean_module(projected_x)\n",
    "        covar_x = self.covar_module(projected_x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# -----------------------------\n",
    "# Experiment Parameters\n",
    "# -----------------------------\n",
    "n_replicates = 11\n",
    "training_iterations = 10#80\n",
    "n_train, n_test = 400000, 20000\n",
    "random_state = 42\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "mse_l_dkl = []\n",
    "time_l_dkl = []\n",
    "vram_l_dkl = []\n",
    "ram_l_dkl = []\n",
    "\n",
    "# -----------------------------\n",
    "# Replicates Loop\n",
    "# -----------------------------\n",
    "for rep in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {rep + 1}/{n_replicates} ===\")\n",
    "\n",
    "    # ---- Split Data ----\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_x, all_y,\n",
    "        n_train=n_train, n_test=n_test,\n",
    "        random_state=random_state + rep,\n",
    "        move_to_gpu=gpu\n",
    "    )\n",
    "\n",
    "    # ---- RAM before model ----\n",
    "    mem_begin = psutil.virtual_memory().used / (1024 ** 2)  # in MB\n",
    "\n",
    "    # ---- Initialize ----\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood, input_dim=train_x.size(-1))\n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.feature_extractor.parameters()},\n",
    "        {'params': model.covar_module.parameters()},\n",
    "        {'params': model.mean_module.parameters()},\n",
    "        {'params': model.likelihood.parameters()},\n",
    "    ], lr=0.01)\n",
    "\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    if gpu:\n",
    "        mll = mll.cuda()\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # ---- Train ----\n",
    "    start = time.time()\n",
    "    iterator = tqdm.tqdm(range(training_iterations), desc=f\"Train {rep + 1}\")\n",
    "    for it in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # ---- RAM after ----\n",
    "    mem_end = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    delta_ram = mem_end - mem_begin\n",
    "\n",
    "    # ---- Evaluate ----\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "    means = observed_pred.mean.cpu()\n",
    "    mse = torch.mean((means - test_y.cpu()) ** 2).item()\n",
    "\n",
    "    # ---- Log Resources ----\n",
    "    peak_alloc, peak_reserved, gpu_used, sys_used = log_memory()\n",
    "\n",
    "    mse_l_dkl.append(mse)\n",
    "    time_l_dkl.append(elapsed)\n",
    "    vram_l_dkl.append(peak_alloc)\n",
    "    ram_l_dkl.append(delta_ram)\n",
    "\n",
    "    print(f\"DKL Rep {rep + 1}: MSE={mse:.4f}, Time={elapsed:.2f}s, RAM Δ={delta_ram:.2f}MB, VRAM={peak_alloc:.2f}MB\")\n",
    "\n",
    "    # ---- Cleanup ----\n",
    "    del model, likelihood\n",
    "    gc.collect()\n",
    "    if gpu:\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac51aa14-6510-4d76-9fcd-4a025320087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8934573709964753\n",
      "0.33908235064845654\n",
      "5.327223777770996\n",
      "0.09148011857315932\n"
     ]
    }
   ],
   "source": [
    "print(statistics.mean(mse_l_dkl[1:]))\n",
    "print(statistics.stdev(mse_l_dkl[1:]))\n",
    "\n",
    "print(statistics.mean(time_l_dkl[1:]))\n",
    "print(statistics.stdev(time_l_dkl[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eab984-9b64-4e7f-9532-039bbb00be33",
   "metadata": {},
   "source": [
    "# SVGP CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00b464d4-2ff1-4d2f-a573-ae6822d71849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVGP_CI\n",
    "max_vram = 0\n",
    "gc.collect()\n",
    "if gpu:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a0611cb-3119-4842-a668-3c2acd3fcf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Replicate 1/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 10/10 [01:44<00:00, 10.41s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 734.46 MB | Max Reserved: 784.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 2548.07 MB | [System RAM]: 23.88 GB\n",
      "SVGP-CIQ Rep 1: MSE=0.3011, Time=104.13s, RAM Δ=185.08MB, VRAM=734.46MB\n",
      "\n",
      "=== Replicate 2/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 10/10 [01:44<00:00, 10.45s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 731.37 MB | Max Reserved: 898.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 2520.16 MB | [System RAM]: 23.90 GB\n",
      "SVGP-CIQ Rep 2: MSE=0.3024, Time=104.52s, RAM Δ=22.14MB, VRAM=731.37MB\n",
      "\n",
      "=== Replicate 3/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 10/10 [01:45<00:00, 10.59s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 731.37 MB | Max Reserved: 896.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 2462.62 MB | [System RAM]: 23.87 GB\n",
      "SVGP-CIQ Rep 3: MSE=0.2986, Time=105.87s, RAM Δ=-27.45MB, VRAM=731.37MB\n",
      "\n",
      "=== Replicate 4/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 10/10 [01:42<00:00, 10.27s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 731.37 MB | Max Reserved: 896.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 2370.94 MB | [System RAM]: 23.96 GB\n",
      "SVGP-CIQ Rep 4: MSE=0.3028, Time=102.73s, RAM Δ=90.41MB, VRAM=731.37MB\n",
      "\n",
      "=== Replicate 5/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 10/10 [01:42<00:00, 10.29s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 731.37 MB | Max Reserved: 896.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 2453.15 MB | [System RAM]: 23.94 GB\n",
      "SVGP-CIQ Rep 5: MSE=0.3057, Time=102.91s, RAM Δ=-4.32MB, VRAM=731.37MB\n",
      "\n",
      "=== Replicate 6/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 10/10 [01:42<00:00, 10.26s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 731.37 MB | Max Reserved: 896.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 2328.50 MB | [System RAM]: 23.95 GB\n",
      "SVGP-CIQ Rep 6: MSE=0.3074, Time=102.59s, RAM Δ=6.32MB, VRAM=731.37MB\n",
      "\n",
      "=== Replicate 7/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 10/10 [01:50<00:00, 11.03s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 731.37 MB | Max Reserved: 896.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 2321.69 MB | [System RAM]: 23.95 GB\n",
      "SVGP-CIQ Rep 7: MSE=0.3050, Time=110.27s, RAM Δ=-1.09MB, VRAM=731.37MB\n",
      "\n",
      "=== Replicate 8/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 10/10 [01:43<00:00, 10.32s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 731.37 MB | Max Reserved: 896.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 2321.88 MB | [System RAM]: 23.97 GB\n",
      "SVGP-CIQ Rep 8: MSE=0.3022, Time=103.23s, RAM Δ=31.02MB, VRAM=731.37MB\n",
      "\n",
      "=== Replicate 9/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 10/10 [01:43<00:00, 10.32s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 731.37 MB | Max Reserved: 896.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 2318.62 MB | [System RAM]: 23.99 GB\n",
      "SVGP-CIQ Rep 9: MSE=0.3050, Time=103.19s, RAM Δ=21.62MB, VRAM=731.37MB\n",
      "\n",
      "=== Replicate 10/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 10/10 [01:48<00:00, 10.81s/it]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 731.37 MB | Max Reserved: 896.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 2320.75 MB | [System RAM]: 24.34 GB\n",
      "SVGP-CIQ Rep 10: MSE=0.3042, Time=108.09s, RAM Δ=358.32MB, VRAM=731.37MB\n",
      "\n",
      "=== Replicate 11/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 10/10 [01:43<00:00, 10.32s/it]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 731.37 MB | Max Reserved: 896.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 2316.25 MB | [System RAM]: 24.44 GB\n",
      "SVGP-CIQ Rep 11: MSE=0.3020, Time=103.18s, RAM Δ=97.77MB, VRAM=731.37MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import psutil\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.CiqVariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2)\n",
    "        )\n",
    "        self.covar_module.base_kernel.initialize(lengthscale=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "n_replicates = 11\n",
    "num_epochs = 10#5\n",
    "batch_size = 3200\n",
    "random_state = 42\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "mse_l_svgpci = []\n",
    "time_l_svgpci = []\n",
    "vram_l_svgpci = []\n",
    "ram_l_svgpci = []\n",
    "\n",
    "\n",
    "for rep in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {rep + 1}/{n_replicates} ===\")\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_x, all_y,\n",
    "        n_train=400000, n_test=20000,\n",
    "        random_state=random_state + rep,\n",
    "        move_to_gpu=gpu\n",
    "    )\n",
    "\n",
    "    mem_begin = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    torch.linspace(train_x.min(), train_x.max(), steps=80).unsqueeze(-1)\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    inducing_points = torch.linspace(0.2, 0.8, 100).unsqueeze(-1)\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(\n",
    "        model.variational_parameters(), num_data=train_y.size(0), lr=0.1\n",
    "    )\n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.002)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start = time.time()\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=f\"Epoch {rep + 1}\")\n",
    "    for epoch in epochs_iter:\n",
    "        minibatch_iter = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False, position=0)\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "            hyperparameter_optimizer.step()\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "    elapsed = time.time() - start\n",
    "    mem_end = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    delta_ram = mem_end - mem_begin\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "    means = observed_pred.mean.cpu()\n",
    "    mse = torch.mean((means - test_y.cpu()) ** 2).item()\n",
    "    peak_alloc, peak_reserved, gpu_used, sys_used = log_memory()\n",
    "    mse_l_svgpci.append(mse)\n",
    "    time_l_svgpci.append(elapsed)\n",
    "    vram_l_svgpci.append(peak_alloc)\n",
    "    ram_l_svgpci.append(delta_ram)\n",
    "    print(f\"SVGP-CIQ Rep {rep + 1}: MSE={mse:.4f}, Time={elapsed:.2f}s, RAM Δ={delta_ram:.2f}MB, VRAM={peak_alloc:.2f}MB\")\n",
    "    del model, likelihood\n",
    "    gc.collect()\n",
    "    if gpu:\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8f1f02c-431f-48d7-9da1-7d52ff797bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3035478860139847\n",
      "0.002477861637851579\n",
      "104.6592376947403\n",
      "2.6280065018581458\n"
     ]
    }
   ],
   "source": [
    "print(statistics.mean(mse_l_svgpci[1:]))\n",
    "print(statistics.stdev(mse_l_svgpci[1:]))\n",
    "\n",
    "print(statistics.mean(time_l_svgpci[1:]))\n",
    "print(statistics.stdev(time_l_svgpci[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174305f1-9eca-4bfb-8b45-55dcb521db47",
   "metadata": {},
   "source": [
    "# NGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53ec3b4b-9332-4954-8e4e-6c688b0fcac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGD\n",
    "max_vram = 0\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41490c0b-1b29-4b3f-882d-0835174f149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Replicate 1/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 45.05 MB | Max Reserved: 62.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 1570.41 MB | [System RAM]: 24.37 GB\n",
      "SVGP-NGD Rep 1: MSE=0.3027, Time=34.03s, RAM Δ=1.39MB, VRAM=45.05MB\n",
      "\n",
      "=== Replicate 2/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 53.14 MB | Max Reserved: 82.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 1582.47 MB | [System RAM]: 24.36 GB\n",
      "SVGP-NGD Rep 2: MSE=0.3007, Time=37.76s, RAM Δ=-13.51MB, VRAM=53.14MB\n",
      "\n",
      "=== Replicate 3/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 53.53 MB | Max Reserved: 82.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 1582.03 MB | [System RAM]: 24.35 GB\n",
      "SVGP-NGD Rep 3: MSE=0.3006, Time=35.54s, RAM Δ=31.96MB, VRAM=53.53MB\n",
      "\n",
      "=== Replicate 4/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 53.14 MB | Max Reserved: 82.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 1577.52 MB | [System RAM]: 24.31 GB\n",
      "SVGP-NGD Rep 4: MSE=0.3035, Time=35.33s, RAM Δ=-46.11MB, VRAM=53.14MB\n",
      "\n",
      "=== Replicate 5/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 53.53 MB | Max Reserved: 82.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 1615.95 MB | [System RAM]: 24.38 GB\n",
      "SVGP-NGD Rep 5: MSE=0.3092, Time=34.29s, RAM Δ=67.24MB, VRAM=53.53MB\n",
      "\n",
      "=== Replicate 6/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 53.14 MB | Max Reserved: 82.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 1593.12 MB | [System RAM]: 24.36 GB\n",
      "SVGP-NGD Rep 6: MSE=0.3088, Time=35.22s, RAM Δ=-17.62MB, VRAM=53.14MB\n",
      "\n",
      "=== Replicate 7/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 53.53 MB | Max Reserved: 82.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 1604.34 MB | [System RAM]: 24.40 GB\n",
      "SVGP-NGD Rep 7: MSE=0.3037, Time=33.68s, RAM Δ=35.67MB, VRAM=53.53MB\n",
      "\n",
      "=== Replicate 8/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 53.14 MB | Max Reserved: 82.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 1900.88 MB | [System RAM]: 24.66 GB\n",
      "SVGP-NGD Rep 8: MSE=0.3033, Time=32.68s, RAM Δ=263.70MB, VRAM=53.14MB\n",
      "\n",
      "=== Replicate 9/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 53.53 MB | Max Reserved: 82.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 1900.73 MB | [System RAM]: 24.62 GB\n",
      "SVGP-NGD Rep 9: MSE=0.3075, Time=34.49s, RAM Δ=-31.50MB, VRAM=53.53MB\n",
      "\n",
      "=== Replicate 10/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 53.14 MB | Max Reserved: 82.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 1887.16 MB | [System RAM]: 24.63 GB\n",
      "SVGP-NGD Rep 10: MSE=0.3056, Time=33.23s, RAM Δ=2.68MB, VRAM=53.14MB\n",
      "\n",
      "=== Replicate 11/11 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] Max Allocated: 53.53 MB | Max Reserved: 82.00 MB\n",
      "[GPU VRAM] Used (nvidia-smi): 1896.39 MB | [System RAM]: 24.52 GB\n",
      "SVGP-NGD Rep 11: MSE=0.3020, Time=34.21s, RAM Δ=-124.29MB, VRAM=53.53MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import psutil\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "n_replicates = 11\n",
    "num_epochs = 10#20#60\n",
    "batch_size = 3200\n",
    "random_state = 42\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "mse_l_ngd = []\n",
    "time_l_ngd = []\n",
    "vram_l_ngd = []\n",
    "ram_l_ngd = []\n",
    "\n",
    "\n",
    "for rep in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {rep + 1}/{n_replicates} ===\")\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_x, all_y,\n",
    "        n_train=400000, n_test=20000,\n",
    "        random_state=random_state + rep,\n",
    "        move_to_gpu=gpu\n",
    "    )\n",
    "    mem_begin = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    inducing_points = torch.linspace(train_x.min(), train_x.max(), steps=50).unsqueeze(-1)\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(\n",
    "        model.variational_parameters(), num_data=train_y.size(0), lr=0.001\n",
    "    )\n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.1)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start = time.time()\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=f\"Epoch {rep + 1}\", leave=False, position=0)\n",
    "    for epoch in epochs_iter:\n",
    "        minibatch_iter = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False, position=0)\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "            hyperparameter_optimizer.step()\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # ---- RAM after ----\n",
    "    mem_end = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    delta_ram = mem_end - mem_begin\n",
    "\n",
    "    # ---- Evaluate ----\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "    means = observed_pred.mean.cpu()\n",
    "    mse = torch.mean((means - test_y.cpu()) ** 2).item()\n",
    "\n",
    "    # ---- Log Resources ----\n",
    "    peak_alloc, peak_reserved, gpu_used, sys_used = log_memory()\n",
    "\n",
    "    mse_l_ngd.append(mse)\n",
    "    time_l_ngd.append(elapsed)\n",
    "    vram_l_ngd.append(peak_alloc)\n",
    "    ram_l_ngd.append(delta_ram)\n",
    "\n",
    "    print(f\"SVGP-NGD Rep {rep + 1}: MSE={mse:.4f}, Time={elapsed:.2f}s, RAM Δ={delta_ram:.2f}MB, VRAM={peak_alloc:.2f}MB\")\n",
    "\n",
    "    # ---- Cleanup ----\n",
    "    del model, likelihood\n",
    "    gc.collect()\n",
    "    if gpu:\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb44415b-9236-403d-ab39-c8e3fbfa28bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30449849665164946\n",
      "0.0031727239269797457\n",
      "34.64251048564911\n",
      "1.4321243837925521\n"
     ]
    }
   ],
   "source": [
    "print(statistics.mean(mse_l_ngd[1:]))\n",
    "print(statistics.stdev(mse_l_ngd[1:]))\n",
    "\n",
    "print(statistics.mean(time_l_ngd[1:]))\n",
    "print(statistics.stdev(time_l_ngd[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b8e294-2c10-447c-b1dd-41f6a4ea12f9",
   "metadata": {},
   "source": [
    "# VNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87cd68-1d06-41cf-9e3b-4eb7a992502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VNN\n",
    "max_vram = 0\n",
    "gc.collect()\n",
    "if gpu:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856a26a-5d71-4a08-9f5c-91ed27786be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import gc\n",
    "import time\n",
    "import statistics\n",
    "import torch\n",
    "import gpytorch\n",
    "import faiss\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import tqdm\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points, likelihood, k=256, training_batch_size=256):\n",
    "        m, d = inducing_points.shape\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "        variational_distribution = gpytorch.variational.MeanFieldVariationalDistribution(m)\n",
    "        if gpu:\n",
    "            inducing_points = inducing_points.cuda()\n",
    "        variational_strategy = NNVariationalStrategy(\n",
    "            self, inducing_points, variational_distribution,\n",
    "            k=k, training_batch_size=training_batch_size\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=d)\n",
    "        )\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        vram_usage()  # assumed to be defined elsewhere\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def __call__(self, x, prior=False, **kwargs):\n",
    "        if x is not None:\n",
    "            if x.dim() == 1:\n",
    "                x = x.unsqueeze(-1)\n",
    "        return self.variational_strategy(x=x, prior=False, **kwargs)\n",
    "\n",
    "\n",
    "n_replicates = 11\n",
    "num_epochs = 10#10#30\n",
    "random_state = 42\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "if False:\n",
    "    k = 32\n",
    "    training_batch_size = 32\n",
    "else:\n",
    "    k = 160#320\n",
    "    training_batch_size = 320 * 4\n",
    "\n",
    "mse_l_vnn = []\n",
    "time_l_vnn = []\n",
    "vram_l_vnn = []\n",
    "ram_l_vnn = []\n",
    "\n",
    "\n",
    "for rep in range(n_replicates):\n",
    "    print(f\"\\n=== Replicate {rep + 1}/{n_replicates} ===\")\n",
    "\n",
    "    train_x, train_y, test_x, test_y = splitter(\n",
    "        all_x, all_y,\n",
    "        n_train=400000, n_test=20000,\n",
    "        random_state=random_state + rep,\n",
    "        move_to_gpu=gpu\n",
    "    )\n",
    "\n",
    "    batch_size = 32\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    mem_begin = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPModel(\n",
    "        inducing_points=train_x[::1].contiguous(),\n",
    "        likelihood=likelihood,\n",
    "        k=k,\n",
    "        training_batch_size=training_batch_size\n",
    "    )\n",
    "\n",
    "    if gpu:\n",
    "        likelihood = likelihood.cuda()\n",
    "        model = model.cuda()\n",
    "\n",
    "    num_batches = model.variational_strategy._total_training_batches\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start = time.time()\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=f\"Epoch {rep + 1}\")\n",
    "    for epoch in epochs_iter:\n",
    "        minibatch_iter = tqdm.tqdm(range(num_batches), leave=False, position=0)\n",
    "        for batch_idx in minibatch_iter:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x=None)\n",
    "            current_indices = model.variational_strategy.current_training_indices\n",
    "            y_batch = train_y[..., current_indices]\n",
    "            if gpu:\n",
    "                y_batch = y_batch.cuda()\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "\n",
    "    mem_end = psutil.virtual_memory().used / (1024 ** 2)\n",
    "    delta_ram = mem_end - mem_begin\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, _ in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    mse = torch.mean((means - test_y.cpu()) ** 2).item()\n",
    "\n",
    "    peak_alloc, peak_reserved, gpu_used, sys_used = log_memory()\n",
    "    mse_l_vnn.append(mse)\n",
    "    time_l_vnn.append(elapsed)\n",
    "    vram_l_vnn.append(peak_alloc)\n",
    "    ram_l_vnn.append(delta_ram)\n",
    "\n",
    "    print(f\"VNN Rep {rep + 1}: MSE={mse:.4f}, Time={elapsed:.2f}s, RAM Δ={delta_ram:.2f}MB, VRAM={peak_alloc:.2f}MB\")\n",
    "    del model, likelihood\n",
    "    gc.collect()\n",
    "    if gpu:\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887db61e-ec92-4906-bfe3-2352d74479d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(mse_l_vnn[1:]))\n",
    "print(statistics.stdev(mse_l_vnn[1:]))\n",
    "\n",
    "print(statistics.mean(time_l_vnn[1:]))\n",
    "print(statistics.stdev(time_l_vnn[1:]))\n",
    "\n",
    "print(\"VNN     --- MSE:\", round(statistics.mean(mse_l_vnn[1:]), 4), \"(\", round(statistics.stdev(mse_l_vnn[1:]), 4), \")  Time:\", round(statistics.mean(time_l_vnn[1:]), 4), \"(\", round(statistics.stdev(time_l_vnn[1:]), 4), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2777a-12b2-4376-8511-e9ff2af69a6e",
   "metadata": {},
   "source": [
    "# Compile Table (MSE and Time only)\n",
    "Order:\n",
    "SKI\n",
    "SGPR\n",
    "LOVE\n",
    "DKL\n",
    "SVGP-CI\n",
    "SVGP\n",
    "NGD\n",
    "VNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddbd90b-6fdf-48d9-a45a-ff205754ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SKI     --- MSE:\", statistics.mean(mse_l_ski[1:]), \"(\", statistics.stdev(mse_l_ski[1:]), \")  Time:\", statistics.mean(time_l_ski[1:]), \"(\", statistics.stdev(time_l_ski[1:]), \")\")\n",
    "print(\"SGPR    --- MSE:\", statistics.mean(mse_l_sgpr[1:]), \"(\", statistics.stdev(mse_l_sgpr[1:]), \")  Time:\", statistics.mean(time_l_sgpr[1:]), \"(\", statistics.stdev(time_l_sgpr[1:]), \")\")\n",
    "print(\"LOVE    --- MSE:\", statistics.mean(mse_l_love[1:]), \"(\", statistics.stdev(mse_l_love[1:]), \")  Time:\", statistics.mean(time_l_love[1:]), \"(\", statistics.stdev(time_l_love[1:]), \")\")\n",
    "print(\"DKL     --- MSE:\", statistics.mean(mse_l_dkl[1:]), \"(\", statistics.stdev(mse_l_dkl[1:]), \")  Time:\", statistics.mean(time_l_dkl[1:]), \"(\", statistics.stdev(time_l_dkl[1:]), \")\")\n",
    "print(\"SVGP-CI --- MSE:\", statistics.mean(mse_l_svgpci[1:]), \"(\", statistics.stdev(mse_l_svgpci[1:]), \")  Time:\", statistics.mean(time_l_svgpci[1:]), \"(\", statistics.stdev(time_l_svgpci[1:]), \")\")\n",
    "print(\"SVGP    --- MSE:\", statistics.mean(mse_l_svgp[1:]), \"(\", statistics.stdev(mse_l_svgp[1:]), \")  Time:\", statistics.mean(time_l_svgp[1:]), \"(\", statistics.stdev(time_l_svgp[1:]), \")\")\n",
    "print(\"NGD     --- MSE:\", statistics.mean(mse_l_ngd[1:]), \"(\", statistics.stdev(mse_l_ngd[1:]), \")  Time:\", statistics.mean(time_l_ngd[1:]), \"(\", statistics.stdev(time_l_ngd[1:]), \")\")\n",
    "print(\"VNN     --- MSE:\", statistics.mean(mse_l_vnn[1:]), \"(\", statistics.stdev(mse_l_vnn[1:]), \")  Time:\", statistics.mean(time_l_vnn[1:]), \"(\", statistics.stdev(time_l_vnn[1:]), \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ecab9-b529-4142-af05-eede13567719",
   "metadata": {},
   "source": [
    "Reordering to match the table in the paper:\n",
    "SVGP\n",
    "SVGP-CI\n",
    "VNN\n",
    "NGD\n",
    "DKL\n",
    "SGPR\n",
    "SKI\n",
    "LOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713418b4-3d7c-4b1a-92ac-a6d31d0ddec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVGP    --- MSE:\", statistics.mean(mse_l_svgp[1:]), \"(\", statistics.stdev(mse_l_svgp[1:]), \")  Time:\", statistics.mean(time_l_svgp[1:]), \"(\", statistics.stdev(time_l_svgp[1:]), \")\")\n",
    "print(\"SVGP-CI --- MSE:\", statistics.mean(mse_l_svgpci[1:]), \"(\", statistics.stdev(mse_l_svgpci[1:]), \")  Time:\", statistics.mean(time_l_svgpci[1:]), \"(\", statistics.stdev(time_l_svgpci[1:]), \")\")\n",
    "print(\"VNN     --- MSE:\", statistics.mean(mse_l_vnn[1:]), \"(\", statistics.stdev(mse_l_vnn[1:]), \")  Time:\", statistics.mean(time_l_vnn[1:]), \"(\", statistics.stdev(time_l_vnn[1:]), \")\")\n",
    "print(\"NGD     --- MSE:\", statistics.mean(mse_l_ngd[1:]), \"(\", statistics.stdev(mse_l_ngd[1:]), \")  Time:\", statistics.mean(time_l_ngd[1:]), \"(\", statistics.stdev(time_l_ngd[1:]), \")\")\n",
    "print(\"DKL     --- MSE:\", statistics.mean(mse_l_dkl[1:]), \"(\", statistics.stdev(mse_l_dkl[1:]), \")  Time:\", statistics.mean(time_l_dkl[1:]), \"(\", statistics.stdev(time_l_dkl[1:]), \")\")\n",
    "print(\"SGPR    --- MSE:\", statistics.mean(mse_l_sgpr[1:]), \"(\", statistics.stdev(mse_l_sgpr[1:]), \")  Time:\", statistics.mean(time_l_sgpr[1:]), \"(\", statistics.stdev(time_l_sgpr[1:]), \")\")\n",
    "print(\"SKI     --- MSE:\", statistics.mean(mse_l_ski[1:]), \"(\", statistics.stdev(mse_l_ski[1:]), \")  Time:\", statistics.mean(time_l_ski[1:]), \"(\", statistics.stdev(time_l_ski[1:]), \")\")\n",
    "print(\"LOVE    --- MSE:\", statistics.mean(mse_l_love[1:]), \"(\", statistics.stdev(mse_l_love[1:]), \")  Time:\", statistics.mean(time_l_love[1:]), \"(\", statistics.stdev(time_l_love[1:]), \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b53ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVGP    --- MSE:\", round(statistics.mean(mse_l_svgp[1:]), 4), \"(\", round(statistics.stdev(mse_l_svgp[1:]), 4), \")  Time:\", round(statistics.mean(time_l_svgp[1:]), 4), \"(\", round(statistics.stdev(time_l_svgp[1:]), 4), \")\")\n",
    "print(\"SVGP-CI --- MSE:\", round(statistics.mean(mse_l_svgpci[1:]), 4), \"(\", round(statistics.stdev(mse_l_svgpci[1:]), 4), \")  Time:\", round(statistics.mean(time_l_svgpci[1:]), 4), \"(\", round(statistics.stdev(time_l_svgpci[1:]), 4), \")\")\n",
    "#print(\"VNN     --- MSE:\", round(statistics.mean(mse_l_vnn[1:]), 4), \"(\", round(statistics.stdev(mse_l_vnn[1:]), 4), \")  Time:\", round(statistics.mean(time_l_vnn[1:]), 4), \"(\", round(statistics.stdev(time_l_vnn[1:]), 4), \")\")\n",
    "print(\"NGD     --- MSE:\", round(statistics.mean(mse_l_ngd[1:]), 4), \"(\", round(statistics.stdev(mse_l_ngd[1:]), 4), \")  Time:\", round(statistics.mean(time_l_ngd[1:]), 4), \"(\", round(statistics.stdev(time_l_ngd[1:]), 4), \")\")\n",
    "print(\"DKL     --- MSE:\", round(statistics.mean(mse_l_dkl[1:]), 4), \"(\", round(statistics.stdev(mse_l_dkl[1:]), 4), \")  Time:\", round(statistics.mean(time_l_dkl[1:]), 4), \"(\", round(statistics.stdev(time_l_dkl[1:]), 4), \")\")\n",
    "print(\"SGPR    --- MSE:\", round(statistics.mean(mse_l_sgpr[1:]), 4), \"(\", round(statistics.stdev(mse_l_sgpr[1:]), 4), \")  Time:\", round(statistics.mean(time_l_sgpr[1:]), 4), \"(\", round(statistics.stdev(time_l_sgpr[1:]), 4), \")\")\n",
    "print(\"SKI     --- MSE:\", round(statistics.mean(mse_l_ski[1:]), 4), \"(\", round(statistics.stdev(mse_l_ski[1:]), 4), \")  Time:\", round(statistics.mean(time_l_ski[1:]), 4), \"(\", round(statistics.stdev(time_l_ski[1:]), 4), \")\")\n",
    "print(\"LOVE    --- MSE:\", round(statistics.mean(mse_l_love[1:]), 4), \"(\", round(statistics.stdev(mse_l_love[1:]), 4), \")  Time:\", round(statistics.mean(time_l_love[1:]), 4), \"(\", round(statistics.stdev(time_l_love[1:]), 4), \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f90b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "\n",
    "methods = [\n",
    "    (\"SVGP\",   mse_l_svgp,   time_l_svgp),\n",
    "    (\"SVGP-CI\",mse_l_svgpci, time_l_svgpci),\n",
    "    (\"NGD\",    mse_l_ngd,    time_l_ngd),\n",
    "    (\"DKL\",    mse_l_dkl,    time_l_dkl),\n",
    "    (\"SGPR\",   mse_l_sgpr,   time_l_sgpr),\n",
    "    (\"SKI\",    mse_l_ski,    time_l_ski),\n",
    "    (\"LOVE\",   mse_l_love,   time_l_love),\n",
    "]\n",
    "\n",
    "for name, mse_list, time_list in methods:\n",
    "    data_mse  = mse_list[1:]\n",
    "    data_time = time_list[1:]\n",
    "    mean_mse  = statistics.mean(data_mse)\n",
    "    sd_mse    = statistics.stdev(data_mse)\n",
    "    mean_time = statistics.mean(data_time)\n",
    "    sd_time   = statistics.stdev(data_time)\n",
    "    print(f\"{name:<8} & {mean_mse:.4f}  & ({sd_mse:.4f} )  & {mean_time:.4f}  & ({sd_time:.4f} )\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scaleGP)",
   "language": "python",
   "name": "scalegp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
